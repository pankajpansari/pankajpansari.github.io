---
title: "Vanilla Transformers"
date: "2023-01-09"
author: "Pankaj Pansari"
---

<div style="text-align: justify"> 

In this post, we are going to look at the architecture of the base version of transformer as proposed in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762).

A transformer is a special type of neural network model which is well-suited for sequence-to-sequence machine learning tasks. These tasks have input as a sequence (words, protein sequences, etc) and the output also as a sequence. Machine translation, text summarization, and question answering are some of the examples of sequence-to-sequence tasks. In fact, transformers can also be used for tasks where either input or output is a sequence. Hence, document classification, sentiment analysis, and protein function prediction are some more tasks amenable to transformers.

At its heart, transformer has two components - an encoder and a decoder. These two components incorporate attention mechanism - two variants called self-attention and cross-attention.

Recurrent neural networks (RNNS) were the state-of-the-art approach to language understanding tasks, along with convolutional neural networks. However, RNNs had two drawbacks which limited their effectiveness. First, they were slow to train because most of the computation was sequential rather than parallel (hence they could not effectively utilize GPUs). Second, due to a problem called vanishing gradients, they could not learn long-range dependencies.

Transformers, on the other hand, need less computation to train and are a better fit for modern machine learning hardware. They also do not suffer from the vanishing gradient problem. This is because they do not use recurrence or convolution. Instead, they use attention mechanism to learn long-range dependencies.

RNNs process input sequentially in left-to-right or right-to-left fashion. They read one word at a time; this forces RNNs to perform multiple steps to make decisions that depend on words far away from each other. The more such steps decisions require, the harder it is for RNNs to learn how to make those decisions.

The sequential nature of RNNs makes it more difficult to take full advantage of modern parallel processing accelerators like GPUs. This is because GPUs are designed to process data in parallel. RNNs are not a good fit for GPUs because they process data sequentially.

CNNs are less sequential than RNNs yet the number of steps required to combine information from distant parts of the input grows with increasing distance.

Transformers perform a small, constant number of steps to compute representations of any input or output sequence.

To compute the next representation for a given word, transformer compares it to every other word in the sentence. It computes an attention score for every pair of words in the sentence. These attention scores are the weights for a weighted average of all the other words' representations. The resulting weighted average is made to pass through a fully-connected neural network yielding the next representation for the given word.
 
Auto-regressive models.