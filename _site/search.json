[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "An Introduction to Large Language Models (LLMs)\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n  \n\n\n\n\nA Simple Painting Style Classifier\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/LLM-intro/index.html",
    "href": "posts/LLM-intro/index.html",
    "title": "An Introduction to Large Language Models (LLMs)",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural human language 1. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models, eg model to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at what differentiates LLMs from earlier language models (LMs), will briefly review the architecture and training strategies, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to latter blog posts.\n\n\n\n\n\nThe essence of tasks such as translating text, answering questions, having a chat is learning how to model human language. In a narrow sense, an LM is a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as:\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1})\\]\nHence, for the sentence “I am going to the market”\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the})\\]\nHence, a LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence\n\nThe cat jumped off the ____\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling)\n\nThe cat ____ off the wall.\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don’t provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled datasets becomes possible. The concept is used not just in the domain of NLP, but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, and facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nEmbeddings represent words by a position in a real-valued vector space, whose dimension can be in hundreds or thousands. The similarity of words and syntax depends on proximity in this space.\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‘encoded’ embeddings. In each operation of that sequence, we allow the embeddings of all words to ‘talk to’ and influence each other. The effect is that the new set of ‘encoded embeddings’ encapsulate higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output.\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention-layers are special architecture features that are present in encoder and decoder which enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word.\nIt’s clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs.\n\n\n\n\n\nModels which are trained on broad data (generally using self-supervision) that can be adapted to a wide range of downstream tasks are called foundation models. Large language models are a specific type of foundation models for NLP tasks and make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and CLIP.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible astonishing performance on a wide variety of tasks. This scale is made possible by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that foundation models with their billions of parameters can be adapted to a wide variety tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it’s better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretrainingng - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let’s say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of text and associated sentiment.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task.\n\n\n\n\n\n\n\nNew research in the direction of foundation models trained on data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nEmbodied foundation models - system situated in an environment where it can interact with other agents and objects. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 2 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence3. There’s been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task4. Clearly, some aspects of LLM behavior indeed seem intelligence, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 5. Out of these, here we only consider fairness.\nFairness - We don’t understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications, since the datasets and models are huge. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 6."
  },
  {
    "objectID": "posts/LLM-intro/index.html#introduction",
    "href": "posts/LLM-intro/index.html#introduction",
    "title": "An Introduction to Large Language Models (LLMs)",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural human language 1. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models, eg model to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at what differentiates LLMs from earlier language models (LMs), will briefly review the architecture and training strategies, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to latter blog posts."
  },
  {
    "objectID": "posts/LLM-intro/index.html#background",
    "href": "posts/LLM-intro/index.html#background",
    "title": "An Introduction to Large Language Models (LLMs)",
    "section": "",
    "text": "The essence of tasks such as translating text, answering questions, having a chat is learning how to model human language. In a narrow sense, an LM is a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as:\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1})\\]\nHence, for the sentence “I am going to the market”\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the})\\]\nHence, a LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence\n\nThe cat jumped off the ____\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling)\n\nThe cat ____ off the wall.\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don’t provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled datasets becomes possible. The concept is used not just in the domain of NLP, but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, and facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nEmbeddings represent words by a position in a real-valued vector space, whose dimension can be in hundreds or thousands. The similarity of words and syntax depends on proximity in this space.\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‘encoded’ embeddings. In each operation of that sequence, we allow the embeddings of all words to ‘talk to’ and influence each other. The effect is that the new set of ‘encoded embeddings’ encapsulate higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output.\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention-layers are special architecture features that are present in encoder and decoder which enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word.\nIt’s clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs."
  },
  {
    "objectID": "posts/LLM-intro/index.html#large-language-models",
    "href": "posts/LLM-intro/index.html#large-language-models",
    "title": "An Introduction to Large Language Models (LLMs)",
    "section": "",
    "text": "Models which are trained on broad data (generally using self-supervision) that can be adapted to a wide range of downstream tasks are called foundation models. Large language models are a specific type of foundation models for NLP tasks and make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and CLIP.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible astonishing performance on a wide variety of tasks. This scale is made possible by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that foundation models with their billions of parameters can be adapted to a wide variety tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it’s better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretrainingng - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let’s say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of text and associated sentiment.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task."
  },
  {
    "objectID": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "href": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "title": "An Introduction to Large Language Models (LLMs)",
    "section": "",
    "text": "New research in the direction of foundation models trained on data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nEmbodied foundation models - system situated in an environment where it can interact with other agents and objects. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 2 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence3. There’s been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task4. Clearly, some aspects of LLM behavior indeed seem intelligence, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 5. Out of these, here we only consider fairness.\nFairness - We don’t understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications, since the datasets and models are huge. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 6."
  },
  {
    "objectID": "posts/LLM-intro/index.html#footnotes",
    "href": "posts/LLM-intro/index.html#footnotes",
    "title": "An Introduction to Large Language Models (LLMs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe language can be artificial, symbolic as well, such as software code.↩︎\nBender, Emily M., et al. “On the dangers of stochastic parrots: Can language models be too big?🦜.” Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2021.↩︎\nBubeck, Sébastien, et al. “Sparks of artificial general intelligence: Early experiments with gpt-4.” arXiv preprint arXiv:2303.12712 (2023).↩︎\nAkyürek, Ekin, et al. “What learning algorithm is in-context learning? investigations with linear models.” arXiv preprint arXiv:2211.15661 (2022).↩︎\nLiu, Yang, et al. “Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment.” arXiv preprint arXiv:2308.05374 (2023).↩︎\nShumailov, Ilia, et al. “The Curse of Recursion: Training on Generated Data Makes Models Forget” arXiv preprint 2305.17493↩︎"
  },
  {
    "objectID": "posts/Painting-classifier/index.html",
    "href": "posts/Painting-classifier/index.html",
    "title": "A Simple Painting Style Classifier",
    "section": "",
    "text": "As a first project for my portfolio, I decided to build a simple deep learning model which can classify paintings based on their styles. As of now, the model supports only binary classification - impressionism and cubism. This project involved the whole end-to-end process - data collection, data cleaning and transformation, model training and testing, and finally deploying it as a web application."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#motivation",
    "href": "posts/Painting-classifier/index.html#motivation",
    "title": "A Simple Painting Style Classifier",
    "section": "Motivation",
    "text": "Motivation\nThe motivation for this project came from an online course on deep learning which I’m currently following (particularly Chapters 1 and 2). In it, the instructor shows how to build an image classifier for a simple task - identifying whether the given image is of a dog or a cat. He then shows how to host the model on a server and deploy it using a basic web application. I decided to adapt the process to my personal interest. I enjoy visiting art museums and looking at paintings. I thought I’d build a painting style identifier. I initially started with only two types - impressionism and cubism, because the two styles have such strong characteristics and hence are easy to distinguish by humans."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#data-collection-and-processing",
    "href": "posts/Painting-classifier/index.html#data-collection-and-processing",
    "title": "A Simple Painting Style Classifier",
    "section": "Data Collection and Processing",
    "text": "Data Collection and Processing\nI began the project on Google Colab using Jupyter notebook. Colab provides free GPU for our model training and Jupyter notebook is helpful for prototyping and quickly visualisation of results. Image collection was done using DuckDuckGo search API. Microsoft Bing API is probably better in terms of providing more relevant images, but inovlves some knotty configuration. I collected 90 images of each class - a small but sufficient dataset because instead of training an image classifier from scratch, I fine-tuned a powerful model trained on a large dataset.\nThere is some preprocessing to be done before we can use our image data:\n\nImage resizing - The images we’ve scrapped are of different sizes. However, our deep learning model expects all images in the dataset to have the same size. Image resizing can be done in multiple ways. Here I chose random cropping. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a 224x224 patch of each image. This method has the advantage that by the end of training, we would’ve used information from all parts of the image and would not have arbitrarily distorted the images. This process is also augments the dataset since each image can yield muliple cropped samples.\nData augmentation - To increase the size of the dataset and to make our model our robust, each image can be flipped, rotated, warped and have brightness and contrast changed.\n\nBoth of these above functionalities are provided by the fastai library. fastai is a high-level Python library for deep learning built on top of PyTorch that makes the whole process of data cleaning, training, and testing both easier and faster."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#model-training",
    "href": "posts/Painting-classifier/index.html#model-training",
    "title": "A Simple Painting Style Classifier",
    "section": "Model Training",
    "text": "Model Training\nI bypassed image cleaning and decided to directly train a deep learning model. I used a ResNet model with 34 layers, pretrained on the ImageNet dataset to perform image classification. This model is provided for use via fastai. Since the model is already pre-trained, we only have to fine-tune it for our style classification task. Using a pre-trained model meant that I did not need a huge amount of training data or computing resource/time.\nfastai provides a very useful function that suggests a good learning rate to use for training. I fined-tuned the model with the suggested learning rate."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#model-validation-and-training",
    "href": "posts/Painting-classifier/index.html#model-validation-and-training",
    "title": "A Simple Painting Style Classifier",
    "section": "Model Validation and Training",
    "text": "Model Validation and Training\nThe error on the validation set during training goes down to zero completely. I looked at some of the outputs from the model and saw that they’re correctly classified.\nI also tested the model on a couple of hand-picked images and visualized the responses."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#deployment-as-a-web-application",
    "href": "posts/Painting-classifier/index.html#deployment-as-a-web-application",
    "title": "A Simple Painting Style Classifier",
    "section": "Deployment as a Web Application",
    "text": "Deployment as a Web Application\nI exported and saved the model from the Colab notebook. I used Gradio to demo my model as a web app. It wraps a python function into a user interface and allows us to launch the demos inside jupyter notebooks. The model was hosted on Hugging Face Spaces.\nFinally to create the web application, I used a JavaScript program that accepts the uploaded image, calls the gradio function for inference, and displays the returned results."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#further-work",
    "href": "posts/Painting-classifier/index.html#further-work",
    "title": "A Simple Painting Style Classifier",
    "section": "Further Work",
    "text": "Further Work\nI tried extending this model to identify more types of painting styles. I’m not an art expert, so after some online research I decided to use 9 different painting styles that are considered the most important. Despite trying various different training parameters, I couldn’t get the final model to be accurate enough.\nA visualisation of the images where the model performed poorly revealed that some of the images scrapped via the DuckDuckGo API were irrelevant and had to be removed. fastai provides a way for us to remove these irrelevant data via a GUI interface. I did this and still the accuracy was poor. I began to suspect that the images returned in response to search queries didn’t actually belong to the right styles. All of this suggested that I needed a better way to build up a cleaner and more accurate dataset. However, this process is going to be time-consuming and hence I’ve decided to postpone it for a later iteration."
  }
]