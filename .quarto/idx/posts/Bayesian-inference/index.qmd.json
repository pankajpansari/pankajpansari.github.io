{"title":"A Probabilistic Perspective on Regularization","markdown":{"yaml":{"title":"A Probabilistic Perspective on Regularization","date":"2023-11-08","author":"Pankaj Pansari"},"headingText":"1. Background","containsRefs":false,"markdown":"\n\nRegularization is a common technique in machine learning to prevent overfitting. The two most widely used regularizers are the L2 and L1 norms. In this post, we look at how there regularizers can be thought of as being derived from prior distributions on the parameters we're estimating.\n\n\n### Model Complexity\n\nOur intuition is that simpler models are preferable; models that are excessively complicated lead to overfitting. Let $L_{\\mathcal D}({\\bf w})$ measure the misfit of model with parameters $\\bf w$ to a given dataset $\\mathcal D$. We can augment the loss function to penalize more complex models:\n\n$$ L({\\bf w}) = L_{\\mathcal D}({\\bf w}) + \\lambda L_{reg}(\\bf w)$$\n\nThis is called *regularization* in machine learning and *shrinkage* in statistics. $\\lambda$ is called the *regularization coefficient* and controls the trade-off between fitting the dataset $\\mathcal D$ well and giving simpler, more generalizable model.\n\nThere are two ways to think of model complexity:\n\n* model complexity as a function of weights of all the features in a model \n\nA feature weight with a high absolute value is more complex than a feature weight with low absolute value. In this case,\n\n$$L_{reg}({\\bf w}) = {\\bf w}^T{\\bf w} = ||{\\bf w}||^2 = w_1^2 + w_2^2 + \\dots + w_n^2$$\n\nThis is known as *L2 regularization* or *weight decay*. Optimizing loss function with L2 regularization is generally easier and results in small parameter values. The resulting model is known as *ridge regression*.\n\n* model complexity as a function of the total number of features with nonzero weights\n\nTo achieve this, we don't set our regularization penaly as the number of nonzero parameters - this makes the optimization difficult. Instead, we use *L1 regularization*:\n\n$$L_{reg}({\\bf w}) = ||{\\bf w}||_1^2 = |w_1| + |w_2| + \\dots + |w_n|$$\n\nThis has the same effect of setting only some parameter values to be nonzero, and is convex hence easier to optimize. The resulting model is known as *Lasso*.\n\n### Maximum Likelihood Estimation\n\nWe are going to make extensive use of the Bayes' theorem. Let $w$ be the parameter we want to estimate and ${\\mathcal D} = (x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)$ be the dataset. Then, we have:\n\n$$P(w|{\\mathcal D}) = \\frac{P({\\mathcal D}|w) \\cdot P(w)}{P({\\mathcal D})}.\n$$\n\nIn the above equation, \n\n* $P(w)$ is the *prior distribution* on the parameters $w$; this encodes our belief about what the parameter values should likely be before we have looked at any data.\n* $P(\\mathcal{D}|w)$ is the *likelihood* of $\\mathcal{D}$ given some assignment of $w$.\n* $P(w|\\mathcal{D})$ is the *posterior distribution* of the parameters $w$ after the dataset $\\mathcal D$ is known.\n\nLet's say we want to estimate parameter $w$ from an observed dataset $\\mathcal D$. We assume the relation between $x$ and $y$ as\n\n$$y = f(x ; w) + \\epsilon$$\n\nwhere $\\epsilon$ is noise drawn from a Gaussian distribution with mean $0$ and variance $\\sigma^2$. This results in a likelihood that is Gaussian distribution:\n\n$$P({\\mathcal D}|w) = {\\mathcal N}(y|f(x ; w), \\sigma^2)$$\n\nLet us assume that the samples in $\\mathcal D$ are independently and identically distributed (i.i.d). Under this assumption and taking logarithms for ease of computation, the likelihood value now becomes: \n\n$$\\log P({\\mathcal D}|w) = \\sum_{i = 1}^{N} \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2).$$\n\nWe can estimate parameter $w$ by maximizing the above quantity. This is called *maximum likelihood estimation* (MLE). Often time, this method of estimating $w$ is called a *frequentist approach*, in constrast to the *Bayesian approach* we discuss below.\n\n## 2. L2 Regularization\n\nWe note that the denominator in the Bayes' theorem does not depend on the parameters $w$ we want to estimate; hence we can ignore that term. We define the unnormalized posterior distribution as\n\n$$P'(w|{\\mathcal D}) = P({\\mathcal D}|w) \\cdot P(w)$$.\n\nLet us now see what happens when we introduce prior distribution on parameter $w$. In the first case, let us assume that $w$ follows a Gaussian distribution ${\\mathcal N}(w|0, \\lambda^{-1})$ where $\\lambda$ is a strictly positive scalar. We then have:\n\n$$P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot {\\mathcal N}(w|0, \\lambda^{-1})$$.\n\nTaking logarithm on both sides, we obtain:\n\n$$log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda w^2 + \\text{const}.$$\n\nWe can now see how our selection of prior for $w$ as normal distribution results in L2 regularization. In the above equation, $w^2$ is the squared L2-norm of the vector $w$ with $\\lambda$ controlling the strength of regularization.\n\nOften, we seek only a point estimate of $w$ instead of the full posterior distribution. One solution is to take the mode of this posterior as the estimate of $w$; this approach is called *maximum a posteriori* (MAP) estimation. MAP estimation differs from MLE in the fact that we incorporate prior knowledge about $w$.\n\n## 3. L1 Regularization\n\nWe'll first need to look at a distribution called the Laplace distribution. With $w$ as the random variable, it is given by\n\n$$ g(w|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|w - \\mu|}{b}\\right)$$\n\nand $\\mu, b$ are referred to as *location parameter* and *diversity* respectively. \n\nNow let us assume that the prior distribution on $w$ is a Laplace distribution with location parameter $\\mu = 0$ and $b = \\lambda^{-1}$. The unnormalized posterior distribution in this case becomes:\n\n$$P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot \\frac{\\lambda}{2} \\exp\\left(-\\lambda |w|\\right).$$\n\nTaking logarithm on both sides,\n\n$$log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda \\cdot |w| + \\text{const}.$$\n\nHence with Laplace distribution as prior on $w$, we arrive at L1 regularization. Again, $\\lambda$ controls the strength of regularization.\n","srcMarkdownNoYaml":"\n\nRegularization is a common technique in machine learning to prevent overfitting. The two most widely used regularizers are the L2 and L1 norms. In this post, we look at how there regularizers can be thought of as being derived from prior distributions on the parameters we're estimating.\n\n## 1. Background\n\n### Model Complexity\n\nOur intuition is that simpler models are preferable; models that are excessively complicated lead to overfitting. Let $L_{\\mathcal D}({\\bf w})$ measure the misfit of model with parameters $\\bf w$ to a given dataset $\\mathcal D$. We can augment the loss function to penalize more complex models:\n\n$$ L({\\bf w}) = L_{\\mathcal D}({\\bf w}) + \\lambda L_{reg}(\\bf w)$$\n\nThis is called *regularization* in machine learning and *shrinkage* in statistics. $\\lambda$ is called the *regularization coefficient* and controls the trade-off between fitting the dataset $\\mathcal D$ well and giving simpler, more generalizable model.\n\nThere are two ways to think of model complexity:\n\n* model complexity as a function of weights of all the features in a model \n\nA feature weight with a high absolute value is more complex than a feature weight with low absolute value. In this case,\n\n$$L_{reg}({\\bf w}) = {\\bf w}^T{\\bf w} = ||{\\bf w}||^2 = w_1^2 + w_2^2 + \\dots + w_n^2$$\n\nThis is known as *L2 regularization* or *weight decay*. Optimizing loss function with L2 regularization is generally easier and results in small parameter values. The resulting model is known as *ridge regression*.\n\n* model complexity as a function of the total number of features with nonzero weights\n\nTo achieve this, we don't set our regularization penaly as the number of nonzero parameters - this makes the optimization difficult. Instead, we use *L1 regularization*:\n\n$$L_{reg}({\\bf w}) = ||{\\bf w}||_1^2 = |w_1| + |w_2| + \\dots + |w_n|$$\n\nThis has the same effect of setting only some parameter values to be nonzero, and is convex hence easier to optimize. The resulting model is known as *Lasso*.\n\n### Maximum Likelihood Estimation\n\nWe are going to make extensive use of the Bayes' theorem. Let $w$ be the parameter we want to estimate and ${\\mathcal D} = (x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)$ be the dataset. Then, we have:\n\n$$P(w|{\\mathcal D}) = \\frac{P({\\mathcal D}|w) \\cdot P(w)}{P({\\mathcal D})}.\n$$\n\nIn the above equation, \n\n* $P(w)$ is the *prior distribution* on the parameters $w$; this encodes our belief about what the parameter values should likely be before we have looked at any data.\n* $P(\\mathcal{D}|w)$ is the *likelihood* of $\\mathcal{D}$ given some assignment of $w$.\n* $P(w|\\mathcal{D})$ is the *posterior distribution* of the parameters $w$ after the dataset $\\mathcal D$ is known.\n\nLet's say we want to estimate parameter $w$ from an observed dataset $\\mathcal D$. We assume the relation between $x$ and $y$ as\n\n$$y = f(x ; w) + \\epsilon$$\n\nwhere $\\epsilon$ is noise drawn from a Gaussian distribution with mean $0$ and variance $\\sigma^2$. This results in a likelihood that is Gaussian distribution:\n\n$$P({\\mathcal D}|w) = {\\mathcal N}(y|f(x ; w), \\sigma^2)$$\n\nLet us assume that the samples in $\\mathcal D$ are independently and identically distributed (i.i.d). Under this assumption and taking logarithms for ease of computation, the likelihood value now becomes: \n\n$$\\log P({\\mathcal D}|w) = \\sum_{i = 1}^{N} \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2).$$\n\nWe can estimate parameter $w$ by maximizing the above quantity. This is called *maximum likelihood estimation* (MLE). Often time, this method of estimating $w$ is called a *frequentist approach*, in constrast to the *Bayesian approach* we discuss below.\n\n## 2. L2 Regularization\n\nWe note that the denominator in the Bayes' theorem does not depend on the parameters $w$ we want to estimate; hence we can ignore that term. We define the unnormalized posterior distribution as\n\n$$P'(w|{\\mathcal D}) = P({\\mathcal D}|w) \\cdot P(w)$$.\n\nLet us now see what happens when we introduce prior distribution on parameter $w$. In the first case, let us assume that $w$ follows a Gaussian distribution ${\\mathcal N}(w|0, \\lambda^{-1})$ where $\\lambda$ is a strictly positive scalar. We then have:\n\n$$P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot {\\mathcal N}(w|0, \\lambda^{-1})$$.\n\nTaking logarithm on both sides, we obtain:\n\n$$log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda w^2 + \\text{const}.$$\n\nWe can now see how our selection of prior for $w$ as normal distribution results in L2 regularization. In the above equation, $w^2$ is the squared L2-norm of the vector $w$ with $\\lambda$ controlling the strength of regularization.\n\nOften, we seek only a point estimate of $w$ instead of the full posterior distribution. One solution is to take the mode of this posterior as the estimate of $w$; this approach is called *maximum a posteriori* (MAP) estimation. MAP estimation differs from MLE in the fact that we incorporate prior knowledge about $w$.\n\n## 3. L1 Regularization\n\nWe'll first need to look at a distribution called the Laplace distribution. With $w$ as the random variable, it is given by\n\n$$ g(w|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|w - \\mu|}{b}\\right)$$\n\nand $\\mu, b$ are referred to as *location parameter* and *diversity* respectively. \n\nNow let us assume that the prior distribution on $w$ is a Laplace distribution with location parameter $\\mu = 0$ and $b = \\lambda^{-1}$. The unnormalized posterior distribution in this case becomes:\n\n$$P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot \\frac{\\lambda}{2} \\exp\\left(-\\lambda |w|\\right).$$\n\nTaking logarithm on both sides,\n\n$$log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda \\cdot |w| + \\text{const}.$$\n\nHence with Laplace distribution as prior on $w$, we arrive at L1 regularization. Again, $\\lambda$ controls the strength of regularization.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","theme":"cosmo","title-block-banner":true,"title":"A Probabilistic Perspective on Regularization","date":"2023-11-08","author":"Pankaj Pansari"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}