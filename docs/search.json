[
  {
    "objectID": "posts/threading-thoughts/index.html",
    "href": "posts/threading-thoughts/index.html",
    "title": "Some Observations on Locks and Threads",
    "section": "",
    "text": "Consider the simple multi-threaded program here; each thread increments the shared global variable counter max number of times (we won’t write a program this way to accomplish this, but it’s good to highlight the issue). Global variable is one way to share data between threads. Can we now refactor the program to make counter a local variable in main()?\nTo answer this question, we first need to think how we’re going to now pass 2 arguments to the thread - the identifier string (\"A\" or \"B\") and a pointer to counter. This is relatively easy - we simply package them in a user-defined struct and pass a pointer to the struct to the thread.\ntypedef struct {\n    char *id;\n    volatile int counter;\n    int max;\n} thread_args;\nWe know that threads share the same address space, but each thread has its own stack. Can a thread access a struct variable stored on the stack of the main thread? This issue did not arise when originally we were passing the string literal \"A\" or \"B\" because they are stored in the read-only part of the address space; this segment is shared by all threads.\nThe answer is technically yes, another thread can access the struct variable stored on the stack of the main thread. This is possible here only because main() uses pthread_join() for proper sequencing that makes stack data sharing safe. So when a child thread p1 or p2 executes, the stack frame of main() is guaranteed to to exist. The refactored program would look something like this.\n\n\n\nAddress space of program with main thread + 2 child threads"
  },
  {
    "objectID": "posts/threading-thoughts/index.html#thread-stacks",
    "href": "posts/threading-thoughts/index.html#thread-stacks",
    "title": "Some Observations on Locks and Threads",
    "section": "",
    "text": "Consider the simple multi-threaded program here; each thread increments the shared global variable counter max number of times (we won’t write a program this way to accomplish this, but it’s good to highlight the issue). Global variable is one way to share data between threads. Can we now refactor the program to make counter a local variable in main()?\nTo answer this question, we first need to think how we’re going to now pass 2 arguments to the thread - the identifier string (\"A\" or \"B\") and a pointer to counter. This is relatively easy - we simply package them in a user-defined struct and pass a pointer to the struct to the thread.\ntypedef struct {\n    char *id;\n    volatile int counter;\n    int max;\n} thread_args;\nWe know that threads share the same address space, but each thread has its own stack. Can a thread access a struct variable stored on the stack of the main thread? This issue did not arise when originally we were passing the string literal \"A\" or \"B\" because they are stored in the read-only part of the address space; this segment is shared by all threads.\nThe answer is technically yes, another thread can access the struct variable stored on the stack of the main thread. This is possible here only because main() uses pthread_join() for proper sequencing that makes stack data sharing safe. So when a child thread p1 or p2 executes, the stack frame of main() is guaranteed to to exist. The refactored program would look something like this.\n\n\n\nAddress space of program with main thread + 2 child threads"
  },
  {
    "objectID": "posts/threading-thoughts/index.html#locks-and-a-concurrency-bug",
    "href": "posts/threading-thoughts/index.html#locks-and-a-concurrency-bug",
    "title": "Some Observations on Locks and Threads",
    "section": "2. Locks and a Concurrency Bug",
    "text": "2. Locks and a Concurrency Bug\nWe use locks when updating shared variables or memory locations in a multi-threaded program. This provides mutual exclusion and is crucial to the correctness of the program. What happens when we make a function call within a critical section protected by a lock, such as here?\nThe function do_something() is called from within the critical section protected by a lock. This will behave just fine. A thread will execute do_something() while still holding the lock. It’s not necessary that the compiler will inline the function within the critical section. So jumps from the lines delineated within lock() and unlock() are possible, provided we come back to the same critical section. This is not ideal, because we usually try to keep critical sections small and protect only the lines where shared memory is updated.\nA serious concurrency bug occurs when we have a goto statement within a critical section and it jumps to a location outside that section. In this case, unlock() is not called by the executing thread; other threads never get a chance to acquire the lock and wait perpetually.\npthread_mutex_lock(&lock);\nbalance = balance + 1;\n\nif (some_condition)\n    goto outside_lock; \n\npthread_mutex_unlock(&lock);\n\noutside_lock:\n    do_something();\nIt’s a nasty bug which can be difficult to diagnose. The compiler will certainly not flag this, but static analysis tools can. Moral of the story: avoid goto entirely in concurrent code.\n\nThanks to Subham and Moti Rattan for posing doubts in my OS class; those led to these observations."
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html",
    "href": "posts/ML-research-vs-production/index.html",
    "title": "Machine Learning in Research versus Production",
    "section": "",
    "text": "I have been going through the book ‘Designing Machine Learning Systems’ by Chip Huyen to better understand how machine learning systems are deployed in production in industry. Coming from a research background, it’s been a good experience to learn the difference in emphasis and priorities between how machine learning is practised in research versus in production. Here, I summarize some of my learnings on this topic:\n\n\nResearch: The number of stakeholders on a research project is small and they are usually aligned in terms of the project objective. The objective is often to achieve state-of-the-art performace on benchmark datasets. Model complexity is usually not an issue if it helps to eke out a tiny percentage of improvement on peformance metrics.\nProduction: There are often many stakeholders and their requirements can often be different and conflicting. It’s usually not possible to design an ML system satisfying all requirements, so a compromise must be reached. The value of performance gain is case-dependent. At times, a tiny improvement can save a lot of money for a business. However, often a small improvement does not justify the increase in model complexity.\n\n\n\n\n\n\n\nModel development code is a small part of the full codebase needed for deployment. Adapted from Sculley et al. 1\n\n\nResearch: A research group focuses most of its effort on model development. Due to the need to train many different models, research prioritizes fast training. In other words, high throughput is the desired quality in an ML system where throughput is the number of prediction requests that can be processed by the system in unit time.\nProduction: When deploying a model in production, the surrounding infrastructure of data pipelines, resource management, and servers has to be constructed and this takes a lot of time and effort. Once deployed, the focus is now on fast inference. Hence, we seek low latency in this case where latency is the amount of average waiting time before a prediction request is processed.\n\n\n\nResearch: The benchmark datasets in research are usually clean, static, and well-formatted. The anomalies in the dataset, if present, are usually known and often open-source code to process the datasets is available.\nProduction By contrast, data in real-world systems can be noisy, unstructured, and its distribution can shift with time. There may be bias in the data. Labels may be sparse, imbalanced, or incorrect. The data may not be available as a complete dataset but may arrive over time in form of a stream as it gets generated by the user. Finally, one has to respect user privacy when dealing with their data.\n\n\n\nResearch: Fairness is rarely given consideration in a research setting where the agenda is achieving state-of-the-art accuracy. Even when fairness is considered, it is an afterthought. This also has to do with lack of meaningful fairness metrics.\nProduction: A machine learning system deployed in the real-world can have influence on society members as a result of the decisions it outputs. Giving strong emphasis to fairness can ensure that no section of society gets adversely affected by ML models and that all sources of bias have been dealt with. Unfortunately, much progress remains to be done on this front as a lot of models deployed for loan applications, predictive policing, employee recruitement still discriminate against minority groups.\n\n\n\nInterpretability is the property of an ML system to explain the rationale behind the decision it makes or the output it gives. If a system is interpretable, one can peek inside the model for better understanding; in case there is some mistake/anomaly, one can pinpoint with confidence where the model is going wrong and debug accordingly.\nResearch: With ML research being evaluated on a single objective, that of model performace, there is no incentive to ensure that the model is interpretable.\nProduction: Intrepretability is a key requirement for real-world ML systems. For us to deploy an ML system in society, we need to be able to trust it. This trust is highly dependent on the system being interpretable. In case of misbehavior, we want to be able to diagnose and rectify our model.\n\n\n\nResearch: In research, one often focuses solely on the model development part. The benchmark dataset is static. There is no monitoring to be done once good results on the benchmark dataset have been achieved. Advances in software and hardware, and accumulated common wisdom have made this process relative fast and cheap.\nProduction: The deployment of ML systems is still relatively fast. Once deployed, an ML system needs to be constantly monitored and maintained and this is the hard part. There are some aspects of ML systems that make them much more challenging to monitor and maintain as compared to traditional software systems, particularly the dependence of model performance on external data. At a high level, one needs to constantly ensure that the data pipeline does not get broken, the incoming data is sensible, and the test data distribution does not differ considerably from training distribution. In case of distribution drift, one needs to retrain the model."
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html#footnotes",
    "href": "posts/ML-research-vs-production/index.html#footnotes",
    "title": "Machine Learning in Research versus Production",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSculley, David, et al. “Hidden technical debt in machine learning systems.” Advances in neural information processing systems 28 (2015).↩︎"
  },
  {
    "objectID": "posts/inference-profiling/index.html",
    "href": "posts/inference-profiling/index.html",
    "title": "Profiling LLM Inference",
    "section": "",
    "text": "Let’s use PyTorch Profiler to get a better understanding of what happens under the hood during LLM inference. The model we’re going to use is the 1B parameter instruction-tuned version of Gemma3 from HuggingFace model hub. I’m going to make use of RTX 4090 GPU on a RunPod vm instance. The profiling code can be found in this Gist\n\n\n\n\n\n\nNote\n\n\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nThis configures to load the model parameters as 8-bit integers to reduce memory footprint of the model. This is the bulk of the memory usage; hence, lots of space saving.\ninputs = tokenizer.apply_chat_template(...).to(model.device).to(torch.bfloat16)\nWe load the input tensor as bfloat16 as accommodate the wider dynamic range of activations.\n\n\nWe profile inference by wrapping profile() around model.generate():\nwith torch.inference_mode():\n    with profile(activities=[ProfilerActivity.CUDA]) as prof:\n        with record_function(\"model_inference\"):\n            outputs = model.generate(**inputs, max_new_tokens=64)\nProfilerActivity.CUDA is saying that we only want to profile operations on GPU. The detailed output is here. In summary, there are two types of operations on GPU - matrix multiplication (called gemm kernels) and elementwise operations such as normalization, activation function, etc.\n\n\n\nCUDA Kernel\nSelf CUDA %\n\n\n\n\ngemm_kernel\n18%\n\n\nelementwise_kernel\n82%\n\n\n\nObservation 1: The majority of time is spent on elementwise operations. This happens because LLM inference consists of one prefill step and all the rest decode steps. Due to KV-caching, these decode steps involve thin matrix multiplications, thereby under-utilizing the GPU. This is bad news because elementwise operations use particular cores on the GPU that have much lower throughput than the cores used for matrix multiplication. This also implies our second observation.\nObservation 2: To predict each token, all the model parameters have to be shunted from GPU global memory (DRAM/HBM) to shared memory (SRAM). Memory transfer is relatively expensive and to make good use of this expense, we usually try to amortize by doing a lot of computation on the transferred data. This becomes difficult in our scenario where matrix multiplications are thin and elementwise operations are dominant. We say that our inference computation is memory-bound.\nObservation 3: Over the course of inference, the CPU is launching a huge number of CUDA operations for the GPU to perform and the GPU performs each very quickly and waits idly for the next command. Anything happening on the CPU is like a overhead and these arise from slow Python interpreter, layers of dispatch on PyTorch framework, and launching of CUDA kernels for the GPU. As shown by the following numbers from the profiler, CPU overhead is significant in our case.\nSelf CPU time total: 2.502s\nSelf CUDA time total: 1.196s\nIn this case, GPU and CPU are operating synchronously - a bad state of affairs. We can also say we’re overhead-bound.\n\n\n\n(A)synchronous mode of CPU-GPU operation\n\n\nOverall, we can characterize our workload as follows:\n\n\n\nCharacterization of workload\n\n\nSo, what options have we got?\n\n❌ No point optimizing already efficient matmul/sgemm kernels since GPU compute is such a tiny part of the equation.\n✅ Use efficient serving engines like vLLM or SGLang or TGI. They do various QoL improvements such as CUDA graphs, speculative decoding, PagedAttention, and more.\n\n\nThanks to Eric, Jeremy, and Ramjee for helpful discussions."
  },
  {
    "objectID": "posts/Grounding-LLMs/index.html",
    "href": "posts/Grounding-LLMs/index.html",
    "title": "Grounding Language Models in the Physical World",
    "section": "",
    "text": "I recently listened to a podcast episode on The Robot Brains where Jitendra Malik, an eminent computer vision researcher, shared his thoughts and experiences on grounding large language models (LLMs) in the physical world and how to approach this through robotics. I summarize the discussion below:\n\nMoravec’s Paradox\n\nMoravec wrote in 1988, “it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility” 1.\nLLMs have shown enormous success recently on a wide variety of cognitive tasks. Among other benchmark tests, these systems have done well on challenging competitive exams. This gives us a feeling that these systems have acquired impressive intelligence. However, this holds true for tasks involving abstract, higher-level cognition. The challenging problems of locomotion and other desirable motor skills in robotics have not become solved as a result of this progress on LLMs. We seem to be still consistent with Moravec’s Paradox.\n\nEvolutionary Perspective\n\nOn our human evolution journey, brain development followed the development of hands with opposable thumb; hand development in turn followed the development of bipedal walking which left our hands free. We developed sensorimotor skills first, language acquisition is a more recent phenomenon. If we think of human evolution on a 24-hour timeframe, langugage development corresponds only to the final 2-3 minutes. Clearly, all of intelligence cannot be said to reside in those final couple of minutes. Besides, different species of animals have different flavours of intelligence, and these are sophisticated in many cases. These animals do not possess language ability in the conventional sense. Hence, what we can learn from language alone may be inherently limited.\n\nHuman Learning and Development\n\nWe can take inspiration from how babies and children learn. Babies interact with the world around them in a multi-modal way, using different senses. They gradually learn to manipulate objects and perform small experiments of their own. The acquisition of words at this stage is grounded in physical objects and interactions with them. So for instance when a mother says ‘this is a ball’, there is a visual input of the ball along with the motor desire to throw or catch a ball.\nWhen children go to school after the age of 5 and acquire knowledge through books, they already have a basic understanding of the world, people, and objects on which to build upon. This points to the need for a multi-modal and staged process of learning (curriculum learning).\n\nEmbodied AI\n\nA new paradigm of intelligent models can be robots equipped with vision, touch, audio sensors with the ability to move, manipulate objects and interact with the real-world. As the robot learns more about the dynamics of the world, we can teach it basics of language. The rest of langugage acquisition will happen in an in-context manner, by combining atomic concepts, much like children do after age 5.\n\nRapid Adaptation of Motor Skills\n\nThere is a compelling argument for this approach of acquiring atomic concepts that are grounded in the world followed by general language acquisition and development of abstract thinking. Any robotic system needs to be adaptable and robust. For instance, a robot must be able to walk on diverse, unseen terrains. The robot must be capable of learning new policies quickly, with the time-frame being task dependent. For example, a walking robot needs to learn to stabilize on a new terrain in about a second else the robot will fall. Just like humans need only a handful of examples to pick up a new concept, we can hope our ML systems modeled on lines of human learning and development, will be able to quickly adapt with a small number of simulations/trial and error."
  },
  {
    "objectID": "posts/Grounding-LLMs/index.html#footnotes",
    "href": "posts/Grounding-LLMs/index.html#footnotes",
    "title": "Grounding Language Models in the Physical World",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia contributors. “Moravec’s paradox.” Wikipedia.↩︎"
  },
  {
    "objectID": "posts/Code_insight/index.html",
    "href": "posts/Code_insight/index.html",
    "title": "Automated Qualitative Feedback on Programming Assignments",
    "section": "",
    "text": "I’ve been teaching courses on Computer Systems and Operating Systems at Plaksha University. Programming assignments are an integral part of such systems courses. The challenge comes in evaluating the large number of submissions. One can write scripts to score the correctness of code, perhaps by test cases; even performance measures such as execution time or cache memory hits/misses can be easily captured. The harder part is giving feedback on the quality of the code. Usually, TAs annotate the submission indicating how readable or well-organized the program is and whether it makes use of good language conventions. This calls for good programming expertise from the TAs themselves; a lot of effort is also required to give such feedback for a large class.\nI wrote a tool called CodeInsight to provide such feedback automatically. It makes use of a suite of state-of-the-art LLMs in an agent-based architecture. Given a problem statement and an assignment program, the tool returns an annotated version of the program, inserting comments at appropriate points, either to suggest improvement or to commend good implementation; the original code remains as it is. The parameters are cleanliness, language conventions, organization, data structures, and use of pointers (for C). It goes without saying that program correctness comes first and foremost; this tool does not check for that.\nWhile state-of-the-art models are good at code understanding, they do make mistakes. If let’s say Claude-3.5-Sonnet is asked to provide feedback in our desired format, three cases can arise:\n\nprovides the correct feedback at the appropriate point\nprovides feedback at some point, but that isn’t quite correct or relevant. For instance, it may suggest some optimization that may be valid but isn’t required as part of the assignment\nfails to identify a bad practice where it actually occurs\n\nBesides, different LLMs have different strengths. ChatGPT 4.0 may correctly pick up some points that Claude missed.\nConsidering these, it seemed appropriate to me to use an agent-based approach. Specifically, I used a multi-agent collaboration design pattern. Andrew Ng puts the idea succinctly: “More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would”. The architecture looks like the following:\n\n\n\nFig. 1 - CodeInsight architecture\n\n\nProposers, aggregators, annotators, and comparators are role names given to LLMs hinting the function they carry out. More details can be found in the home page of the GitHub repository. I’d just like to mention that there are actually 4 proposers (rather than 3 in the figure) and they are distinct models - Claude-3.5-Sonnet, GPT-4o, Mixtral-8x22B, and Llama-3.1-405B. Also in the spirit of breaking down tasks, the proposer-aggregator loop is run once for every evaluation parameter - readability, organization, language convention, etc. Finally, I found that it was critical for the prompts to be detailed and accurate. I made use of this excellent documentation on programming assignment grading from a Stanford course.\nThere are two challenges with such an agent-based approach - overall system latency and API costs. The first is not an issue for us since we use the tool offline on a batch of assignments. The cost can be a concern when the number of assignments is large. I don’t have cost estimates yet, but I felt it was important to first make the tool output as good as we can. I have yet to check the degradation of performance if we use, let’s say, 2 proposers instead of 4, or the latest suite of smaller models.\nThe fun part was the system evaluation. I wanted to compare the tool output with sample programs annotated by a good programmer. For the sample programs, I selected 10 C programs written by not-expert programmers (the programs should have scope for us to suggest feedback). I requested two of my good students to annotate those programs with feedback, and also ran the tool on the programs. I let GPT-4o do the comparison and provide a 0 or 1 score based on which of two inputs - tool or human annotation - is better (the comparison prompt was more elaborate). I computed the average win-ratio of the tool for each student. Averaged over both programmers, the win ratio was 5-5. This was encouraging.\nAlthough our tool goes beyond what traditional linters do, it’ll be helpful to incorporate linter output and provide them as part of prompts to proposers. I’d like to do a more exhaustive human comparison testing on more C programs. Once I use it for some assignments in my course, hopefully students’ feedback will give me pointers to improve the tool."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Elegant Tensors: Attention Forward & Backward Passes using PyTorch Einsum/Einops\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nA Closer Look at Helgrind for Concurrency Issues\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nSome Observations on Locks and Threads\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nProfiling LLM Inference\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nThroughput vs Latency - GPU vs CPU\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Qualitative Feedback on Programming Assignments\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nGrounding Language Models in the Physical World\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nA Probabilistic Perspective on Regularization\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in Research versus Production\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Large Language Models\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Visiting Faculty member in the Computer Science and Artificial Intelligence division of Plaksha University, where I teach courses on Computer Systems and Operating Systems. My current research interests lie in benchmarking reasoning abilities of LLMs.\nI completed my PhD under Dr. Pawan Mudigonda in the OVAL group at University of Oxford, where I worked on optimization problems arising in machine learning. I’ve been a Research Scientist at Naver Labs Europe in Grenoble, France. I also worked as a Data Analyst at General Electric in Bangalore, India.\nI completed my undergraduate studies at the Indian Institute of Technology, Kharagpur.\nCurriculum Vitae: pdf\nEmail: pankaj.pansari1@proton.me"
  },
  {
    "objectID": "posts/Bayesian-inference/index.html",
    "href": "posts/Bayesian-inference/index.html",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "",
    "text": "Regularization is a common technique in machine learning to prevent overfitting. The two most widely used regularizers are the L2 and L1 norms. In this post, we look at how there regularizers can be thought of as being derived from prior distributions on the parameters we’re estimating."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#background",
    "href": "posts/Bayesian-inference/index.html#background",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "1. Background",
    "text": "1. Background\n\nModel Complexity\nOur intuition is that simpler models are preferable; models that are excessively complicated lead to overfitting. Let \\(L_{\\mathcal D}({\\bf w})\\) measure the misfit of model with parameters \\(\\bf w\\) to a given dataset \\(\\mathcal D\\). We can augment the loss function to penalize more complex models:\n\\[ L({\\bf w}) = L_{\\mathcal D}({\\bf w}) + \\lambda L_{reg}(\\bf w)\\]\nThis is called regularization in machine learning and shrinkage in statistics. \\(\\lambda\\) is called the regularization coefficient and controls the trade-off between fitting the dataset \\(\\mathcal D\\) well and giving simpler, more generalizable model.\nThere are two ways to think of model complexity:\n\nmodel complexity as a function of weights of all the features in a model\n\nA feature weight with a high absolute value is more complex than a feature weight with low absolute value. In this case,\n\\[L_{reg}({\\bf w}) = {\\bf w}^T{\\bf w} = ||{\\bf w}||^2 = w_1^2 + w_2^2 + \\dots + w_n^2\\]\nThis is known as L2 regularization or weight decay. Optimizing loss function with L2 regularization is generally easier and results in small parameter values. The resulting model is known as ridge regression.\n\nmodel complexity as a function of the total number of features with nonzero weights\n\nTo achieve this, we don’t set our regularization penaly as the number of nonzero parameters - this makes the optimization difficult. Instead, we use L1 regularization:\n\\[L_{reg}({\\bf w}) = ||{\\bf w}||_1^2 = |w_1| + |w_2| + \\dots + |w_n|\\]\nThis has the same effect of setting only some parameter values to be nonzero, and is convex hence easier to optimize. The resulting model is known as Lasso.\n\n\nMaximum Likelihood Estimation\nWe are going to make extensive use of the Bayes’ theorem. Let \\(w\\) be the parameter we want to estimate and \\({\\mathcal D} = (x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\) be the dataset. Then, we have:\n\\[P(w|{\\mathcal D}) = \\frac{P({\\mathcal D}|w) \\cdot P(w)}{P({\\mathcal D})}.\n\\]\nIn the above equation,\n\n\\(P(w)\\) is the prior distribution on the parameters \\(w\\); this encodes our belief about what the parameter values should likely be before we have looked at any data.\n\\(P(\\mathcal{D}|w)\\) is the likelihood of \\(\\mathcal{D}\\) given some assignment of \\(w\\).\n\\(P(w|\\mathcal{D})\\) is the posterior distribution of the parameters \\(w\\) after the dataset \\(\\mathcal D\\) is known.\n\nLet’s say we want to estimate parameter \\(w\\) from an observed dataset \\(\\mathcal D\\). We assume the relation between \\(x\\) and \\(y\\) as\n\\[y = f(x ; w) + \\epsilon\\]\nwhere \\(\\epsilon\\) is noise drawn from a Gaussian distribution with mean \\(0\\) and variance \\(\\sigma^2\\). This results in a likelihood that is Gaussian distribution:\n\\[P({\\mathcal D}|w) = {\\mathcal N}(y|f(x ; w), \\sigma^2)\\]\nLet us assume that the samples in \\(\\mathcal D\\) are independently and identically distributed (i.i.d). Under this assumption and taking logarithms for ease of computation, the likelihood value now becomes:\n\\[\\log P({\\mathcal D}|w) = \\sum_{i = 1}^{N} \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2).\\]\nWe can estimate parameter \\(w\\) by maximizing the above quantity. This is called maximum likelihood estimation (MLE). Often time, this method of estimating \\(w\\) is called a frequentist approach, in constrast to the Bayesian approach we discuss below."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l2-regularization",
    "href": "posts/Bayesian-inference/index.html#l2-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "2. L2 Regularization",
    "text": "2. L2 Regularization\nWe note that the denominator in the Bayes’ theorem does not depend on the parameters \\(w\\) we want to estimate; hence we can ignore that term. We define the unnormalized posterior distribution as\n\\[P'(w|{\\mathcal D}) = P({\\mathcal D}|w) \\cdot P(w)\\].\nLet us now see what happens when we introduce prior distribution on parameter \\(w\\). In the first case, let us assume that \\(w\\) follows a Gaussian distribution \\({\\mathcal N}(w|0, \\lambda^{-1})\\) where \\(\\lambda\\) is a strictly positive scalar. We then have:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot {\\mathcal N}(w|0, \\lambda^{-1})\\].\nTaking logarithm on both sides, we obtain:\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda w^2 + \\text{const}.\\]\nWe can now see how our selection of prior for \\(w\\) as normal distribution results in L2 regularization. In the above equation, \\(w^2\\) is the squared L2-norm of the vector \\(w\\) with \\(\\lambda\\) controlling the strength of regularization.\nOften, we seek only a point estimate of \\(w\\) instead of the full posterior distribution. One solution is to take the mode of this posterior as the estimate of \\(w\\); this approach is called maximum a posteriori (MAP) estimation. MAP estimation differs from MLE in the fact that we incorporate prior knowledge about \\(w\\)."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l1-regularization",
    "href": "posts/Bayesian-inference/index.html#l1-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "3. L1 Regularization",
    "text": "3. L1 Regularization\nWe’ll first need to look at a distribution called the Laplace distribution. With \\(w\\) as the random variable, it is given by\n\\[ g(w|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|w - \\mu|}{b}\\right)\\]\nand \\(\\mu, b\\) are referred to as location parameter and diversity respectively.\nNow let us assume that the prior distribution on \\(w\\) is a Laplace distribution with location parameter \\(\\mu = 0\\) and \\(b = \\lambda^{-1}\\). The unnormalized posterior distribution in this case becomes:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot \\frac{\\lambda}{2} \\exp\\left(-\\lambda |w|\\right).\\]\nTaking logarithm on both sides,\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda \\cdot |w| + \\text{const}.\\]\nHence with Laplace distribution as prior on \\(w\\), we arrive at L1 regularization. Again, \\(\\lambda\\) controls the strength of regularization."
  },
  {
    "objectID": "posts/einsum/index.html",
    "href": "posts/einsum/index.html",
    "title": "Elegant Tensors: Attention Forward & Backward Passes using PyTorch Einsum/Einops",
    "section": "",
    "text": "I was looking at the FlashAttention-2 paper recently. It’s about optimizing the forward and backward pass through the attention layer - this being the bottleneck to scaling transformers for longer sequence lengths. As a start, I implemented the forward and backward passes of the standard (unoptimized) attention module in PyTorch but with einsum and einops instead of view, reshape, transpose, permute, and matmul which I habitually use.\n\nEinsum and Einops\nI found einsum and einops to be very useful and elegant for working on tensors - a core requirement of deep learning. One ends up writing more readable and less error-prone PyTorch code with einsum (based on Einstein summation) and einops (general tensor manipulation using Einstein notation). Moreover, einsum + einops can lead to faster and more memory-efficient implementations, especially by potentially fusing operations; however, I need to investigate the efficient part more.\nThere are excellent tutorials and introductions that will get you started with einsum/einops in no time - 1, 2, 3\nAs a motivating example, consider a vector A(3) and another matrix B(3, 4). We want to multiply each element of vector A with the corresponding row of matrix B (element-wise), and then sum the results along each row to get a final vector. In PyTorch, we can do\n((A[:, torch.newaxis] * B).sum(axis=1))\nwhere \\(*\\) denotes element-wise product. A bit tedious, isn’t it? With einsum, we simply do:\ntorch.einsum('i,ij -&gt; i', A, B)\nEinsum is a little limited in its functionality. That is why, we also use einops which can be thought of as an extension of einsum. Whereas einsum mostly permits reduce-sum type of operations, einops is very convenient for adding dimensions (repeat), performing general reduce (whether max, sum, mean), and providing a new view of the tensor (rearrange). Taken together, one can do almost all tensor computations using einsum/einops.\n\n\nImplementing Attention Forward Pass using Einsum/Einops\nConsider the implementation of standard single-head attention from here:\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = scores.softmax(dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\nWith einsum, the same implementation is more concise and it took me less time and effort to implement. Note that our implementation below works for multi-head attention, but for comparison with above reference code, we used a single head (\\(h = 1\\)). Moreover, we don’t make use of mask or dropout in the above reference implementation for comparison.\ndef forward(self, q, k, v):\n    # I'm using notation from flash-attention2 paper\n    S = torch.einsum('bihd,bjhd -&gt; bhij', q, k) / self.scale\n    self.P = F.softmax(S, dim = -1)\n    O = torch.einsum('bhin,bnhj -&gt; bihj', self.P, v)\n    return O\nTo test, we simply pass random q, k, v of appropriate shapes to both implementations and compare the output.\n\n\nBackprop through Softmax\nTo implement our own backward pass of the attention module (so that we can later optimize it), one can use the equations in Section 2.2 of the FlashAttention-2 paper. It’s straightforward matrix calculus. One confusion may be symbols are in shorthand format - hence \\(\\mathbf dO\\) stands for \\(\\frac{\\partial L}{\\partial O}\\) - that is, the partial derivative of loss function \\(L\\) with respect to \\(\\mathbf O\\), and it has the same shape as \\(\\mathbf O\\). Same for \\(\\mathbf dV, dP, dS, dQ, dK\\).\nThe derivation of softmax Jacobian may need some explanation (refer Section 2.2 of paper). Given \\({\\mathbf S} \\in {\\mathbb R}^{N \\times N}\\), the attention matrix is computed as:\n\\({\\mathbf P} = softmax({\\mathbf S}) \\in {\\mathbb R}^{N \\times N}\\)\nwhere the softmax is applied row-wise to \\(\\mathbf S\\). Given \\({\\mathbf dP} \\in {\\mathbb R}^{N \\times N}\\), we want to derive \\({\\mathbf dS} \\in {\\mathbb R}^{N \\times N}\\). First, let’s consider one row of \\(\\mathbf S\\) and \\(\\mathbf P\\) and denote them by \\(\\mathbf s\\) and \\(\\mathbf p\\) respectively. The corresponding rows of \\(\\mathbf dS\\) and \\(\\mathbf dP\\) are denoted by \\(\\mathbf ds\\) and \\(\\mathbf dp\\). By chain rule, we have\n\\({\\mathbf ds} = J^T \\cdot {\\mathbf dp}\\)\nwhere \\(J\\) is the Jacobian matrix of \\(\\frac{\\partial \\mathbf p}{\\partial \\mathbf s}\\). Let \\({\\mathbf p} = [p_1, p_2, \\dots , p_N]\\). \\(p_i\\) is computed as \\(\\frac{\\exp(s_i)}{\\sum_j {\\exp(s_j)}}\\). The entries of the Jacobian matrix are given by\n\\(J_{ii} = \\frac{\\exp(s_i)}{\\sum_j {\\exp(s_j)}} - \\frac{\\exp(2s_i)}{(\\sum_j \\exp(s_j))^2} = p_i - p_i^2\\)\nand for non-diagonal elements of \\(J\\), we have\n\\(J_{ij} = - \\frac{\\exp(s_i + s_j)}{(\\sum_j \\exp(s_j))^2} = - p_i \\cdot p_j\\)\nThe Jacobian can be written more concisely as\n\\(J = {\\text diag}({\\mathbf p}) - {\\mathbf p}{\\mathbf p}^T\\)\nwhere diag() is the matrix formed by placing elements of \\({\\mathbf p}\\) on the diagonal; the second term is the outer product of \\(\\mathbf p\\). In our case, \\(J\\) is symmetric, hence\n\\({\\mathbf ds} = ({\\text diag}({\\mathbf p}) - {\\mathbf p}{\\mathbf p}^T) \\cdot {\\mathbf dp}\\)\nTo generalize from rows of \\({\\mathbf P}, {\\mathbf S}\\) to full matrices, we’ll manipulate the above expression a bit:\n\\({\\mathbf ds} = ({\\text diag}({\\mathbf p}) \\cdot {\\mathbf dp}- {\\mathbf p}{\\mathbf p}^T \\cdot {\\mathbf dp})\\)\nThe above can be written in terms of the elementwise-product (\\(*\\)):\n\\({\\mathbf ds} = {\\mathbf p} * {\\mathbf dp}- {\\mathbf p} * ({\\mathbf p}^T \\cdot {\\mathbf dp})\\)\nThe dot product \\(({\\mathbf p}^T \\cdot {\\mathbf dp})\\) is a scalar. Assuming broadcasting, we can write\n\\({\\mathbf ds} = {\\mathbf p} * ({\\mathbf dp}- ({\\mathbf p}^T \\cdot {\\mathbf dp}))\\)\nThe above formulation was for single rows of \\(\\mathbf S, P, dS, dP\\), but viewed as column vectors. Generalizing to full matrices, we derive\n\\({\\mathbf dS} = {\\mathbf P} * ({\\mathbf dP} - \\operatorname{dot\\_prod})\\)\nwhere \\(\\operatorname{dot\\_prod} = \\sum_k P[i, k] * dP[i, k]\\) is the dot product in the last dimension between matrices \\(\\mathbf P\\) and \\(\\mathbf dP\\). The above works because \\(\\operatorname{dot\\_prod}\\) will get broadcasted to dimension of \\(\\mathbf P\\); for that, we need to insert an extra dimension at the end of \\(\\operatorname{dot\\_prod}\\), else PyTorch will complain about dimension mismatch. One can check the correctness of our own backward() implementation by comparing output with that given by autograd. The full implementation is available in this notebook.\nIn summary, we implemented forward and backward passes of standard attention module as an exercise in using einsum and einops, and also because it provides us a starting point for hardware optimizations to speed up attention for long context lengths. Along the way, we saw that tensor formulation of backprop through softmax needed some matrix manipulations.\n\n\n\n\n\n\nNote\n\n\n\nBackpropagation is amazing since it lets us reuse computation, resulting in gradient computation expense comparable to forward pass (within a constant factor). Autograd is great since it speeds up implementation by freeing us from the need to write gradient code. However, notice how autograd required saving intermediate computational states (in our example, we saved the \\(\\bf P\\) matrix; see notebook). That’s a load on limited GPU memory.\nA fun exercise: why do you think topological sort is needed during backprop?\n\n\n\nThanks to Eric for introduction to new concepts."
  },
  {
    "objectID": "posts/helgrind_atomics/index.html",
    "href": "posts/helgrind_atomics/index.html",
    "title": "A Closer Look at Helgrind for Concurrency Issues",
    "section": "",
    "text": "Concurrency issues in multi-threaded programs are notorious to track down. Helgrind is a dynamic analysis tool, part of the Valgrind suite, that helps in detecting such errors for C/C++ programs.\nWe are going to understand Helgrind a little better through the lens of two classes of concurrency errors - data race and deadlock due to lock ordering. There is another class of errors that Helgrind can help with - incorrect usage of the pthreads library; we’ll not consider that here.\n\n\nHelgrind first makes a note of memory locations accessed by more than more thread. For each such memory location, Helgrind asks: is there at least one lock used by all threads to protect that location?. If the answer to that question is no, Helgrind reports a data race.\n\n\n\nHow Helgrind detects data races?\n\n\nWith this understanding, we are ready to look at a simple example. Consider the following 2 threaded program here with unprotected access to a shared variable global_var. Clearly, there’s a data race condition. However, this error may not manifest itself during every execution of the program - let’s say thread p starts running and completes immediately after it’s created. However, the way Helgrind detects data races is by keeping internal data structures which let it answer the above question.\nAnother way of restating our lock question is that Helgrind asks: are accesses to shared memory addresses in happens-before relation? With respect to this version of the program, now using lock to avoid data race condition, we can say that whichever thread acquires the lock first, its operation on global_var happen-before the operation of the thread that acquires the lock second. It’s another way of framing of the desired atomicity property.\nOne thing to be careful is that Helgrind cannot detect potential data races in code paths that aren’t executed during the analysis run. So, for instance, for this program if we always give 0 as command line argument, Helgrind won’t be able to located the unprotected decrement to global_var in the else branch.\n\n\n\nHelgrind detects inconsistent lock ordering by maintaining a directed graph indicating the order in which locks have been acquired - if there’s a cycle in the graph, Helgrind will report the inconsistent lock ordering. This implies that even if make use of a meta gate-lock, such as here, to hide the inconsistent lock ordering, Helgrind will still be able to detect the deadlock and will complain. Some may think that it’s a false positive and that Helgrind is not able to operate at a higher-level to figure that the gate lock, in reality, protects the lock ordering. But I think there’s a good reason for Helgrind to still report this - what if there’s some code refactoring and someone decides to get rid of the meta-gate lock g? In any case, one may simply choose to ignore the warning if they are sure about what they’re doing.\nLock destruction: The pthreads API provides a function pthread_mutex_destroy to destroy a mutex. Normally it’s a good practice to destroy any mutex or conditional variable we’ve created. If we fail to do so, Helgrind will not complain. For small programs, it isn’t important and even for larger ones it may not be a big deal. However, if the API provides a way to destroy an entity we’ve created, it’s a good practice to use it.\n\nCode in the associated gist has been borrowed and modified from Operating Systems: Three Easy Pieces by Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau. Thanks to Usman, Arbaaz, and Mukundan for the interesting discussion on these topics."
  },
  {
    "objectID": "posts/helgrind_atomics/index.html#helgrind",
    "href": "posts/helgrind_atomics/index.html#helgrind",
    "title": "A Closer Look at Helgrind for Concurrency Issues",
    "section": "",
    "text": "Concurrency issues in multi-threaded programs are notorious to track down. Helgrind is a dynamic analysis tool, part of the Valgrind suite, that helps in detecting such errors for C/C++ programs.\nWe are going to understand Helgrind a little better through the lens of two classes of concurrency errors - data race and deadlock due to lock ordering. There is another class of errors that Helgrind can help with - incorrect usage of the pthreads library; we’ll not consider that here.\n\n\nHelgrind first makes a note of memory locations accessed by more than more thread. For each such memory location, Helgrind asks: is there at least one lock used by all threads to protect that location?. If the answer to that question is no, Helgrind reports a data race.\n\n\n\nHow Helgrind detects data races?\n\n\nWith this understanding, we are ready to look at a simple example. Consider the following 2 threaded program here with unprotected access to a shared variable global_var. Clearly, there’s a data race condition. However, this error may not manifest itself during every execution of the program - let’s say thread p starts running and completes immediately after it’s created. However, the way Helgrind detects data races is by keeping internal data structures which let it answer the above question.\nAnother way of restating our lock question is that Helgrind asks: are accesses to shared memory addresses in happens-before relation? With respect to this version of the program, now using lock to avoid data race condition, we can say that whichever thread acquires the lock first, its operation on global_var happen-before the operation of the thread that acquires the lock second. It’s another way of framing of the desired atomicity property.\nOne thing to be careful is that Helgrind cannot detect potential data races in code paths that aren’t executed during the analysis run. So, for instance, for this program if we always give 0 as command line argument, Helgrind won’t be able to located the unprotected decrement to global_var in the else branch.\n\n\n\nHelgrind detects inconsistent lock ordering by maintaining a directed graph indicating the order in which locks have been acquired - if there’s a cycle in the graph, Helgrind will report the inconsistent lock ordering. This implies that even if make use of a meta gate-lock, such as here, to hide the inconsistent lock ordering, Helgrind will still be able to detect the deadlock and will complain. Some may think that it’s a false positive and that Helgrind is not able to operate at a higher-level to figure that the gate lock, in reality, protects the lock ordering. But I think there’s a good reason for Helgrind to still report this - what if there’s some code refactoring and someone decides to get rid of the meta-gate lock g? In any case, one may simply choose to ignore the warning if they are sure about what they’re doing.\nLock destruction: The pthreads API provides a function pthread_mutex_destroy to destroy a mutex. Normally it’s a good practice to destroy any mutex or conditional variable we’ve created. If we fail to do so, Helgrind will not complain. For small programs, it isn’t important and even for larger ones it may not be a big deal. However, if the API provides a way to destroy an entity we’ve created, it’s a good practice to use it.\n\nCode in the associated gist has been borrowed and modified from Operating Systems: Three Easy Pieces by Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau. Thanks to Usman, Arbaaz, and Mukundan for the interesting discussion on these topics."
  },
  {
    "objectID": "posts/LLM-intro/index.html",
    "href": "posts/LLM-intro/index.html",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts.\n\n\n\n\n\nThe essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence “I am going to the market”, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don’t provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‘encoded’ embeddings. In each operation of that sequence, we allow the embeddings of all words to ‘interact’ and influence each other. The effect is that the new set of ‘encoded embeddings’ encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output. The softmax function normalizes the values in a vector to a probability distribution, bringing higher values closer to 1 and lower values to 0.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt’s clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs.\n\n\n\n\n\nA machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it’s better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let’s say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task.\n\n\n\n\n\n\n\nA lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There’s been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don’t clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#introduction",
    "href": "posts/LLM-intro/index.html#introduction",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts."
  },
  {
    "objectID": "posts/LLM-intro/index.html#background",
    "href": "posts/LLM-intro/index.html#background",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "The essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence “I am going to the market”, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don’t provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‘encoded’ embeddings. In each operation of that sequence, we allow the embeddings of all words to ‘interact’ and influence each other. The effect is that the new set of ‘encoded embeddings’ encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output. The softmax function normalizes the values in a vector to a probability distribution, bringing higher values closer to 1 and lower values to 0.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt’s clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs."
  },
  {
    "objectID": "posts/LLM-intro/index.html#large-language-models",
    "href": "posts/LLM-intro/index.html#large-language-models",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it’s better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let’s say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task."
  },
  {
    "objectID": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "href": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There’s been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don’t clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#footnotes",
    "href": "posts/LLM-intro/index.html#footnotes",
    "title": "An Introduction to Large Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBender, Emily M., et al. “On the dangers of stochastic parrots: Can language models be too big?🦜.” Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2021.↩︎\nBubeck, Sébastien, et al. “Sparks of artificial general intelligence: Early experiments with gpt-4.” arXiv preprint arXiv:2303.12712 (2023).↩︎\nAkyürek, Ekin, et al. “What learning algorithm is in-context learning? investigations with linear models.” arXiv preprint arXiv:2211.15661 (2022).↩︎\nLiu, Yang, et al. “Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment.” arXiv preprint arXiv:2308.05374 (2023).↩︎\nShumailov, Ilia, et al. “The Curse of Recursion: Training on Generated Data Makes Models Forget” arXiv preprint 2305.17493↩︎"
  },
  {
    "objectID": "posts/throughtput_latency/index.html",
    "href": "posts/throughtput_latency/index.html",
    "title": "Throughput vs Latency - GPU vs CPU",
    "section": "",
    "text": "CPUs are optimized for latency; GPUs are optimized for throughput. They are independent processors, so can and should work on different things at the same time.\nLatency - Time taken to complete a single operation or instruction; commonly measured in clock cycles. Lower latency is obviously desirable. Note that latency can be talked of for CPU/GPU computation or memory access - they are two different things. The value of latency of an instruction depends on the type of instruction (integer/floating-point/SIMD). Here we are not referring to a particular type of instruction.\nThroughput - Number of operations or instructions completed per unit of time. Again, throughput can refer to computation or memory access. By convention, when we say throughput, we mean computation throughput and that is measured typically in FLOPS (Floating Point Operations Per Second). Higher throughput is obviously better.\nThe analogue of throughput for memory is called bandwidth. It’s the maximum amount of data transfer that can happen per unit time and these days it’s typically measured in GB/s. Note that bandwidth encompasses the memory device (DRAM/SRAM etc.) as well as the bus speeds.\nSo here we are - we want higher throughput and lower latency. It may seem like lowering latency would automatically increase throughput and that’s true in theory; in practice there are constraints and trade-offs. We have to decide whether it’s more critical for us to have latency-optimized or throughput-optimized system. Optimizing for one practically leads to sub-optimal performance for the other.\n\nAnalogy\nConsider an analogy: we want to get from Secretariat in New Delhi (green) to Dwarka (red). We can take either the metro which takes about 75 minutes or use a car which takes 45 minutes. The metro takes longer for us (higher latency) but it transports a lot more people in let’s say an hour (higher throughput). So, the metro is optimized for throughput and the car is optimized for latency. Note that a car is more flexible and can take us to places where the metro doesn’t go.\n\n\n\nMetro vs Car\n\n\nCPUs are the cars - optimized for latency. Context switching between threads is expensive. So the CPU makes an individual thread as fast as possible. By CPU here, we actually mean both the processor and the memory system. Focusing on the processor, there are complex things like branch prediction, out of order execution, prefetching to make the latency low - this makes the cores complex and takes up real-estate on chip. Hence, we only have a few cores on a CPU. The memory hierarchy is elaborate with multiple levels to optimize for latency.\nGPUs, on the other hand, are the metros. The cores are simple and hence there can be a lot of them. The focus is on massive parallelism, so throughput is high. Latency for an individual thread may not be great, but for GPU workloads that’s not so important. Even the GPU memory has simpler organization and wider buses to optimize for throughput. Such a system, like a metro, works best when it’s oversubscribed; there is a deep queue of pending threads to be executed.\n\n\nAsynchrony\nCPUs and GPUs are independent processors. They can and should work on different things at the same time. The following is a bad model of how to use a CPU and a GPU together:\n\n\n\nSynchronous\n\n\nNote how GPU does some computation and then waits for something for CPU - this happens repeatedly. It’s as if the CPU and GPU are operating synchronously. In this case, a much better flow is the following:\n\n\n\nAsynchronous\n\n\nIn the above, the CPU issues computation to GPU and then starts working on its own thing separately. There is good utilization of both processors. We say that they are operating asynchronously - we’re aiming for this.\n\nThis post was inspired by part of Steve Jones’ talk at GTC 2021. Thanks to Eric for telling me about it."
  }
]