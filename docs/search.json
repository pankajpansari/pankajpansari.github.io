[
  {
    "objectID": "posts/threading-thoughts/index.html",
    "href": "posts/threading-thoughts/index.html",
    "title": "Some Observations on Locks and Threads",
    "section": "",
    "text": "Consider the simple multi-threaded program here; each thread increments the shared global variable counter max number of times (we won’t write a program this way to accomplish this, but it’s good to highlight the issue). Global variable is one way to share data between threads. Can we now refactor the program to make counter a local variable in main()?\nTo answer this question, we first need to think how we’re going to now pass 2 arguments to the thread - the identifier string (\"A\" or \"B\") and a pointer to counter. This is relatively easy - we simply package them in a user-defined struct and pass a pointer to the struct to the thread.\ntypedef struct {\n    char *id;\n    volatile int counter;\n    int max;\n} thread_args;\nWe know that threads share the same address space, but each thread has its own stack. Can a thread access a struct variable stored on the stack of the main thread? This issue did not arise when originally we were passing the string literal \"A\" or \"B\" because they are stored in the read-only part of the address space; this segment is shared by all threads.\nThe answer is technically yes, another thread can access the struct variable stored on the stack of the main thread. This is possible here only because main() uses pthread_join() for proper sequencing that makes stack data sharing safe. So when a child thread p1 or p2 executes, the stack frame of main() is guaranteed to to exist. The refactored program would look something like this.\n\n\n\nAddress space of program with main thread + 2 child threads"
  },
  {
    "objectID": "posts/threading-thoughts/index.html#thread-stacks",
    "href": "posts/threading-thoughts/index.html#thread-stacks",
    "title": "Some Observations on Locks and Threads",
    "section": "",
    "text": "Consider the simple multi-threaded program here; each thread increments the shared global variable counter max number of times (we won’t write a program this way to accomplish this, but it’s good to highlight the issue). Global variable is one way to share data between threads. Can we now refactor the program to make counter a local variable in main()?\nTo answer this question, we first need to think how we’re going to now pass 2 arguments to the thread - the identifier string (\"A\" or \"B\") and a pointer to counter. This is relatively easy - we simply package them in a user-defined struct and pass a pointer to the struct to the thread.\ntypedef struct {\n    char *id;\n    volatile int counter;\n    int max;\n} thread_args;\nWe know that threads share the same address space, but each thread has its own stack. Can a thread access a struct variable stored on the stack of the main thread? This issue did not arise when originally we were passing the string literal \"A\" or \"B\" because they are stored in the read-only part of the address space; this segment is shared by all threads.\nThe answer is technically yes, another thread can access the struct variable stored on the stack of the main thread. This is possible here only because main() uses pthread_join() for proper sequencing that makes stack data sharing safe. So when a child thread p1 or p2 executes, the stack frame of main() is guaranteed to to exist. The refactored program would look something like this.\n\n\n\nAddress space of program with main thread + 2 child threads"
  },
  {
    "objectID": "posts/threading-thoughts/index.html#locks-and-a-concurrency-bug",
    "href": "posts/threading-thoughts/index.html#locks-and-a-concurrency-bug",
    "title": "Some Observations on Locks and Threads",
    "section": "2. Locks and a Concurrency Bug",
    "text": "2. Locks and a Concurrency Bug\nWe use locks when updating shared variables or memory locations in a multi-threaded program. This provides mutual exclusion and is crucial to the correctness of the program. What happens when we make a function call within a critical section protected by a lock, such as here?\nThe function do_something() is called from within the critical section protected by a lock. This will behave just fine. A thread will execute do_something() while still holding the lock. It’s not necessary that the compiler will inline the function within the critical section. So jumps from the lines delineated within lock() and unlock() are possible, provided we come back to the same critical section. This is not ideal, because we usually try to keep critical sections small and protect only the lines where shared memory is updated.\nA serious concurrency bug occurs when we have a goto statement within a critical section and it jumps to a location outside that section. In this case, unlock() is not called by the executing thread; other threads never get a chance to acquire the lock and wait perpetually.\npthread_mutex_lock(&lock);\nbalance = balance + 1;\n\nif (some_condition)\n    goto outside_lock; \n\npthread_mutex_unlock(&lock);\n\noutside_lock:\n    do_something();\nIt’s a nasty bug which can be difficult to diagnose. The compiler will certainly not flag this, but static analysis tools can. Moral of the story: avoid goto entirely in concurrent code.\n\nThanks to Subham and Moti Rattan for posing doubts in my OS class; those led to these observations."
  },
  {
    "objectID": "posts/LLM-intro/index.html",
    "href": "posts/LLM-intro/index.html",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts.\n\n\n\n\n\nThe essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence “I am going to the market”, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don’t provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‘encoded’ embeddings. In each operation of that sequence, we allow the embeddings of all words to ‘interact’ and influence each other. The effect is that the new set of ‘encoded embeddings’ encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output. The softmax function normalizes the values in a vector to a probability distribution, bringing higher values closer to 1 and lower values to 0.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt’s clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs.\n\n\n\n\n\nA machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it’s better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let’s say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task.\n\n\n\n\n\n\n\nA lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There’s been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don’t clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#introduction",
    "href": "posts/LLM-intro/index.html#introduction",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts."
  },
  {
    "objectID": "posts/LLM-intro/index.html#background",
    "href": "posts/LLM-intro/index.html#background",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "The essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence “I am going to the market”, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don’t provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‘encoded’ embeddings. In each operation of that sequence, we allow the embeddings of all words to ‘interact’ and influence each other. The effect is that the new set of ‘encoded embeddings’ encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output. The softmax function normalizes the values in a vector to a probability distribution, bringing higher values closer to 1 and lower values to 0.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt’s clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs."
  },
  {
    "objectID": "posts/LLM-intro/index.html#large-language-models",
    "href": "posts/LLM-intro/index.html#large-language-models",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it’s better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let’s say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task."
  },
  {
    "objectID": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "href": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There’s been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don’t clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#footnotes",
    "href": "posts/LLM-intro/index.html#footnotes",
    "title": "An Introduction to Large Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBender, Emily M., et al. “On the dangers of stochastic parrots: Can language models be too big?🦜.” Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2021.↩︎\nBubeck, Sébastien, et al. “Sparks of artificial general intelligence: Early experiments with gpt-4.” arXiv preprint arXiv:2303.12712 (2023).↩︎\nAkyürek, Ekin, et al. “What learning algorithm is in-context learning? investigations with linear models.” arXiv preprint arXiv:2211.15661 (2022).↩︎\nLiu, Yang, et al. “Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment.” arXiv preprint arXiv:2308.05374 (2023).↩︎\nShumailov, Ilia, et al. “The Curse of Recursion: Training on Generated Data Makes Models Forget” arXiv preprint 2305.17493↩︎"
  },
  {
    "objectID": "posts/helgrind_atomics/index.html",
    "href": "posts/helgrind_atomics/index.html",
    "title": "A Closer Look at Helgrind for Concurrency Issues",
    "section": "",
    "text": "Concurrency issues in multi-threaded programs are notorious to track down. Helgrind is a dynamic analysis tool, part of the Valgrind suite, that helps in detecting such errors for C/C++ programs.\nWe are going to understand Helgrind a little better through the lens of two classes of concurrency errors - data race and deadlock due to lock ordering. There is another class of errors that Helgrind can help with - incorrect usage of the pthreads library; we’ll not consider that here.\n\n\nHelgrind first makes a note of memory locations accessed by more than more thread. For each such memory location, Helgrind asks: is there at least one lock used by all threads to protect that location?. If the answer to that question is no, Helgrind reports a data race.\n\n\n\nHow Helgrind detects data races?\n\n\nWith this understanding, we are ready to look at a simple example. Consider the following 2 threaded program here with unprotected access to a shared variable global_var. Clearly, there’s a data race condition. However, this error may not manifest itself during every execution of the program - let’s say thread p starts running and completes immediately after it’s created. However, the way Helgrind detects data races is by keeping internal data structures which let it answer the above question.\nAnother way of restating our lock question is that Helgrind asks: are accesses to shared memory addresses in happens-before relation? With respect to this version of the program, now using lock to avoid data race condition, we can say that whichever thread acquires the lock first, its operation on global_var happen-before the operation of the thread that acquires the lock second. It’s another way of framing of the desired atomicity property.\nOne thing to be careful is that Helgrind cannot detect potential data races in code paths that aren’t executed during the analysis run. So, for instance, for this program if we always give 0 as command line argument, Helgrind won’t be able to located the unprotected decrement to global_var in the else branch.\n\n\n\nHelgrind detects inconsistent lock ordering by maintaining a directed graph indicating the order in which locks have been acquired - if there’s a cycle in the graph, Helgrind will report the inconsistent lock ordering. This implies that even if make use of a meta gate-lock, such as here, to hide the inconsistent lock ordering, Helgrind will still be able to detect the deadlock and will complain. Some may think that it’s a false positive and that Helgrind is not able to operate at a higher-level to figure that the gate lock, in reality, protects the lock ordering. But I think there’s a good reason for Helgrind to still report this - what if there’s some code refactoring and someone decides to get rid of the meta-gate lock g? In any case, one may simply choose to ignore the warning if they are sure about what they’re doing.\nLock destruction: The pthreads API provides a function pthread_mutex_destroy to destroy a mutex. Normally it’s a good practice to destroy any mutex or conditional variable we’ve created. If we fail to do so, Helgrind will not complain. For small programs, it isn’t important and even for larger ones it may not be a big deal. However, if the API provides a way to destroy an entity we’ve created, it’s a good practice to use it.\n\nCode in the associated gist has been borrowed and modified from Operating Systems: Three Easy Pieces by Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau. Thanks to Usman, Arbaaz, and Mukundan for the interesting discussion on these topics."
  },
  {
    "objectID": "posts/helgrind_atomics/index.html#helgrind",
    "href": "posts/helgrind_atomics/index.html#helgrind",
    "title": "A Closer Look at Helgrind for Concurrency Issues",
    "section": "",
    "text": "Concurrency issues in multi-threaded programs are notorious to track down. Helgrind is a dynamic analysis tool, part of the Valgrind suite, that helps in detecting such errors for C/C++ programs.\nWe are going to understand Helgrind a little better through the lens of two classes of concurrency errors - data race and deadlock due to lock ordering. There is another class of errors that Helgrind can help with - incorrect usage of the pthreads library; we’ll not consider that here.\n\n\nHelgrind first makes a note of memory locations accessed by more than more thread. For each such memory location, Helgrind asks: is there at least one lock used by all threads to protect that location?. If the answer to that question is no, Helgrind reports a data race.\n\n\n\nHow Helgrind detects data races?\n\n\nWith this understanding, we are ready to look at a simple example. Consider the following 2 threaded program here with unprotected access to a shared variable global_var. Clearly, there’s a data race condition. However, this error may not manifest itself during every execution of the program - let’s say thread p starts running and completes immediately after it’s created. However, the way Helgrind detects data races is by keeping internal data structures which let it answer the above question.\nAnother way of restating our lock question is that Helgrind asks: are accesses to shared memory addresses in happens-before relation? With respect to this version of the program, now using lock to avoid data race condition, we can say that whichever thread acquires the lock first, its operation on global_var happen-before the operation of the thread that acquires the lock second. It’s another way of framing of the desired atomicity property.\nOne thing to be careful is that Helgrind cannot detect potential data races in code paths that aren’t executed during the analysis run. So, for instance, for this program if we always give 0 as command line argument, Helgrind won’t be able to located the unprotected decrement to global_var in the else branch.\n\n\n\nHelgrind detects inconsistent lock ordering by maintaining a directed graph indicating the order in which locks have been acquired - if there’s a cycle in the graph, Helgrind will report the inconsistent lock ordering. This implies that even if make use of a meta gate-lock, such as here, to hide the inconsistent lock ordering, Helgrind will still be able to detect the deadlock and will complain. Some may think that it’s a false positive and that Helgrind is not able to operate at a higher-level to figure that the gate lock, in reality, protects the lock ordering. But I think there’s a good reason for Helgrind to still report this - what if there’s some code refactoring and someone decides to get rid of the meta-gate lock g? In any case, one may simply choose to ignore the warning if they are sure about what they’re doing.\nLock destruction: The pthreads API provides a function pthread_mutex_destroy to destroy a mutex. Normally it’s a good practice to destroy any mutex or conditional variable we’ve created. If we fail to do so, Helgrind will not complain. For small programs, it isn’t important and even for larger ones it may not be a big deal. However, if the API provides a way to destroy an entity we’ve created, it’s a good practice to use it.\n\nCode in the associated gist has been borrowed and modified from Operating Systems: Three Easy Pieces by Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau. Thanks to Usman, Arbaaz, and Mukundan for the interesting discussion on these topics."
  },
  {
    "objectID": "posts/einsum/index.html",
    "href": "posts/einsum/index.html",
    "title": "Attention Forward & Backward Passes using PyTorch Einsum/Einops",
    "section": "",
    "text": "I was looking at the FlashAttention-2 paper recently. It’s about optimizing the forward and backward pass through the attention layer - this being the bottleneck to scaling transformers for longer sequence lengths. As a start, I implemented the forward and backward passes of the standard (unoptimized) attention module in PyTorch but with einsum and einops instead of view, reshape, transpose, permute, and matmul which I habitually use.\n\nEinsum and Einops\nI found einsum and einops to be very useful and elegant for working on tensors - a core requirement of deep learning. One ends up writing more readable and less error-prone PyTorch code with einsum (based on Einstein summation) and einops (general tensor manipulation using Einstein notation). Moreover, einsum + einops can lead to faster and more memory-efficient implementations, especially by potentially fusing operations; however, I need to investigate the efficient part more.\nThere are excellent tutorials and introductions that will get you started with einsum/einops in no time - 1, 2, 3\nAs a motivating example, consider a vector A(3) and another matrix B(3, 4). We want to multiply each element of vector A with the corresponding row of matrix B (element-wise), and then sum the results along each row to get a final vector. In PyTorch, we can do\n((A[:, torch.newaxis] * B).sum(axis=1))\nwhere \\(*\\) denotes element-wise product. A bit tedious, isn’t it? With einsum, we simply do:\ntorch.einsum('i,ij -&gt; i', A, B)\nEinsum is a little limited in its functionality. That is why, we also use einops which can be thought of as an extension of einsum. Whereas einsum mostly permits reduce-sum type of operations, einops is very convenient for adding dimensions (repeat), performing general reduce (whether max, sum, mean), and providing a new view of the tensor (rearrange). Taken together, one can do almost all tensor computations using einsum/einops.\n\n\nImplementing Attention Forward Pass using Einsum/Einops\nConsider the implementation of standard single-head attention from here:\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = scores.softmax(dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\nWith einsum, the same implementation is more concise and it took me less time and effort to implement. Note that our implementation below works for multi-head attention, but for comparison with above reference code, we used a single head (\\(h = 1\\)). Moreover, we don’t make use of mask or dropout in the above reference implementation for comparison.\ndef forward(self, q, k, v):\n    # I'm using notation from flash-attention2 paper\n    S = torch.einsum('bihd,bjhd -&gt; bhij', q, k) / self.scale\n    self.P = F.softmax(S, dim = -1)\n    O = torch.einsum('bhin,bnhj -&gt; bihj', self.P, v)\n    return O\nTo test, we simply pass random q, k, v of appropriate shapes to both implementations and compare the output.\n\n\nBackprop through Softmax\nTo implement our own backward pass of the attention module (so that we can later optimize it), one can use the equations in Section 2.2 of the FlashAttention-2 paper. It’s straightforward matrix calculus. One confusion may be symbols are in shorthand format - hence \\(\\mathbf dO\\) stands for \\(\\frac{\\partial L}{\\partial O}\\) - that is, the partial derivative of loss function \\(L\\) with respect to \\(\\mathbf O\\), and it has the same shape as \\(\\mathbf O\\). Same for \\(\\mathbf dV, dP, dS, dQ, dK\\).\nThe derivation of softmax Jacobian may need some explanation (refer Section 2.2 of paper). Given \\({\\mathbf S} \\in {\\mathbb R}^{N \\times N}\\), the attention matrix is computed as:\n\\({\\mathbf P} = softmax({\\mathbf S}) \\in {\\mathbb R}^{N \\times N}\\)\nwhere the softmax is applied row-wise to \\(\\mathbf S\\). Given \\({\\mathbf dP} \\in {\\mathbb R}^{N \\times N}\\), we want to derive \\({\\mathbf dS} \\in {\\mathbb R}^{N \\times N}\\). First, let’s consider one row of \\(\\mathbf S\\) and \\(\\mathbf P\\) and denote them by \\(\\mathbf s\\) and \\(\\mathbf p\\) respectively. The corresponding rows of \\(\\mathbf dS\\) and \\(\\mathbf dP\\) are denoted by \\(\\mathbf ds\\) and \\(\\mathbf dp\\). By chain rule, we have\n\\({\\mathbf ds} = J^T \\cdot {\\mathbf dp}\\)\nwhere \\(J\\) is the Jacobian matrix of \\(\\frac{\\partial \\mathbf p}{\\partial \\mathbf s}\\). Let \\({\\mathbf p} = [p_1, p_2, \\dots , p_N]\\). \\(p_i\\) is computed as \\(\\frac{\\exp(s_i)}{\\sum_j {\\exp(s_j)}}\\). The entries of the Jacobian matrix are given by\n\\(J_{ii} = \\frac{\\exp(s_i)}{\\sum_j {\\exp(s_j)}} - \\frac{\\exp(2s_i)}{(\\sum_j \\exp(s_j))^2} = p_i - p_i^2\\)\nand for non-diagonal elements of \\(J\\), we have\n\\(J_{ij} = - \\frac{\\exp(s_i + s_j)}{(\\sum_j \\exp(s_j))^2} = - p_i \\cdot p_j\\)\nThe Jacobian can be written more concisely as\n\\(J = {\\text diag}({\\mathbf p}) - {\\mathbf p}{\\mathbf p}^T\\)\nwhere diag() is the matrix formed by placing elements of \\({\\mathbf p}\\) on the diagonal; the second term is the outer product of \\(\\mathbf p\\). In our case, \\(J\\) is symmetric, hence\n\\({\\mathbf ds} = ({\\text diag}({\\mathbf p}) - {\\mathbf p}{\\mathbf p}^T) \\cdot {\\mathbf dp}\\)\nTo generalize from rows of \\({\\mathbf P}, {\\mathbf S}\\) to full matrices, we’ll manipulate the above expression a bit:\n\\({\\mathbf ds} = ({\\text diag}({\\mathbf p}) \\cdot {\\mathbf dp}- {\\mathbf p}{\\mathbf p}^T \\cdot {\\mathbf dp})\\)\nThe above can be written in terms of the elementwise-product (\\(*\\)):\n\\({\\mathbf ds} = {\\mathbf p} * {\\mathbf dp}- {\\mathbf p} * ({\\mathbf p}^T \\cdot {\\mathbf dp})\\)\nThe dot product \\(({\\mathbf p}^T \\cdot {\\mathbf dp})\\) is a scalar. Assuming broadcasting, we can write\n\\({\\mathbf ds} = {\\mathbf p} * ({\\mathbf dp}- ({\\mathbf p}^T \\cdot {\\mathbf dp}))\\)\nThe above formulation was for single rows of \\(\\mathbf S, P, dS, dP\\), but viewed as column vectors. Generalizing to full matrices, we derive\n\\({\\mathbf dS} = {\\mathbf P} * ({\\mathbf dP} - \\operatorname{dot\\_prod})\\)\nwhere \\(\\operatorname{dot\\_prod} = \\sum_k P[i, k] * dP[i, k]\\) is the dot product in the last dimension between matrices \\(\\mathbf P\\) and \\(\\mathbf dP\\). The above works because \\(\\operatorname{dot\\_prod}\\) will get broadcasted to dimension of \\(\\mathbf P\\); for that, we need to insert an extra dimension at the end of \\(\\operatorname{dot\\_prod}\\), else PyTorch will complain about dimension mismatch. One can check the correctness of our own backward() implementation by comparing output with that given by autograd. The full implementation is available in this notebook.\nIn summary, we implemented forward and backward passes of standard attention module as an exercise in using einsum and einops, and also because it provides us a starting point for hardware optimizations to speed up attention for long context lengths. Along the way, we saw that tensor formulation of backprop through softmax needed some matrix manipulations.\n\n\n\n\n\n\nNote\n\n\n\nBackpropagation is amazing since it lets us reuse computation, resulting in gradient computation expense comparable to forward pass (within a constant factor). Autograd is great since it speeds up implementation by freeing us from the need to write gradient code. However, notice how autograd required saving intermediate computational states (in our example, we saved the \\(\\bf P\\) matrix; see notebook). That’s a load on limited GPU memory.\nA fun exercise: why do you think topological sort is needed during backprop?\n\n\n\nThanks to Eric for introduction to new concepts."
  },
  {
    "objectID": "posts/Code_insight/index.html",
    "href": "posts/Code_insight/index.html",
    "title": "Automated Qualitative Feedback on Programming Assignments",
    "section": "",
    "text": "I’ve been teaching courses on Computer Systems and Operating Systems at Plaksha University. Programming assignments are an integral part of such systems courses. The challenge comes in evaluating the large number of submissions. One can write scripts to score the correctness of code, perhaps by test cases; even performance measures such as execution time or cache memory hits/misses can be easily captured. The harder part is giving feedback on the quality of the code. Usually, TAs annotate the submission indicating how readable or well-organized the program is and whether it makes use of good language conventions. This calls for good programming expertise from the TAs themselves; a lot of effort is also required to give such feedback for a large class.\nI wrote a tool called CodeInsight to provide such feedback automatically. It makes use of a suite of state-of-the-art LLMs in an agent-based architecture. Given a problem statement and an assignment program, the tool returns an annotated version of the program, inserting comments at appropriate points, either to suggest improvement or to commend good implementation; the original code remains as it is. The parameters are cleanliness, language conventions, organization, data structures, and use of pointers (for C). It goes without saying that program correctness comes first and foremost; this tool does not check for that.\nWhile state-of-the-art models are good at code understanding, they do make mistakes. If let’s say Claude-3.5-Sonnet is asked to provide feedback in our desired format, three cases can arise:\n\nprovides the correct feedback at the appropriate point\nprovides feedback at some point, but that isn’t quite correct or relevant. For instance, it may suggest some optimization that may be valid but isn’t required as part of the assignment\nfails to identify a bad practice where it actually occurs\n\nBesides, different LLMs have different strengths. ChatGPT 4.0 may correctly pick up some points that Claude missed.\nConsidering these, it seemed appropriate to me to use an agent-based approach. Specifically, I used a multi-agent collaboration design pattern. Andrew Ng puts the idea succinctly: “More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would”. The architecture looks like the following:\n\n\n\nFig. 1 - CodeInsight architecture\n\n\nProposers, aggregators, annotators, and comparators are role names given to LLMs hinting the function they carry out. More details can be found in the home page of the GitHub repository. I’d just like to mention that there are actually 4 proposers (rather than 3 in the figure) and they are distinct models - Claude-3.5-Sonnet, GPT-4o, Mixtral-8x22B, and Llama-3.1-405B. Also in the spirit of breaking down tasks, the proposer-aggregator loop is run once for every evaluation parameter - readability, organization, language convention, etc. Finally, I found that it was critical for the prompts to be detailed and accurate. I made use of this excellent documentation on programming assignment grading from a Stanford course.\nThere are two challenges with such an agent-based approach - overall system latency and API costs. The first is not an issue for us since we use the tool offline on a batch of assignments. The cost can be a concern when the number of assignments is large. I don’t have cost estimates yet, but I felt it was important to first make the tool output as good as we can. I have yet to check the degradation of performance if we use, let’s say, 2 proposers instead of 4, or the latest suite of smaller models.\nThe fun part was the system evaluation. I wanted to compare the tool output with sample programs annotated by a good programmer. For the sample programs, I selected 10 C programs written by not-expert programmers (the programs should have scope for us to suggest feedback). I requested two of my good students to annotate those programs with feedback, and also ran the tool on the programs. I let GPT-4o do the comparison and provide a 0 or 1 score based on which of two inputs - tool or human annotation - is better (the comparison prompt was more elaborate). I computed the average win-ratio of the tool for each student. Averaged over both programmers, the win ratio was 5-5. This was encouraging.\nAlthough our tool goes beyond what traditional linters do, it’ll be helpful to incorporate linter output and provide them as part of prompts to proposers. I’d like to do a more exhaustive human comparison testing on more C programs. Once I use it for some assignments in my course, hopefully students’ feedback will give me pointers to improve the tool."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html",
    "href": "posts/Bayesian-inference/index.html",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "",
    "text": "Regularization is a common technique in machine learning to prevent overfitting. The two most widely used regularizers are the L2 and L1 norms. In this post, we look at how there regularizers can be thought of as being derived from prior distributions on the parameters we’re estimating."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#background",
    "href": "posts/Bayesian-inference/index.html#background",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "1. Background",
    "text": "1. Background\n\nModel Complexity\nOur intuition is that simpler models are preferable; models that are excessively complicated lead to overfitting. Let \\(L_{\\mathcal D}({\\bf w})\\) measure the misfit of model with parameters \\(\\bf w\\) to a given dataset \\(\\mathcal D\\). We can augment the loss function to penalize more complex models:\n\\[ L({\\bf w}) = L_{\\mathcal D}({\\bf w}) + \\lambda L_{reg}(\\bf w)\\]\nThis is called regularization in machine learning and shrinkage in statistics. \\(\\lambda\\) is called the regularization coefficient and controls the trade-off between fitting the dataset \\(\\mathcal D\\) well and giving simpler, more generalizable model.\nThere are two ways to think of model complexity:\n\nmodel complexity as a function of weights of all the features in a model\n\nA feature weight with a high absolute value is more complex than a feature weight with low absolute value. In this case,\n\\[L_{reg}({\\bf w}) = {\\bf w}^T{\\bf w} = ||{\\bf w}||^2 = w_1^2 + w_2^2 + \\dots + w_n^2\\]\nThis is known as L2 regularization or weight decay. Optimizing loss function with L2 regularization is generally easier and results in small parameter values. The resulting model is known as ridge regression.\n\nmodel complexity as a function of the total number of features with nonzero weights\n\nTo achieve this, we don’t set our regularization penaly as the number of nonzero parameters - this makes the optimization difficult. Instead, we use L1 regularization:\n\\[L_{reg}({\\bf w}) = ||{\\bf w}||_1^2 = |w_1| + |w_2| + \\dots + |w_n|\\]\nThis has the same effect of setting only some parameter values to be nonzero, and is convex hence easier to optimize. The resulting model is known as Lasso.\n\n\nMaximum Likelihood Estimation\nWe are going to make extensive use of the Bayes’ theorem. Let \\(w\\) be the parameter we want to estimate and \\({\\mathcal D} = (x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\) be the dataset. Then, we have:\n\\[P(w|{\\mathcal D}) = \\frac{P({\\mathcal D}|w) \\cdot P(w)}{P({\\mathcal D})}.\n\\]\nIn the above equation,\n\n\\(P(w)\\) is the prior distribution on the parameters \\(w\\); this encodes our belief about what the parameter values should likely be before we have looked at any data.\n\\(P(\\mathcal{D}|w)\\) is the likelihood of \\(\\mathcal{D}\\) given some assignment of \\(w\\).\n\\(P(w|\\mathcal{D})\\) is the posterior distribution of the parameters \\(w\\) after the dataset \\(\\mathcal D\\) is known.\n\nLet’s say we want to estimate parameter \\(w\\) from an observed dataset \\(\\mathcal D\\). We assume the relation between \\(x\\) and \\(y\\) as\n\\[y = f(x ; w) + \\epsilon\\]\nwhere \\(\\epsilon\\) is noise drawn from a Gaussian distribution with mean \\(0\\) and variance \\(\\sigma^2\\). This results in a likelihood that is Gaussian distribution:\n\\[P({\\mathcal D}|w) = {\\mathcal N}(y|f(x ; w), \\sigma^2)\\]\nLet us assume that the samples in \\(\\mathcal D\\) are independently and identically distributed (i.i.d). Under this assumption and taking logarithms for ease of computation, the likelihood value now becomes:\n\\[\\log P({\\mathcal D}|w) = \\sum_{i = 1}^{N} \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2).\\]\nWe can estimate parameter \\(w\\) by maximizing the above quantity. This is called maximum likelihood estimation (MLE). Often time, this method of estimating \\(w\\) is called a frequentist approach, in constrast to the Bayesian approach we discuss below."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l2-regularization",
    "href": "posts/Bayesian-inference/index.html#l2-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "2. L2 Regularization",
    "text": "2. L2 Regularization\nWe note that the denominator in the Bayes’ theorem does not depend on the parameters \\(w\\) we want to estimate; hence we can ignore that term. We define the unnormalized posterior distribution as\n\\[P'(w|{\\mathcal D}) = P({\\mathcal D}|w) \\cdot P(w)\\].\nLet us now see what happens when we introduce prior distribution on parameter \\(w\\). In the first case, let us assume that \\(w\\) follows a Gaussian distribution \\({\\mathcal N}(w|0, \\lambda^{-1})\\) where \\(\\lambda\\) is a strictly positive scalar. We then have:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot {\\mathcal N}(w|0, \\lambda^{-1})\\].\nTaking logarithm on both sides, we obtain:\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda w^2 + \\text{const}.\\]\nWe can now see how our selection of prior for \\(w\\) as normal distribution results in L2 regularization. In the above equation, \\(w^2\\) is the squared L2-norm of the vector \\(w\\) with \\(\\lambda\\) controlling the strength of regularization.\nOften, we seek only a point estimate of \\(w\\) instead of the full posterior distribution. One solution is to take the mode of this posterior as the estimate of \\(w\\); this approach is called maximum a posteriori (MAP) estimation. MAP estimation differs from MLE in the fact that we incorporate prior knowledge about \\(w\\)."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l1-regularization",
    "href": "posts/Bayesian-inference/index.html#l1-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "3. L1 Regularization",
    "text": "3. L1 Regularization\nWe’ll first need to look at a distribution called the Laplace distribution. With \\(w\\) as the random variable, it is given by\n\\[ g(w|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|w - \\mu|}{b}\\right)\\]\nand \\(\\mu, b\\) are referred to as location parameter and diversity respectively.\nNow let us assume that the prior distribution on \\(w\\) is a Laplace distribution with location parameter \\(\\mu = 0\\) and \\(b = \\lambda^{-1}\\). The unnormalized posterior distribution in this case becomes:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot \\frac{\\lambda}{2} \\exp\\left(-\\lambda |w|\\right).\\]\nTaking logarithm on both sides,\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda \\cdot |w| + \\text{const}.\\]\nHence with Laplace distribution as prior on \\(w\\), we arrive at L1 regularization. Again, \\(\\lambda\\) controls the strength of regularization."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an Assistant Professor in the Computer Science and Artificial Intelligence division of Plaksha University, where I teach courses on Computer Systems and Operating Systems. My research interests lie in improving reasoning abilities of LLMs and on speeding up inference and training of LLMs.\nI completed my PhD under Dr. Pawan Mudigonda in the OVAL group at University of Oxford, where I worked on optimization problems arising in machine learning. Post-PhD, I was a Research Scientist at Naver Labs Europe in Grenoble, France. I also worked as Data Analyst at General Electric in Bangalore, India.\nI completed my undergraduate studies at the Indian Institute of Technology, Kharagpur.\nEmail: pankaj.pansari1@proton.me"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Vector Addition - A First Look at CUDA\n\n\n\n\n\nA simple program let’s us learn a surprising amount about CUDA and GPU\n\n\n\n\n\nJun 12, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\ncode-critique\n\n\n\n\n\nLLM-based tool for automated feedback on code quality\n\n\n\n\n\nJun 4, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek in the Uncanny Valley of Paradoxes\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nYashika Mittal, Aman Paliwal, Pankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nAttention Forward & Backward Passes using PyTorch Einsum/Einops\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nA Closer Look at Helgrind for Concurrency Issues\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nSome Observations on Locks and Threads\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nProfiling LLM Inference\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nThroughput vs Latency - GPU vs CPU\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Qualitative Feedback on Programming Assignments\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nGrounding Language Models in the Physical World\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nA Probabilistic Perspective on Regularization\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in Research versus Production\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Large Language Models\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nA Simple Painting Style Classifier\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nVanilla Transformers\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/code-critique/index.html",
    "href": "posts/code-critique/index.html",
    "title": "code-critique",
    "section": "",
    "text": "GitHub repo: https://github.com/pankajpansari/code-critique\n\nMotivation\nProgramming assignments are first and foremost tested for correctness, usually by a suite of test cases. It’s also a good opportunity to give feedback to students on the quality of their implementation. Good code should be readable, follow language convention, have good design, make judicious use of data structures, and manage memory properly (for C programs). It’s not often easy to give such feedback individually to all students due to time and resource constraints. Hence, I wrote code-critique for my own use-case. The students found the feedback from the tool helpful and I thought it may be of use to the broader CS teaching community.\ncode-critique is a command-line tool that makes use of LLMs in a specific pipeline, working with code diffs and using linter tool, to annotate the original submission with feedback inserted as comments at appropriate points. It works really well when the assignment involves making addition/modifications to an existing repository, for example implementing a system call in an educational OS (like xv6).\n\n\n\nWhy not just use a linter?\nBoth code-critique and linter are static code-analysis tool. Linters check for adherence to language convention and also flag possible bugs or security issues, so they address part of our aim. However, linters are rule-based and struggle with pointing out issues in design of program and whether descriptive comments and names are being used. The biggest issue is that linters don’t take into account the problem statement into consideration. This is not to say that linters cannot help us; in fact, we integrate clang-tidy output in our pipeline.\n\n\nDesign Choices\n\nReflection - We ask an LLM model to give feedback on code submission in context of problem statement and what we consider good programming practices. We’re likely to obtain better results if we ask the LLM to go through its response and refine or correct it. We assign the role of Proposer to the first LLM call and Reviewer to the second LLM.\nLinter integration - We run clang-tidy linter on our C program submission (you can use a suitable linter for your language). To prevent issue repetitions and lengthy pathnames from confusing the Reviewer LLM, we obtain a compact summary of linter output via an LLM that we term Summarizer. We leave it to Reviewer to formulate proper corresponding comment annotations and situate them via line numbers.\n\n\n\n\n\n\n\n\n\n\nDirect output of clang-tidy\n\n\n\n\n\n\n\nSummary of clang-tidy output\n\n\n\n\n\nIn my implementation, I did not use linter for repo submission since I evaluated assignments on xv6 OS codebase. Kernel code is not very amenable to linter analysis due to particular programming conventions.\n\nStructured output - It’s easier for us if both Proposer and Reviewer give us their output as a list of feedback annotations, where each item of list has two components - line number where the comment should go, and the actual comment. It’s more compact. It’s also helpful because we can take in the Reviewer structured output and treat it like comment diffs - we take the original code and simply add in the comments at the respective line numbers. This way, we don’t modify the original implementation.\n\n\n\n\nStructured output format for Proposer and Reviewer LLMs\n\n\nLLM model selection - I used o4-mini as Proposer and Reviewer LLMs and gpt-4.1 as Summarizer. I’d recommend using a decent reasoning model (doesn’t have to be the most expensive) as Proposer-Reviewer, and a very good non-reasoning model as Summarizer. I used OpenAI API, but structured output is also supported by Gemini API; you can use Gemini models as well. The API syntax for Gemini is a wee different.\n\n\nOutput\nSingle-file submission: In this case, we want one output program file where the code is exactly as in the original submission. Feedback annotations are inserted at appropriate points, and start with REVIEW:. At the end, we have a Summary - this makes it easier to give feedback on global high-level design and also avoids cluttering the code section with feedback for repetitive issue.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;fcntl.h&gt;\n\n#define MAX_ARGS 64       // Max arguments per command\n#define MAX_PATHS 64      // Max paths in search path\n#define MAX_COMMANDS 64   // Max parallel commands\n\n\n/* \n * REVIEW: `paths` and `path_count` are declared as globals, which increases coupling and \n * hinders testing. Consider encapsulating shell state in a struct passed to \n * functions, or at minimum mark these variables `static` to limit their linkage.\n */\nchar *paths[MAX_PATHS];\nint path_count = 0;\n\nvoid change_directory(char **args) {\n    if (args[1] == NULL || args[2] != NULL) {\n\n/* \n * REVIEW: The literal length `22` is repeated in each `write` call for the error message. \n * Define a single `const char error_message[] = \"An error has occurred\\n\";` and \n * use `sizeof error_message` to avoid mismatches and improve maintainability.\n */\n        write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n        return;\n    }\n    if (chdir(args[1]) != 0) {\n        write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n    }\n}\n\nvoid update_path(char **args) {\n    for (int i = 0; i &lt; path_count; i++) {\n        free(paths[i]);\n    }\n    path_count = 0;\n\n    int i = 1;\n    while (args[i] != NULL && path_count &lt; MAX_PATHS) {\n        paths[path_count] = strdup(args[i]);\n        path_count++;\n        i++;\n    }\n}\n\nint run_command(char **args) {\n    if (path_count == 0) {\n        write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n        return -1;\n    }\n\n    for (int i = 0; i &lt; path_count; i++) {\n\n/* \n * REVIEW: Using a fixed 256-byte buffer for `full_path` can overflow if the directory plus \n * command name exceeds this length. Consider using `PATH_MAX` or dynamically \n * allocating exactly the needed size.\n */\n        char full_path[256];\n        snprintf(full_path, sizeof(full_path), \"%s/%s\", paths[i], args[0]);\n\n        if (access(full_path, X_OK) == 0) {\n            execv(full_path, args);\n            return -1;\n        }\n    }\n\n    write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n    return -1;\n}\n\nvoid clean_input(char *input) {\n\n/* \n * REVIEW: The local buffer `cleaned[1024]` can overflow if `input` exceeds 1023 \n * characters. Since `getline` may return arbitrarily long lines, either \n * dynamically size `cleaned` or enforce a bounds check before writing into it.\n */\n    char cleaned[1024];\n    int i = 0, j = 0;\n\n/* \n * REVIEW: You assign `strlen(input)` (a `size_t`) to an `int len`. This narrowing \n * conversion can overflow on large inputs. Use `size_t` for `len` to match the \n * return type of `strlen`.\n */\n    int len = strlen(input);\n\n    while (i &lt; len) {\n        if (input[i] == '&gt;' || input[i] == '&') {\n            if (j &gt; 0 && cleaned[j - 1] != ' ') {\n                cleaned[j++] = ' ';\n            }\n            cleaned[j++] = input[i];\n            if (i + 1 &lt; len && input[i + 1] != ' ') {\n                cleaned[j++] = ' ';\n            }\n        } else {\n            cleaned[j++] = input[i];\n        }\n        i++;\n    }\n    cleaned[j] = '\\0';\n\n/* \n * REVIEW: `strcpy(input, cleaned)` is unbounded and risks buffer overflows. Use a bounded \n * copy such as `strlcpy` (if available) or `snprintf` to ensure you don’t exceed \n * `input`’s allocated size.\n */\n    strcpy(input, cleaned);\n}\n\nint split_input(char *input, char **args, char **output_file) {\n    int argc = 0;\n    char *token;\n    int redirect_found = 0;\n    int no_more_args = 0;\n\n    clean_input(input);\n\n\n/* \n * REVIEW: `strsep(&input, ...)` modifies the `input` pointer itself, which can make later \n * debugging harder and might surprise readers. Consider working on a duplicate \n * pointer or using `strtok_r` if you need to preserve the original buffer.\n */\n    while ((token = strsep(&input, \" \\t\\n\")) != NULL) {\n        if (strlen(token) == 0) {\n            continue;\n        }\n\n        if (no_more_args) {\n            write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n            return -1;\n        }\n\n        if (strcmp(token, \"&gt;\") == 0) {\n            if (redirect_found || argc == 0) {\n                write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n                return -1;\n            }\n            redirect_found = 1;\n        } else if (redirect_found) {\n            *output_file = token;\n            redirect_found = 0;\n            no_more_args = 1;\n        } else {\n            args[argc++] = token;\n        }\n    }\n\n    if (redirect_found) {\n        write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n        return -1;\n    }\n\n    args[argc] = NULL;\n    return argc;\n}\n\nvoid execute_command(char *command) {\n    char *args[MAX_ARGS];\n    char *output_file = NULL;\n\n    int argc = split_input(command, args, &output_file);\n\n    if (argc == -1 || argc == 0) {\n        return;\n    }\n\n    if (strcmp(args[0], \"exit\") == 0) {\n        if (argc &gt; 1) {\n            write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n        } else {\n            exit(0);\n        }\n    } else if (strcmp(args[0], \"cd\") == 0) {\n        change_directory(args);\n    } else if (strcmp(args[0], \"path\") == 0) {\n        update_path(args);\n    } else {\n        pid_t pid = fork();\n        if (pid == 0) {\n            if (output_file != NULL) {\n\n/* \n * REVIEW: The file-creation mode `0644` is a magic constant. Define a macro or constant \n * like `#define OUTPUT_MODE 0644` to document its meaning and avoid repetition.\n */\n                int fd = open(output_file, O_WRONLY | O_CREAT | O_TRUNC, 0644);\n                if (fd &lt; 0) {\n                    write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n                    exit(1);\n                }\n                dup2(fd, STDOUT_FILENO);\n                dup2(fd, STDERR_FILENO);\n                close(fd);\n            }\n            if (run_command(args) == -1) {\n                exit(1);\n            }\n        } else if (pid &gt; 0) {\n            wait(NULL);\n        } else {\n            write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n        }\n    }\n}\n\n\n/* \n * REVIEW: `main` handles initialization, the prompt loop, parsing, execution, and cleanup, \n * giving it high cognitive complexity. Break it into smaller functions (e.g., \n * `run_interactive()`, `run_batch()`, `cleanup()`) to improve readability and \n * testability.\n */\nint main(int argc, char **argv) {\n    char *line = NULL;\n    size_t len = 0;\n    ssize_t read;\n    FILE *input_file = stdin;\n\n    paths[path_count++] = strdup(\"/bin\");\n\n    if (argc == 2) {\n        input_file = fopen(argv[1], \"r\");\n        if (input_file == NULL) {\n            write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n            exit(1);\n        }\n    } else if (argc &gt; 2) {\n        write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n        exit(1);\n    }\n\n    while (1) {\n        char *commands[MAX_COMMANDS];\n        int command_count = 0;\n\n        if (input_file == stdin) {\n            printf(\"wish&gt; \");\n            fflush(stdout);\n        }\n\n        read = getline(&line, &len, input_file);\n        if (read == -1) {\n            break;\n        }\n\n        clean_input(line);\n\n        int contains_ampersand = 0;\n        for (int i = 0; line[i] != '\\0'; i++) {\n            if (line[i] == '&') {\n                contains_ampersand = 1;\n                break;\n            }\n        }\n\n        if (contains_ampersand) {\n            char *token;\n            while ((token = strsep(&line, \"&\")) != NULL) {\n                if (strlen(token) &gt; 0) {\n                    commands[command_count++] = strdup(token);\n                }\n            }\n\n            pid_t pids[MAX_COMMANDS];\n            for (int i = 0; i &lt; command_count; i++) {\n                pid_t pid = fork();\n                if (pid == 0) {\n                    execute_command(commands[i]);\n                    exit(0);\n                } else if (pid &gt; 0) {\n                    pids[i] = pid;\n                } else {\n                    write(STDERR_FILENO, \"An error has occurred\\n\", 22);\n                }\n            }\n\n            for (int i = 0; i &lt; command_count; i++) {\n                waitpid(pids[i], NULL, 0);\n            }\n\n            for (int i = 0; i &lt; command_count; i++) {\n                free(commands[i]);\n            }\n        } else {\n            execute_command(line);\n        }\n    }\n\n    for (int i = 0; i &lt; path_count; i++) {\n        free(paths[i]);\n    }\n    if (input_file != stdin) {\n        fclose(input_file);\n    }\n    free(line);\n    return 0;\n}\n\n/*\n *\n * STRENGTHS: \n *The implementation cleanly separates built-in commands (`exit`, `cd`, `path`), \n * parsing, and execution; correctly uses `fork`/`execv`; handles I/O redirection \n * with `dup2`; supports parallel commands; frees dynamic memory; and maintains \n * consistent indentation and naming conventions with appropriate standard library \n * calls.\n *\n * AREAS FOR IMPROVEMENT: \n *Eliminate or guard fixed-size buffers (`full_path`, `cleaned`) to prevent \n * overflows; replace unsafe APIs (`strcpy`) and magic literals (`22`, `256`, \n * `1024`, `0644`) with named constants; encapsulate global state; check return \n * values of `snprintf`, `fflush`, `fclose`; reduce duplication in error handling \n * and fork/wait logic; and decompose large functions into smaller, focused units.\n *\n * OVERALL ASSESSMENT: \n *A solid, functional shell implementation covering the required features. \n * Addressing buffer-safety issues, magic numbers, and refactoring for modularity \n * will enhance robustness and maintainability.\n*/\nRepository Submission: In these assignments, students start with a base repository and add functionality on top of it. For example, students may implement system calls in the xv6 repo as part of an OS course. This involves either modifications to existing files in source repo and new files added. We use diff to find out all such changes and run a pipeline similar to that for single-file submission in a loop for all such modified/added files. We want a single output file here that collates results for all such modified/added files in submitted repo. To situate a feedback comment, we place it under respective program filename and line number.\n/*============================threadtest.c=========================================/*\n\n/* \n * REVIEW: The include of \"fs.h\" is not used in this program. Removing unnecessary headers \n * can reduce compilation time and improve clarity.\n */\nline 5: | + #include \"fs.h\"     // For T_DIR, etc if needed by other includes, not directly by test\n\n/* \n * REVIEW: Indentation here is 5 spaces, but the project style recommends 2–4 spaces per \n * level. Please adjust to match the agreed indentation width.\n */\nline 18:  | +     for (int i = 0; i &lt; NUM_INCREMENTS; i++) {\n\n/* \n * REVIEW: The array `tids` is declared but not initialized. If `thread_create` fails, \n * `tids[i]` remains indeterminate. Consider initializing the array (e.g., to 0) or \n * setting `tids[i] = 0` when creation fails.\n */\nline 29:  | +     int tids[NUM_THREADS]; // To store PIDs returned by thread_create\n\n/* \n * REVIEW: You check `if (tids[i] == 0)` to skip failed threads, but since `tids[i]` may be \n * uninitialized, this check is unreliable. Instead, track success via the return \n * value of `thread_create` or ensure `tids[i]` is always set on both success and \n * failure.\n */\nline 50:  | +         if (tids[i] == 0) continue; // If creation failed for this slot\n\n/*============================proc.c=========================================/*\n\n/* \n * REVIEW: The closing brace here appears on its own and may misalign with the opening \n * `mycpu()` definition. Ensure the brace is indented consistently and the function \n * structure is clear.\n */\nline 53:  | + }\n\n/* \n * REVIEW: The prototype for `wakeup1()` is declared twice (also at line 21). Remove the \n * redundant declaration to avoid confusion.\n */\nline 57:  | + void wakeup1(void *chan);\n\n/* \n * REVIEW: Consider adding a function comment header for `clone()` summarizing its purpose, \n * parameters, and return values to improve readability and maintenance.\n */\nline 72:  | + clone(void (*fcn)(void *, void *), void *arg1, void *arg2, void *stack)\n\n/* \n * REVIEW: This `cprintf` debug statement (and similar ones on lines 84, 109, 121, 131, \n * 163) should be removed or guarded by a debug flag before final submission to \n * avoid cluttering kernel output.\n */\nline 79:  | +   cprintf(\"kernel clone: called with stack=0x%x, fcn=0x%x\\n\", stack, fcn);\n\n/* \n * REVIEW: Hard-coding `4` when adjusting the stack pointer reduces portability. Use \n * `sizeof(uint)` or `sizeof(void*)` for clarity and correctness on different \n * architectures.\n */\nline 106: 6 | +   ustack_ptr -= 4;\n\n/* \n * REVIEW: Large block of commented-out `join()` implementation is dead code in this \n * context. Remove it or move it to a separate patch to keep the file concise.\n */\nline 167: 7 | + // In kernel/proc.c\n\n/* \n * REVIEW: Returning `-2` for a `copyout` failure diverges from the `-1` convention used \n * elsewhere. Consider unifying error codes or documenting this special case so \n * callers can handle it properly.\n */\nline 460: 0 | +         if (copyout(curproc-&gt;pgdir, (uint)stack_ptr_user, &p-&gt;user_stack, sizeof(void *)) &lt; 0) {\n\n/* \n * REVIEW: `check_and_free_shared_pgdir()` nicely encapsulates shared-pgdir logic. Add a \n * brief comment stating its precondition (e.g., 'assumes ptable.lock is held') to \n * guide future maintainers.\n */\nline 501: 1 | + check_and_free_shared_pgdir(struct proc *dying_proc)\n\n/*============================syscall.c=========================================/*\n\n/* \n * REVIEW: Indentation of the `static int (*syscalls[])` declaration and its entries should \n * match the existing style in this file (e.g., two spaces from the margin). \n * Consistent indentation improves readability and helps maintain a uniform \n * codebase.\n */\nline 109: 9 | + static int (*syscalls[])(void) = {\n\n/* \n * REVIEW: Designated initializers in C require an `=` after the index. Change `[SYS_fork] \n * sys_fork,` to `[SYS_fork] = sys_fork,` (and similarly for all entries) to \n * conform to standard C syntax and avoid compiler errors.\n */\nline 110: 0 | + [SYS_fork]    sys_fork,\n\n/* \n * REVIEW: Add a brief function‐header comment above `syscall()` summarizing its role \n * (dispatching system calls based on %eax). A per‐function overview aids future \n * maintainers in quickly understanding its purpose.\n */\nline 135: 5 | + void\n\n/*============================sysproc.c=========================================/*\n\n/* \n * REVIEW: Add a brief header comment for sys_clone explaining its purpose, expected \n * arguments, and return value. This matches the style of other syscall definitions \n * and aids future maintenance.\n */\nline 93:  | + sys_clone(void)\n\n/* \n * REVIEW: The name `fcn` is terse and non-descriptive. Consider renaming to something like \n * `start_routine` or `fn` to more clearly convey its role.\n */\nline 95:  | +   void (*fcn)(void *, void *);\n\n/* \n * REVIEW: The alignment check is commented out. Either fully enable this runtime check \n * (per the assignment spec) or remove the dead code to keep the implementation \n * uncluttered.\n */\nline 108: 8 | +   // if ((uint)stack % PGSIZE != 0) {\n\n/* \n * REVIEW: This compound `if` condition is quite long and dense. Break it into multiple \n * sub-checks or extract parts into well-named boolean variables (e.g., \n * `stack_overflows` and `stack_outside_user_space`) for clarity.\n */\nline 113: 3 | + if ((uint)stack &gt;= curproc-&gt;sz || (uint)stack + PGSIZE &gt; curproc-&gt;sz || (uint)stack + PGSIZE &lt; (uint)stack /*overflow*/) {\n\n/* \n * REVIEW: Using `cprintf` in a syscall for argument validation can flood the kernel log on \n * bad user calls. It’s cleaner to silently return `-1` and let the user-level \n * library handle diagnostics.\n */\nline 115: 5 | +     cprintf(\"clone: stack invalid (outside user space or wraps around)\\n\"); // Modified message for clarity\n\n/* \n * REVIEW: Add a function comment for sys_join describing its behavior (waiting on a \n * thread, where the stack pointer is stored, and the return semantics). This keeps \n * the syscall interface self-documented.\n */\nline 123: 3 | + sys_join(void)\n\n/* \n * REVIEW: You cast to `(char **)` when fetching a `void **` from user space. Consider \n * casting to `(void ***)` or using an intermediate `char *` to avoid potential \n * pointer-type mismatches and compiler warnings.\n */\nline 127: 7 | +   if (argptr(0, (char **)&stack_ptr_user, sizeof(void **)) &lt; 0) {\n\n/*============================ulib.c=========================================/*\n\n/* \n * REVIEW: You reimplement `strcpy` here, which may conflict with existing library versions \n * and increases maintenance burden. If XV6’s user library already provides this, \n * prefer that. Otherwise, mark it `static` to limit its scope or document why a \n * custom version is needed.\n */\nline 14:  | + strcpy(char *s, const char *t)\n\n/* \n * REVIEW: This custom `memset` duplicates standard functionality. If a user‐level `memset` \n * exists in XV6, use it. Otherwise, declare this function `static` and add a brief \n * comment explaining why a custom version is required.\n */\nline 42:  | + memset(void *dst, int c, uint n)\n\n/* \n * REVIEW: Your `memmove` always copies forward and does not handle overlapping \n * source/destination regions correctly. Either rename it to `memcpy` (and document \n * no-overlap requirement) or add logic to detect overlap and copy backwards when \n * necessary.\n */\nline 114: 4 | +     *dst++ = *src++;\n\n/* \n * REVIEW: The spec defines `thread_create` as returning the thread ID directly (and \n * returning 0 in the child). Here you take an extra `int *tid` out parameter. \n * Consider matching the assignment’s prototype or clearly document the difference.\n */\nline 121: 1 | + thread_create(int *tid, void (*start_routine)(void *, void *), void *arg1, void *arg2)\n\n/* \n * REVIEW: `malloc` does not guarantee page alignment on XV6. Since `clone` requires a \n * page-aligned stack, ensure the pointer you pass is page-aligned (e.g. allocate \n * extra bytes and round up, or use a page-size allocator).\n */\nline 125: 5 | +   stack = malloc(USER_THREAD_STACK_SIZE);\n\n/* \n * REVIEW: You pass the base of the allocated stack to `clone`, but on x86 the stack grows \n * downward: you should pass `stack + USER_THREAD_STACK_SIZE` (the top of the \n * stack) to the syscall so the thread starts with a valid stack pointer.\n */\nline 131: 1 | +   int pid = clone(start_routine, arg1, arg2, stack);\n\n/* \n * REVIEW: Your lock APIs (`ticket_lock_t`, `ticket_lock_init`, `ticket_lock_acquire`, \n * `ticket_lock_release`) differ in naming from the spec’s `lock_t`, `lock_init`, \n * `lock_acquire`, `lock_release`. Align names with the assignment or update your \n * `thread.h` accordingly to avoid confusion.\n */\nline 165: 5 | + ticket_lock_init(ticket_lock_t *lk)\n\n\nResults\nI was able to run code-critique on 26 submissions of shell programming assignment for less than 1 USD in API costs and under 30 minutes. I distributed the feedback via a new branch to students’ GitHub Classroom repo. I collected feedback from students on how satisfied they were with this automated feedback quality and their suggestions for improvements. Generally, I found students appreciated getting such additional comments on their code quality. Here are some testimonials.\n\nThe feedback really helps us understand how we did in the assignment even better. Sometimes testcases pass without us doing much changes but understanding why it has happened through a partially detailed feedback is really helpful. - Gaurav\n\nHere’s what another student had to say:\n\nThank you so much for the feedback, Professor! The tool managed to point out some inefficiencies in my code which I had missed. It was nice to see that there was an alternative implementation which was more concise. - Nikhil\n\n\nAn early version of tool was written by me in August 2024 - see blog post. That early version worked only for single-file submissions, did not integrate linter, made a lot more LLM calls, and did not preserve original code because of lack of structured output."
  },
  {
    "objectID": "posts/cuda_vector_add/index.html",
    "href": "posts/cuda_vector_add/index.html",
    "title": "Vector Addition - A First Look at CUDA",
    "section": "",
    "text": "In this blog post, we’ll write a simple CUDA program to add up elements from 2 arrays on the GPU. We’ll use the nsys profiler to identify bottlenecks and do some basic code optimizations. We’ll see how calculating bandwidth and arithmetic intensity helps us think about the characteristics of our problem (vector-addition here). I’ll intersperse some tips I’ve found helpful in working with GPU on cloud VM.\nThis post builds upon the excellent article An Even Easier Introduction to CUDA."
  },
  {
    "objectID": "posts/cuda_vector_add/index.html#cpu-version-à-la-cuda",
    "href": "posts/cuda_vector_add/index.html#cpu-version-à-la-cuda",
    "title": "Vector Addition - A First Look at CUDA",
    "section": "CPU-version: À la CUDA",
    "text": "CPU-version: À la CUDA\nFirst, let’s write a simple C++ program to add up elements from 2 arrays. It’s short and is in this gist. Let’s look at the function that does the main computation:\n/* Function to add idx-th element from two arrays (x and y)\n and store it in sum. Written a la CUDA. */\nvoid add_arr(int N, float *sum, float *x, float *y, int idx)\n{\n    if (idx &lt; N)\n      sum[idx] = y[idx] + x[idx];\n}\nWe’ve separated out add_arr() from main() because this is the part that does the important computation. We think of this as kernel function and this is the part which has to be modified to work on GPU. Note how add_arr() operates on idx-th index of the arrays which is a function parameter. This has a similar flavor to CUDA implementation below (hence, à la CUDA) and makes it straightforward for us to port the C++ implementation to CUDA version. I got the idea for this from Jeremy Howard’s talk\n\n\n\n\n\n\nCloud VM Tip\n\n\n\nI made use of GPU on a RunPod VM instance. To save money, it’s helpful to check correctness of our implementation on CPU first; hence we wrote the above version. In fact, we can write a draft version of CUDA code on local machine as well, and then debug it after spinning up the VM."
  },
  {
    "objectID": "posts/cuda_vector_add/index.html#cuda-version",
    "href": "posts/cuda_vector_add/index.html#cuda-version",
    "title": "Vector Addition - A First Look at CUDA",
    "section": "CUDA version",
    "text": "CUDA version\nWhen we rewrite the above program to run on GPU using CUDA, we get the code in this gist. Note the .cu extension rather than .cpp. We discuss below the important aspects of this code.\nThe first thing for us to know is that in CUDA, the CPU is referred to as host and GPU as device. However, when we refer to GPU as device, the CPU-GPU interaction is very different from, let’s say, CPU-I/O device interaction. It’s more helpful to think of GPU as a coprocessor - the CPU orchestrates and launches work (kernel) for the GPU, and the GPU executes these tasks autonomously. That’s why a GPU + CPU system is also referred to as heterogeneous computing system.\n\nDevice Code\nWe have rewritten our kernel function for CUDA as:\n__global__ void add_arr(int N, float *sum, float *x, float *y)\n{\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx &lt; N)\n      sum[idx] = y[idx] + x[idx];\n}\nThe specifier __global__ is used to annotate this function to say that this will be run on GPU. This tells the CUDA compiler (nvcc) to compile this function to the instruction set architecture (ISA) of the GPU called PTX. This is different from say the x86-64 ISA for the CPU. nvcc takes care of compiling __global__ kernel functions to PTX instructions and passes on the host code to an underlying compiler (say g++) to be compiled to the CPU ISA. Specifiers like __global__ and __device__ are also useful to us to parse through the codebase and quickly get an idea as to which functions will run on which hardware.\n\nCUDA Threads\nIn the GPU execution model, threads are contained within warps; warps are organized within blocks; finally blocks are organized within grid. Please refer this guide for a clearer picture of thread hierarchy. CUDA threads are different from CPU threads in that they’re more lightweight, the hardware (GPU) has a greater role to play in their management rather than software (OS), and tend to be much more homogeneous in computation than CPU threads.\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\nCUDA makes it possible for each running thread to obtain its unique id. threadIdx.x says what’s the id of the this thread within it’s block; so this is unique in a block of threads. blockIdx.x says what’s the id of this particular block, and blockDim.x says how many threads are contained in a single block. This way each thread in a kernel has its own unique id. This unique id lets each thread know the part of the inputs to act on to produce a specific part of the output, avoiding race conditions.\n\n\n\nHost Code\n\nUnified Memory\nCPU and GPU have separate memories. Pieces of data have to be moved between CPU and GPU memories, either explicitly by us programmers or implicitly by CUDA runtime. The most important change we need to do to our original CPU-only code in main(), is to allocate arrays x, y, sum in memory accessible to GPU.\n    cudaMallocManaged(&x, size);\nThe CUDA runtime allocates memory pages for x in a virtual address space accessible to both CPU and GPU; it’s called Unified Memory. At this point, physical pages for x are not allocated either on GPU or CPU.\n    for (int i = 0; i &lt; N; i++) {\n        x[i] = 1.0f;  // First touch happens on CPU\n        y[i] = 2.0f;\n    }\nWe’re initializing x and this runs on CPU because it’s in main() (if a function specifier is not given, the default is __host__). So CPU touches the memory of x first, a page fault occurs, and CUDA runtime allocates page for x in CPU main memory. Hence, page allocations happen at runtime, on demand; it’s a bit like the way we do memory mapping via mmap() on a CPU system.\n\n\nPrefetching\n    cudaMemPrefetchAsync(x, size, 0, 0); \nThe kernel needs x, y, sum to be resident on GPU memory. By default, when the kernel starts running, it’ll encounter page faults, and memory pages for x, y, sum will be migrated by CUDA runtime on demand.\nThe problem with this is that running GPU threads have to stall until the pages are migrated, making this inefficient. The performance gain from prefetching is huge. For my VM with RTX A4000, nsys profiler showed that the kernel execution time decreased from 9.2 ms to 32 microseconds.\nHowever, I feel that if we have to keep track of which object is in which memory at some point of time, we’re probably better off doing separate allocations on CPU and GPU ourselves and doing explicit copying via cudaMemcpy(), rather than using Unified Memory. Also, notice that these prefetches are asynchronous, so copying of x, y, sum can happen concurrently. Besides, no explicit synchronization is needed before calling the kernel.\n\n\n\n\n\n\nCloud VM Tip\n\n\n\nMost VM services allow us to select VM templates. Please make sure that the CUDA toolkit that is installed as part of template is compatible with the GPU driver. This may be an issue with less recent GPUs like RTX A4000.\n\n\n\n\nKernel Call\n    int blockSize = 256;\n    int numBlocks = (N + blockSize - 1) / blockSize;\n    add_arr&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;(N, sum, x, y);\nFor many kernels, it’s possible to judiciously select the blockSize and numBlocks so that kernel execution is optimized. In this case, we select some plausible blockSize and have all blocks in a single grid. The add_arr() kernel call is async, which is good because it lets the CPU do some work when the kernel is running. However, if we have to process the results, we have to explicitly synchronize after kernel call via cudaDeviceSynchronize()."
  },
  {
    "objectID": "posts/cuda_vector_add/index.html#bandwidth-and-arithmetic-intensity",
    "href": "posts/cuda_vector_add/index.html#bandwidth-and-arithmetic-intensity",
    "title": "Vector Addition - A First Look at CUDA",
    "section": "Bandwidth and Arithmetic Intensity",
    "text": "Bandwidth and Arithmetic Intensity\n    sum[idx] = x[idx] + y[idx]\nEach thread in vector addition does 2 reads (x[idx] and y[idx]) and one write (sum[idx]); each thread also does one math operation (addition here). We say that the arithmetic intensity in such cases is very low. Vector addition is bottlenecked by memory read/write rather than math computation.\nThis shows up in our profiling data as well. Each array of a million floats is of size 4 bytes * 1&lt;&lt;20 = 4 MB. Our code involves moving around 4 MB * 3 = 12 MB of data between the global memory of GPU and on-chip SM memory. Because kernel execution for us takes 32 microseconds, the effective bandwidth of our implementation is 375 GB/s. This is about 83% of RTX A4000’s peak bandwidth of 448 GB/s. Our kernel is limited by how fast data can be moved within the GPU.\n\n\n\n\n\n\nCloud VM Tip\n\n\n\nDotfiles are plain-text files, starting with . like .vimrc, that configure vim, shell, tmux, git, and other programs. To quickly customize our VM to feel like our local setup, we can organize dotfiles in a separate folder, under version control, and use a script to symlink them to ~/. Here is my dotfiles repo, with the setup script.\n\n\n\nThanks to Eric for new ideas and inspiration."
  },
  {
    "objectID": "posts/Grounding-LLMs/index.html",
    "href": "posts/Grounding-LLMs/index.html",
    "title": "Grounding Language Models in the Physical World",
    "section": "",
    "text": "I recently listened to a podcast episode on The Robot Brains where Jitendra Malik, an eminent computer vision researcher, shared his thoughts and experiences on grounding large language models (LLMs) in the physical world and how to approach this through robotics. I summarize the discussion below:\n\nMoravec’s Paradox\n\nMoravec wrote in 1988, “it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility” 1.\nLLMs have shown enormous success recently on a wide variety of cognitive tasks. Among other benchmark tests, these systems have done well on challenging competitive exams. This gives us a feeling that these systems have acquired impressive intelligence. However, this holds true for tasks involving abstract, higher-level cognition. The challenging problems of locomotion and other desirable motor skills in robotics have not become solved as a result of this progress on LLMs. We seem to be still consistent with Moravec’s Paradox.\n\nEvolutionary Perspective\n\nOn our human evolution journey, brain development followed the development of hands with opposable thumb; hand development in turn followed the development of bipedal walking which left our hands free. We developed sensorimotor skills first, language acquisition is a more recent phenomenon. If we think of human evolution on a 24-hour timeframe, langugage development corresponds only to the final 2-3 minutes. Clearly, all of intelligence cannot be said to reside in those final couple of minutes. Besides, different species of animals have different flavours of intelligence, and these are sophisticated in many cases. These animals do not possess language ability in the conventional sense. Hence, what we can learn from language alone may be inherently limited.\n\nHuman Learning and Development\n\nWe can take inspiration from how babies and children learn. Babies interact with the world around them in a multi-modal way, using different senses. They gradually learn to manipulate objects and perform small experiments of their own. The acquisition of words at this stage is grounded in physical objects and interactions with them. So for instance when a mother says ‘this is a ball’, there is a visual input of the ball along with the motor desire to throw or catch a ball.\nWhen children go to school after the age of 5 and acquire knowledge through books, they already have a basic understanding of the world, people, and objects on which to build upon. This points to the need for a multi-modal and staged process of learning (curriculum learning).\n\nEmbodied AI\n\nA new paradigm of intelligent models can be robots equipped with vision, touch, audio sensors with the ability to move, manipulate objects and interact with the real-world. As the robot learns more about the dynamics of the world, we can teach it basics of language. The rest of langugage acquisition will happen in an in-context manner, by combining atomic concepts, much like children do after age 5.\n\nRapid Adaptation of Motor Skills\n\nThere is a compelling argument for this approach of acquiring atomic concepts that are grounded in the world followed by general language acquisition and development of abstract thinking. Any robotic system needs to be adaptable and robust. For instance, a robot must be able to walk on diverse, unseen terrains. The robot must be capable of learning new policies quickly, with the time-frame being task dependent. For example, a walking robot needs to learn to stabilize on a new terrain in about a second else the robot will fall. Just like humans need only a handful of examples to pick up a new concept, we can hope our ML systems modeled on lines of human learning and development, will be able to quickly adapt with a small number of simulations/trial and error."
  },
  {
    "objectID": "posts/Grounding-LLMs/index.html#footnotes",
    "href": "posts/Grounding-LLMs/index.html#footnotes",
    "title": "Grounding Language Models in the Physical World",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia contributors. “Moravec’s paradox.” Wikipedia.↩︎"
  },
  {
    "objectID": "posts/inference-profiling/index.html",
    "href": "posts/inference-profiling/index.html",
    "title": "Profiling LLM Inference",
    "section": "",
    "text": "Let’s use PyTorch Profiler to get a better understanding of what happens under the hood during LLM inference. The model we’re going to use is the 1B parameter instruction-tuned version of Gemma3 from HuggingFace model hub. I’m going to make use of RTX 4090 GPU on a RunPod vm instance. The profiling code can be found in this Gist\n\n\n\n\n\n\nNote\n\n\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nThis configures to load the model parameters as 8-bit integers to reduce memory footprint of the model. This is the bulk of the memory usage; hence, lots of space saving.\ninputs = tokenizer.apply_chat_template(...).to(model.device).to(torch.bfloat16)\nWe load the input tensor as bfloat16 as accommodate the wider dynamic range of activations.\n\n\nWe profile inference by wrapping profile() around model.generate():\nwith torch.inference_mode():\n    with profile(activities=[ProfilerActivity.CUDA]) as prof:\n        with record_function(\"model_inference\"):\n            outputs = model.generate(**inputs, max_new_tokens=64)\nProfilerActivity.CUDA is saying that we only want to profile operations on GPU. The detailed output is here. In summary, there are two types of operations on GPU - matrix multiplication (called gemm kernels) and elementwise operations such as normalization, activation function, etc.\n\n\n\nCUDA Kernel\nSelf CUDA %\n\n\n\n\ngemm_kernel\n18%\n\n\nelementwise_kernel\n82%\n\n\n\nObservation 1: The majority of time is spent on elementwise operations. This happens because LLM inference consists of one prefill step and all the rest decode steps. Due to KV-caching, these decode steps involve thin matrix multiplications, thereby under-utilizing the GPU. This is bad news because elementwise operations use particular cores on the GPU that have much lower throughput than the cores used for matrix multiplication. This also implies our second observation.\nObservation 2: To predict each token, all the model parameters have to be shunted from GPU global memory (DRAM/HBM) to shared memory (SRAM). Memory transfer is relatively expensive and to make good use of this expense, we usually try to amortize by doing a lot of computation on the transferred data. This becomes difficult in our scenario where matrix multiplications are thin and elementwise operations are dominant. We say that our inference computation is memory-bound.\nObservation 3: Over the course of inference, the CPU is launching a huge number of CUDA operations for the GPU to perform and the GPU performs each very quickly and waits idly for the next command. Anything happening on the CPU is like a overhead and these arise from slow Python interpreter, layers of dispatch on PyTorch framework, and launching of CUDA kernels for the GPU. As shown by the following numbers from the profiler, CPU overhead is significant in our case.\nSelf CPU time total: 2.502s\nSelf CUDA time total: 1.196s\nIn this case, GPU and CPU are operating synchronously - a bad state of affairs. We can also say we’re overhead-bound.\n\n\n\n(A)synchronous mode of CPU-GPU operation\n\n\nOverall, we can characterize our workload as follows:\n\n\n\nCharacterization of workload\n\n\nSo, what options have we got?\n\n❌ No point optimizing already efficient matmul/sgemm kernels since GPU compute is such a tiny part of the equation.\n✅ Use efficient serving engines like vLLM or SGLang or TGI. They do various QoL improvements such as CUDA graphs, speculative decoding, PagedAttention, and more.\n\n\nThanks to Eric, Jeremy, and Ramjee for helpful discussions."
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html",
    "href": "posts/ML-research-vs-production/index.html",
    "title": "Machine Learning in Research versus Production",
    "section": "",
    "text": "I have been going through the book ‘Designing Machine Learning Systems’ by Chip Huyen to better understand how machine learning systems are deployed in production in industry. Coming from a research background, it’s been a good experience to learn the difference in emphasis and priorities between how machine learning is practised in research versus in production. Here, I summarize some of my learnings on this topic:\n\n\nResearch: The number of stakeholders on a research project is small and they are usually aligned in terms of the project objective. The objective is often to achieve state-of-the-art performace on benchmark datasets. Model complexity is usually not an issue if it helps to eke out a tiny percentage of improvement on peformance metrics.\nProduction: There are often many stakeholders and their requirements can often be different and conflicting. It’s usually not possible to design an ML system satisfying all requirements, so a compromise must be reached. The value of performance gain is case-dependent. At times, a tiny improvement can save a lot of money for a business. However, often a small improvement does not justify the increase in model complexity.\n\n\n\n\n\n\n\nModel development code is a small part of the full codebase needed for deployment. Adapted from Sculley et al. 1\n\n\nResearch: A research group focuses most of its effort on model development. Due to the need to train many different models, research prioritizes fast training. In other words, high throughput is the desired quality in an ML system where throughput is the number of prediction requests that can be processed by the system in unit time.\nProduction: When deploying a model in production, the surrounding infrastructure of data pipelines, resource management, and servers has to be constructed and this takes a lot of time and effort. Once deployed, the focus is now on fast inference. Hence, we seek low latency in this case where latency is the amount of average waiting time before a prediction request is processed.\n\n\n\nResearch: The benchmark datasets in research are usually clean, static, and well-formatted. The anomalies in the dataset, if present, are usually known and often open-source code to process the datasets is available.\nProduction By contrast, data in real-world systems can be noisy, unstructured, and its distribution can shift with time. There may be bias in the data. Labels may be sparse, imbalanced, or incorrect. The data may not be available as a complete dataset but may arrive over time in form of a stream as it gets generated by the user. Finally, one has to respect user privacy when dealing with their data.\n\n\n\nResearch: Fairness is rarely given consideration in a research setting where the agenda is achieving state-of-the-art accuracy. Even when fairness is considered, it is an afterthought. This also has to do with lack of meaningful fairness metrics.\nProduction: A machine learning system deployed in the real-world can have influence on society members as a result of the decisions it outputs. Giving strong emphasis to fairness can ensure that no section of society gets adversely affected by ML models and that all sources of bias have been dealt with. Unfortunately, much progress remains to be done on this front as a lot of models deployed for loan applications, predictive policing, employee recruitement still discriminate against minority groups.\n\n\n\nInterpretability is the property of an ML system to explain the rationale behind the decision it makes or the output it gives. If a system is interpretable, one can peek inside the model for better understanding; in case there is some mistake/anomaly, one can pinpoint with confidence where the model is going wrong and debug accordingly.\nResearch: With ML research being evaluated on a single objective, that of model performace, there is no incentive to ensure that the model is interpretable.\nProduction: Intrepretability is a key requirement for real-world ML systems. For us to deploy an ML system in society, we need to be able to trust it. This trust is highly dependent on the system being interpretable. In case of misbehavior, we want to be able to diagnose and rectify our model.\n\n\n\nResearch: In research, one often focuses solely on the model development part. The benchmark dataset is static. There is no monitoring to be done once good results on the benchmark dataset have been achieved. Advances in software and hardware, and accumulated common wisdom have made this process relative fast and cheap.\nProduction: The deployment of ML systems is still relatively fast. Once deployed, an ML system needs to be constantly monitored and maintained and this is the hard part. There are some aspects of ML systems that make them much more challenging to monitor and maintain as compared to traditional software systems, particularly the dependence of model performance on external data. At a high level, one needs to constantly ensure that the data pipeline does not get broken, the incoming data is sensible, and the test data distribution does not differ considerably from training distribution. In case of distribution drift, one needs to retrain the model."
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html#footnotes",
    "href": "posts/ML-research-vs-production/index.html#footnotes",
    "title": "Machine Learning in Research versus Production",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSculley, David, et al. “Hidden technical debt in machine learning systems.” Advances in neural information processing systems 28 (2015).↩︎"
  },
  {
    "objectID": "posts/paradoxes/index.html",
    "href": "posts/paradoxes/index.html",
    "title": "DeepSeek in the Uncanny Valley of Paradoxes",
    "section": "",
    "text": "The funky post title is inspired by DeepSeek-R1’s response to one of our paradoxical situations where it used the apt phrase ‘Ethical Uncanny Valley’ in its response.\nParadoxical situations are some of the hardest reasoning tasks we can give to LLMs. Most of the existing benchmarks are based on math and programming problems. However, as these models are increasingly used in non-technical domains, it’s useful to see how well these models do in the context of open-ended, ambiguous scenarios inspired by classical paradoxes proposed in philosophical tradition. Operationalizing such evaluation is difficult for two reasons:\n\nWe want that we actually make the model reason/think on a novel problem, rather than making it easy to answer from its memory of training data (a perennial problem with eval). Since these paradoxes are well-documented in literature and the framing of situations encoding the paradox tends to be standard, this makes our task challenging.\nUnlike math and coding problems, it’s not helpful for our task to look at the final answer and judge correctness. Instead, the evaluation here is more nuanced; we have to ascertain carefully whether the model captured the various dimensions of the situation and worked with multiple threads of possibilities before finally concluding that perhaps multiple conclusions may hold true simultaneously.\n\n\nData and Evaluation\nIn view of the above, we manually create a set of 12 problems involving situations that encode classical paradoxes, some of them being a combination of two paradoxes together (example). All of them are twists and riffs on standard formulations, thereby allowing us to ameliorate the effect of training data memory to the extent we can. We point out that these paradox prompts are different from dilemmas which require the model to select from two possible actions. In our case, the situations are fundamentally unresolvable though one can still make good statements about different possibilities.\nWe noticed ambiguities in our prompt formulations. For example, in problem 7 it’s not obvious whether the fact that the VR simulation was partly real is releaved at the end of each screening or at the very end when all screenings are done. We decided to retain such ambiguities and see how the models deal with it.\n\n\n\n\n\n\nNote\n\n\n\nSome of the paradoxes we worked with are Liar’s, Russell’s, Barber’s, Newcomb’s and Knower’s paradoxes, and also some inspired by quantum mechanics.\n\n\nWe manually assign a score between 0-4 to model’s response with 0 being poor and 4 being excellent; see our evaluation criteria here. In addition, we give an explanation for the score and identify where the model did well or poorly in its thinking. As a result, our evaluation is of necessity subjective, and some of our observations below are opinionated.\n\n\nResults\nWe compared the quality of outputs of a thinking model DeepSeek-R1 and a non-thinking model Claude-3.7-Sonnet (without extended thinking turned on). Across the board, we found DeepSeek-R1’s responses to be more detailed, nuanced, and overall better quality than Claude’s.\n\n\n\n\nClaude-3.7-Sonnet (no extended thinking)\nDeepSeek-R1\n\n\n\n\nAverage score (over 12 prompts)\n2.63\n3.42\n\n\n\nClearly, there’s a lot of benefit in having thinking ability for these kinds of problems.\n\n\nObservations\n\nGood\n\nEven though DeepSeek-R1 acquired its thinking capability by RL on math and coding tasks, we were surprised how well it transposed this ability to the new domain of philosophical quandries.\nWe were struck by the richness of the language used by DeepSeek-R1. Even in the absence of a system prompt encouraging it to say things with good literary merit, it used words and phrases very well-suited to the philosophical discussions at hand (examples from problem 7 - moral labyrinth, scaffold for empathy, ethical uncanny valley, quantum state of being simultaneously fictional and real, testament to the irreducible complexity of human feeling.)\nEqually impressive was DeepSeek-R1’s ability to make useful associations (Susan Feagin, Walter Benjamin, and Aristotle in problem 7) and often recognize the name of the paradox to which a given situation was related. We would like to point out that being able to recognize the name of the paradox isn’t sufficient for the model - it still has to apply the ideas in the context of the novel situation which may have more complexities (see Liar’s paradox with temporal twist).\nDeepSeek-R1 backtracks often in the course of thinking and explores alternate pathways of thoughts (saying Alternatively, Or maybe); this is very desirable. We observed that it would sometimes reach a dead-end when thinking, and would start afresh with another line of thought - this was often preceded by But wait.\n\n\n\n\nA rough sketch of DeepSeek-R1’s thinking pattern\n\n\n\n\nNot-so-good\n\nThe final answer given by DeepSeek-R1 was of much better quality and often contained novel points not brought up in the &lt;think&gt; part. This suggests the final answer was not a mere summary of the text within &lt;think&gt; tags. Our hypothesis is that the primary benefit of the &lt;think&gt; part is to allow more computation before giving a final answer. The same may be better accomplished by &lt;wait&gt; tags without the model having to give verbose explanations.\nThere was a lot of repetition and circularity in the thought process that went on within &lt;think&gt; tags. On some harder problems (see here and here) the model could not give a final answer because it ran out of output token limit within &lt;think&gt; part. This lends support to the idea that thinking in words, though interpretable, is very inefficient.\n\nIdeally, we want thinking to happen efficiently in some latent space, and then for us to derive a concise summary by probing the latent embeddings by specially-designed probes (small models).\n\nDeepSeek-R1 just took our prompt and ran off with it to think and give an answer. In the presence of ambiguities in the prompt, the model asked no clarifying questions. This resulted in a lot of speculative ruminations and circular thinking in the &lt;think&gt; part.\n\n\n\n\nConclusion\nIt was impressive how DeepSeek-R1 operated in this new, open-ended domain. Its richness of language and ability to make associations were also remarkable.\nOn the other hand, we feel that what goes on in &lt;think&gt; part is not very efficient. It may be better to reason in a latent space, provided we are able to sumarize the thinking encoded in latent embeddings using specially-designed probes. Finally, thinking models may benefit from recognizing under-specifications and ambiguities in prompts; making an effort to get them clarified early may lead to more desirable outputs.\n\nYashika is an undergraduate student in Philosophy at Lady Shri Ram College, Delhi. Aman is an undergraduate student in Computer Science at Plaksha University, Mohali."
  },
  {
    "objectID": "posts/throughtput_latency/index.html",
    "href": "posts/throughtput_latency/index.html",
    "title": "Throughput vs Latency - GPU vs CPU",
    "section": "",
    "text": "CPUs are optimized for latency; GPUs are optimized for throughput. They are independent processors, so can and should work on different things at the same time.\nLatency - Time taken to complete a single operation or instruction; commonly measured in clock cycles. Lower latency is obviously desirable. Note that latency can be talked of for CPU/GPU computation or memory access - they are two different things. The value of latency of an instruction depends on the type of instruction (integer/floating-point/SIMD). Here we are not referring to a particular type of instruction.\nThroughput - Number of operations or instructions completed per unit of time. Again, throughput can refer to computation or memory access. By convention, when we say throughput, we mean computation throughput and that is measured typically in FLOPS (Floating Point Operations Per Second). Higher throughput is obviously better.\nThe analogue of throughput for memory is called bandwidth. It’s the maximum amount of data transfer that can happen per unit time and these days it’s typically measured in GB/s. Note that bandwidth encompasses the memory device (DRAM/SRAM etc.) as well as the bus speeds.\nSo here we are - we want higher throughput and lower latency. It may seem like lowering latency would automatically increase throughput and that’s true in theory; in practice there are constraints and trade-offs. We have to decide whether it’s more critical for us to have latency-optimized or throughput-optimized system. Optimizing for one practically leads to sub-optimal performance for the other.\n\nAnalogy\nConsider an analogy: we want to get from Secretariat in New Delhi (green) to Dwarka (red). We can take either the metro which takes about 75 minutes or use a car which takes 45 minutes. The metro takes longer for us (higher latency) but it transports a lot more people in let’s say an hour (higher throughput). So, the metro is optimized for throughput and the car is optimized for latency. Note that a car is more flexible and can take us to places where the metro doesn’t go.\n\n\n\nMetro vs Car\n\n\nCPUs are the cars - optimized for latency. Context switching between threads is expensive. So the CPU makes an individual thread as fast as possible. By CPU here, we actually mean both the processor and the memory system. Focusing on the processor, there are complex things like branch prediction, out of order execution, prefetching to make the latency low - this makes the cores complex and takes up real-estate on chip. Hence, we only have a few cores on a CPU. The memory hierarchy is elaborate with multiple levels to optimize for latency.\nGPUs, on the other hand, are the metros. The cores are simple and hence there can be a lot of them. The focus is on massive parallelism, so throughput is high. Latency for an individual thread may not be great, but for GPU workloads that’s not so important. Even the GPU memory has simpler organization and wider buses to optimize for throughput. Such a system, like a metro, works best when it’s oversubscribed; there is a deep queue of pending threads to be executed.\n\n\nAsynchrony\nCPUs and GPUs are independent processors. They can and should work on different things at the same time. The following is a bad model of how to use a CPU and a GPU together:\n\n\n\nSynchronous\n\n\nNote how GPU does some computation and then waits for something for CPU - this happens repeatedly. It’s as if the CPU and GPU are operating synchronously. In this case, a much better flow is the following:\n\n\n\nAsynchronous\n\n\nIn the above, the CPU issues computation to GPU and then starts working on its own thing separately. There is good utilization of both processors. We say that they are operating asynchronously - we’re aiming for this.\n\nThis post was inspired by part of Steve Jones’ talk at GTC 2021. Thanks to Eric for telling me about it."
  }
]