[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "A Probabilistic Perspective on Regularization\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning in Research versus Production\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Large Language Models\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n  \n\n\n\n\nA Simple Painting Style Classifier\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/LLM-intro/index.html",
    "href": "posts/LLM-intro/index.html",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts.\n\n\n\n\n\nThe essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence ‚ÄúI am going to the market‚Äù, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don‚Äôt provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‚Äòencoded‚Äô embeddings. In each operation of that sequence, we allow the embeddings of all words to ‚Äòinteract‚Äô and influence each other. The effect is that the new set of ‚Äòencoded embeddings‚Äô encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt‚Äôs clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs.\n\n\n\n\n\nA machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it‚Äôs better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let‚Äôs say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task.\n\n\n\n\n\n\n\nA lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There‚Äôs been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don‚Äôt clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#introduction",
    "href": "posts/LLM-intro/index.html#introduction",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts."
  },
  {
    "objectID": "posts/LLM-intro/index.html#background",
    "href": "posts/LLM-intro/index.html#background",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "The essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence ‚ÄúI am going to the market‚Äù, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don‚Äôt provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‚Äòencoded‚Äô embeddings. In each operation of that sequence, we allow the embeddings of all words to ‚Äòinteract‚Äô and influence each other. The effect is that the new set of ‚Äòencoded embeddings‚Äô encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt‚Äôs clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs."
  },
  {
    "objectID": "posts/LLM-intro/index.html#large-language-models",
    "href": "posts/LLM-intro/index.html#large-language-models",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it‚Äôs better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let‚Äôs say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task."
  },
  {
    "objectID": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "href": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There‚Äôs been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don‚Äôt clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#footnotes",
    "href": "posts/LLM-intro/index.html#footnotes",
    "title": "An Introduction to Large Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBender, Emily M., et al.¬†‚ÄúOn the dangers of stochastic parrots: Can language models be too big?ü¶ú.‚Äù Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2021.‚Ü©Ô∏é\nBubeck, S√©bastien, et al.¬†‚ÄúSparks of artificial general intelligence: Early experiments with gpt-4.‚Äù arXiv preprint arXiv:2303.12712 (2023).‚Ü©Ô∏é\nAky√ºrek, Ekin, et al.¬†‚ÄúWhat learning algorithm is in-context learning? investigations with linear models.‚Äù arXiv preprint arXiv:2211.15661 (2022).‚Ü©Ô∏é\nLiu, Yang, et al.¬†‚ÄúTrustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models‚Äô Alignment.‚Äù arXiv preprint arXiv:2308.05374 (2023).‚Ü©Ô∏é\nShumailov, Ilia, et al.¬†‚ÄúThe Curse of Recursion: Training on Generated Data Makes Models Forget‚Äù arXiv preprint 2305.17493‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Pankaj Pansari. I‚Äôm a researcher in machine learning and combinatorial optimization. Previously, I was a Research Scientist at Naver Labs Europe in Grenoble, France. In the past, I‚Äôve been a data analyst with General Electric in Bangalore, India.\nI completed my PhD under Dr M. Pawan Kumar Mudigonda in the OVAL group at University of Oxford. I was an undergraduate student at the Indian Institute of Technology, Kharagpur.\nCurriculum Vitae: pdf\nEmail: pankaj.pansari1@proton.me"
  },
  {
    "objectID": "posts/Painting-classifier/index.html",
    "href": "posts/Painting-classifier/index.html",
    "title": "A Simple Painting Style Classifier",
    "section": "",
    "text": "As a first project for my portfolio, I decided to build a simple deep learning model which can classify paintings based on their styles. As of now, the model supports only binary classification - impressionism and cubism. This project involved the whole end-to-end process - data collection, data cleaning and transformation, model training and testing, and finally deploying it as a web application."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#motivation",
    "href": "posts/Painting-classifier/index.html#motivation",
    "title": "A Simple Painting Style Classifier",
    "section": "Motivation",
    "text": "Motivation\nThe motivation for this project came from an online course on deep learning which I‚Äôm currently following (particularly Chapters 1 and 2). In it, the instructor shows how to build an image classifier for a simple task - identifying whether the given image is of a dog or a cat. He then shows how to host the model on a server and deploy it using a basic web application. I decided to adapt the process to my personal interest. I enjoy visiting art museums and looking at paintings. I thought I‚Äôd build a painting style identifier. I initially started with only two types - impressionism and cubism, because the two styles have such strong characteristics and hence are easy to distinguish by humans."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#data-collection-and-processing",
    "href": "posts/Painting-classifier/index.html#data-collection-and-processing",
    "title": "A Simple Painting Style Classifier",
    "section": "Data Collection and Processing",
    "text": "Data Collection and Processing\nI began the project on Google Colab using Jupyter notebook. Colab provides free GPU for our model training and Jupyter notebook is helpful for prototyping and quickly visualisation of results. Image collection was done using DuckDuckGo search API. Microsoft Bing API is probably better in terms of providing more relevant images, but inovlves some knotty configuration. I collected 90 images of each class - a small but sufficient dataset because instead of training an image classifier from scratch, I fine-tuned a powerful model trained on a large dataset.\nThere is some preprocessing to be done before we can use our image data:\n\nImage resizing - The images we‚Äôve scrapped are of different sizes. However, our deep learning model expects all images in the dataset to have the same size. Image resizing can be done in multiple ways. Here I chose random cropping. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a 224x224 patch of each image. This method has the advantage that by the end of training, we would‚Äôve used information from all parts of the image and would not have arbitrarily distorted the images. This process is also augments the dataset since each image can yield muliple cropped samples.\nData augmentation - To increase the size of the dataset and to make our model our robust, each image can be flipped, rotated, warped and have brightness and contrast changed.\n\nBoth of these above functionalities are provided by the fastai library. fastai is a high-level Python library for deep learning built on top of PyTorch that makes the whole process of data cleaning, training, and testing both easier and faster."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#model-training",
    "href": "posts/Painting-classifier/index.html#model-training",
    "title": "A Simple Painting Style Classifier",
    "section": "Model Training",
    "text": "Model Training\nI bypassed image cleaning and decided to directly train a deep learning model. I used a ResNet model with 34 layers, pretrained on the ImageNet dataset to perform image classification. This model is provided for use via fastai. Since the model is already pre-trained, we only have to fine-tune it for our style classification task. Using a pre-trained model meant that I did not need a huge amount of training data or computing resource/time.\nfastai provides a very useful function that suggests a good learning rate to use for training. I fined-tuned the model with the suggested learning rate."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#model-validation-and-training",
    "href": "posts/Painting-classifier/index.html#model-validation-and-training",
    "title": "A Simple Painting Style Classifier",
    "section": "Model Validation and Training",
    "text": "Model Validation and Training\nThe error on the validation set during training goes down to zero completely. I looked at some of the outputs from the model and saw that they‚Äôre correctly classified.\nI also tested the model on a couple of hand-picked images and visualized the responses."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#deployment-as-a-web-application",
    "href": "posts/Painting-classifier/index.html#deployment-as-a-web-application",
    "title": "A Simple Painting Style Classifier",
    "section": "Deployment as a Web Application",
    "text": "Deployment as a Web Application\nI exported and saved the model from the Colab notebook. I used Gradio to demo my model as a web app. It wraps a python function into a user interface and allows us to launch the demos inside jupyter notebooks. The model was hosted on Hugging Face Spaces.\nFinally to create the web application, I used a JavaScript program that accepts the uploaded image, calls the gradio function for inference, and displays the returned results."
  },
  {
    "objectID": "posts/Painting-classifier/index.html#further-work",
    "href": "posts/Painting-classifier/index.html#further-work",
    "title": "A Simple Painting Style Classifier",
    "section": "Further Work",
    "text": "Further Work\nI tried extending this model to identify more types of painting styles. I‚Äôm not an art expert, so after some online research I decided to use 9 different painting styles that are considered the most important. Despite trying various different training parameters, I couldn‚Äôt get the final model to be accurate enough.\nA visualisation of the images where the model performed poorly revealed that some of the images scrapped via the DuckDuckGo API were irrelevant and had to be removed. fastai provides a way for us to remove these irrelevant data via a GUI interface. I did this and still the accuracy was poor. I began to suspect that the images returned in response to search queries didn‚Äôt actually belong to the right styles. All of this suggested that I needed a better way to build up a cleaner and more accurate dataset. However, this process is going to be time-consuming and hence I‚Äôve decided to postpone it for a later iteration."
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html",
    "href": "posts/ML-research-vs-production/index.html",
    "title": "Machine Learning in Research versus Production",
    "section": "",
    "text": "I have been going through the book ‚ÄòDesigning Machine Learning Systems‚Äô by Chip Huyen to better understand how machine learning systems are deployed in production in industry. Coming from a research background, it‚Äôs been a good experience to learn the difference in emphasis and priorities between how machine learning is practised in research versus in production. Here, I summarize some of my learnings on this topic:\n\n\nResearch: The number of stakeholders on a research project is small and they are usually aligned in terms of the project objective. The objective is often to achieve state-of-the-art performace on benchmark datasets. Model complexity is usually not an issue if it helps to eke out a tiny percentage of improvement on peformance metrics.\nProduction: There are often many stakeholders and their requirements can often be different and conflicting. It‚Äôs usually not possible to design an ML system satisfying all requirements, so a compromise must be reached. The value of performance gain is case-dependent. At times, a tiny improvement can save a lot of money for a business. However, often a small improvement does not justify the increase in model complexity.\n\n\n\n\n\n\n\nModel development code is a small part of the full codebase needed for deployment. Adapted from Sculley et al. 1\n\n\nResearch: A research group focuses most of its effort on model development. Due to the need to train many different models, research prioritizes fast training. In other words, high throughput is the desired quality in an ML system where throughput is the number of prediction requests that can be processed by the system in unit time.\nProduction: When deploying a model in production, the surrounding infrastructure of data pipelines, resource management, and servers has to be constructed and this takes a lot of time and effort. Once deployed, the focus is now on fast inference. Hence, we seek low latency in this case where latency is the amount of average waiting time before a prediction request is processed.\n\n\n\nResearch: The benchmark datasets in research are usually clean, static, and well-formatted. The anomalies in the dataset, if present, are usually known and often open-source code to process the datasets is available.\nProduction By contrast, data in real-world systems can be noisy, unstructured, and its distribution can shift with time. There may be bias in the data. Labels may be sparse, imbalanced, or incorrect. The data may not be available as a complete dataset but may arrive over time in form of a stream as it gets generated by the user. Finally, one has to respect user privacy when dealing with their data.\n\n\n\nResearch: Fairness is rarely given consideration in a research setting where the agenda is achieving state-of-the-art accuracy. Even when fairness is considered, it is an afterthought. This also has to do with lack of meaningful fairness metrics.\nProduction: A machine learning system deployed in the real-world can have influence on society members as a result of the decisions it outputs. Giving strong emphasis to fairness can ensure that no section of society gets adversely affected by ML models and that all sources of bias have been dealt with. Unfortunately, much progress remains to be done on this front as a lot of models deployed for loan applications, predictive policing, employee recruitement still discriminate against minority groups.\n\n\n\nInterpretability is the property of an ML system to explain the rationale behind the decision it makes or the output it gives. If a system is interpretable, one can peek inside the model for better understanding; in case there is some mistake/anomaly, one can pinpoint with confidence where the model is going wrong and debug accordingly.\nResearch: With ML research being evaluated on a single objective, that of model performace, there is no incentive to ensure that the model is interpretable.\nProduction: Intrepretability is a key requirement for real-world ML systems. For us to deploy an ML system in society, we need to be able to trust it. This trust is highly dependent on the system being interpretable. In case of misbehavior, we want to be able to diagnose and rectify our model.\n\n\n\nResearch: In research, one often focuses solely on the model development part. The benchmark dataset is static. There is no monitoring to be done once good results on the benchmark dataset have been achieved. Advances in software and hardware, and accumulated common wisdom have made this process relative fast and cheap.\nProduction: The deployment of ML systems is still relatively fast. Once deployed, an ML system needs to be constantly monitored and maintained and this is the hard part. There are some aspects of ML systems that make them much more challenging to monitor and maintain as compared to traditional software systems, particularly the dependence of model performance on external data. At a high level, one needs to constantly ensure that the data pipeline does not get broken, the incoming data is sensible, and the test data distribution does not differ considerably from training distribution. In case of distribution drift, one needs to retrain the model."
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html#footnotes",
    "href": "posts/ML-research-vs-production/index.html#footnotes",
    "title": "Machine Learning in Research versus Production",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSculley, David, et al.¬†‚ÄúHidden technical debt in machine learning systems.‚Äù Advances in neural information processing systems 28 (2015).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/LLM-intro/bb.html",
    "href": "posts/LLM-intro/bb.html",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "",
    "text": "Regularization is a common technique in machine learning to prevent overfitting. The two most widely used regularizers are the L2 and L1 norms. In this post, we look at how there regularizers can be thought of as being derived from prior distributions on the parameters we‚Äôre estimating.\n\nBackground\nWe are going to make extensive use of the Bayes‚Äô theorem. Let \\(w\\) be the parameter we want to estimate and \\({\\mathcal D} = (x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\) be the dataset. Then, we have:\n\\[P(w|{\\mathcal D}) = \\frac{P({\\mathcal D}|w) \\cdot P(w)}{P({\\mathcal D})}.\n\\]\nIn the above equation,\n\n\\(P(w)\\) is the prior distribution on the parameters \\(w\\); this encodes our belief about what the parameter values should likely be before we have looked at any data.\n\\(P(\\mathcal{D}|w)\\) is the likelihood of \\(\\mathcal{D}\\) given some assignment of \\(w\\).\n\\(P(w|\\mathcal{D})\\) is the posterior distribution of the parameters \\(w\\) after the dataset \\(\\mathcal D\\) is known.\n\nLet‚Äôs say we want to estimate parameter \\(w\\) from an observed dataset \\(\\mathcal D\\). We assume the relation between \\(x\\) and \\(y\\) as\n\\[y = f(x ; w) + \\epsilon\\]\nwhere \\(\\epsilon\\) is noise drawn from a Gaussian distribution with mean \\(0\\) and variance \\(\\sigma^2\\). This results in a Gaussian likelihood:\n\\[P({\\mathcal D}|w) = {\\mathcal N}(y|f(x ; w), \\sigma^2)\\]\nLet us assume that the samples in \\(\\mathcal D\\) are independently and identically distributed (i.i.d). Under this assumption and taking logarithms for ease of computation, the likelihood value now becomes:\n\\[\\log P({\\mathcal D}|w) = \\sum_{i = 1}^{N} \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2).\\]\nWe can estimate parameter \\(w\\) by maximizing the above quantity. This is called maximum likelihood estimation (MLE). Often time, this method of estimating \\(w\\) is called a frequentist approach, in constrast to the Bayesian approach we discuss below.\n\n\nL2 Regularization\nWe note that the denominator in the Bayes‚Äô theorem does not depend on the parameters \\(w\\) we want to estimate; hence we can ignore that term. We define the unnormalized posterior distribution as\n\\[P'(w|{\\mathcal D}) = P({\\mathcal D}|w) \\cdot P(w)\\].\nLet us now see what happens when we introduce prior distribution on parameter \\(w\\). In the first case, let us assume that \\(w\\) follows a Gaussian distribution \\({\\mathcal N}(w|0, \\lambda^{-1})\\) where \\(\\lambda\\) is a strictly positive scalar. We then have:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot {\\mathcal N}(w|0, \\lambda^{-1})\\].\nTaking logarithm on both sides, we obtain:\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda w^2 + \\text{const}.\\]\nWe can now see how our selection of prior for \\(w\\) as normal distribution results in L2 regularization. In the above equation, \\(w^2\\) is the squared L2-norm of the vector \\(w\\) with \\(\\lambda\\) controlling the strength of regularization.\nOften, we seek only a point estimate of \\(w\\) instead of the full posterior distribution. One solution is to take the mode of this posterior as the estimate of \\(w\\); this approach is called maximum a posteriori (MAP) estimation. MAP estimation differs from MLE in the fact that we incorporate prior knowledge about \\(w\\).\n\n\nL1 Regularization\nWe‚Äôll first need to look at a distribution called the Laplace distribution. With \\(w\\) as the random variable, it is given by\n\\[ g(w|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|w - \\mu|}{b}\\right)\\]\nand \\(\\mu, b\\) are referred to as location parameter and diversity respectively.\nNow let us assume that the prior distribution on \\(w\\) is a Laplace distribution with location parameter \\(\\mu = 0\\) and \\(b = \\lambda^{-1}\\). The unnormalized posterior distribution in this case becomes:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot \\frac{\\lambda}{2} \\exp\\left(-\\lambda |w|\\right).\\]\nTaking logarithm on both sides,\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda \\cdot |w| + \\text{const}.\\]\nHence with Laplace distribution as prior on \\(w\\), we arrive at L1 regularization. Again, \\(\\lambda\\) controls the strength of regualarization."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html",
    "href": "posts/Bayesian-inference/index.html",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "",
    "text": "Regularization is a common technique in machine learning to prevent overfitting. The two most widely used regularizers are the L2 and L1 norms. In this post, we look at how there regularizers can be thought of as being derived from prior distributions on the parameters we‚Äôre estimating."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#background",
    "href": "posts/Bayesian-inference/index.html#background",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "1. Background",
    "text": "1. Background\nWe are going to make extensive use of the Bayes‚Äô theorem. Let \\(w\\) be the parameter we want to estimate and \\({\\mathcal D} = (x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\) be the dataset. Then, we have:\n\\[P(w|{\\mathcal D}) = \\frac{P({\\mathcal D}|w) \\cdot P(w)}{P({\\mathcal D})}.\n\\]\nIn the above equation,\n\n\\(P(w)\\) is the prior distribution on the parameters \\(w\\); this encodes our belief about what the parameter values should likely be before we have looked at any data.\n\\(P(\\mathcal{D}|w)\\) is the likelihood of \\(\\mathcal{D}\\) given some assignment of \\(w\\).\n\\(P(w|\\mathcal{D})\\) is the posterior distribution of the parameters \\(w\\) after the dataset \\(\\mathcal D\\) is known.\n\nLet‚Äôs say we want to estimate parameter \\(w\\) from an observed dataset \\(\\mathcal D\\). We assume the relation between \\(x\\) and \\(y\\) as\n\\[y = f(x ; w) + \\epsilon\\]\nwhere \\(\\epsilon\\) is noise drawn from a Gaussian distribution with mean \\(0\\) and variance \\(\\sigma^2\\). This results in a Gaussian likelihood:\n\\[P({\\mathcal D}|w) = {\\mathcal N}(y|f(x ; w), \\sigma^2)\\]\nLet us assume that the samples in \\(\\mathcal D\\) are independently and identically distributed (i.i.d). Under this assumption and taking logarithms for ease of computation, the likelihood value now becomes:\n\\[\\log P({\\mathcal D}|w) = \\sum_{i = 1}^{N} \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2).\\]\nWe can estimate parameter \\(w\\) by maximizing the above quantity. This is called maximum likelihood estimation (MLE). Often time, this method of estimating \\(w\\) is called a frequentist approach, in constrast to the Bayesian approach we discuss below."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l2-regularization",
    "href": "posts/Bayesian-inference/index.html#l2-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "2. L2 Regularization",
    "text": "2. L2 Regularization\nWe note that the denominator in the Bayes‚Äô theorem does not depend on the parameters \\(w\\) we want to estimate; hence we can ignore that term. We define the unnormalized posterior distribution as\n\\[P'(w|{\\mathcal D}) = P({\\mathcal D}|w) \\cdot P(w)\\].\nLet us now see what happens when we introduce prior distribution on parameter \\(w\\). In the first case, let us assume that \\(w\\) follows a Gaussian distribution \\({\\mathcal N}(w|0, \\lambda^{-1})\\) where \\(\\lambda\\) is a strictly positive scalar. We then have:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot {\\mathcal N}(w|0, \\lambda^{-1})\\].\nTaking logarithm on both sides, we obtain:\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda w^2 + \\text{const}.\\]\nWe can now see how our selection of prior for \\(w\\) as normal distribution results in L2 regularization. In the above equation, \\(w^2\\) is the squared L2-norm of the vector \\(w\\) with \\(\\lambda\\) controlling the strength of regularization.\nOften, we seek only a point estimate of \\(w\\) instead of the full posterior distribution. One solution is to take the mode of this posterior as the estimate of \\(w\\); this approach is called maximum a posteriori (MAP) estimation. MAP estimation differs from MLE in the fact that we incorporate prior knowledge about \\(w\\)."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l1-regularization",
    "href": "posts/Bayesian-inference/index.html#l1-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "3. L1 Regularization",
    "text": "3. L1 Regularization\nWe‚Äôll first need to look at a distribution called the Laplace distribution. With \\(w\\) as the random variable, it is given by\n\\[ g(w|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|w - \\mu|}{b}\\right)\\]\nand \\(\\mu, b\\) are referred to as location parameter and diversity respectively.\nNow let us assume that the prior distribution on \\(w\\) is a Laplace distribution with location parameter \\(\\mu = 0\\) and \\(b = \\lambda^{-1}\\). The unnormalized posterior distribution in this case becomes:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot \\frac{\\lambda}{2} \\exp\\left(-\\lambda |w|\\right).\\]\nTaking logarithm on both sides,\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda \\cdot |w| + \\text{const}.\\]\nHence with Laplace distribution as prior on \\(w\\), we arrive at L1 regularization. Again, \\(\\lambda\\) controls the strength of regualarization."
  }
]