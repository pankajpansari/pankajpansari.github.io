[
  {
    "objectID": "posts/LLM-intro/index.html",
    "href": "posts/LLM-intro/index.html",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts.\n\n\n\n\n\nThe essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence ‚ÄúI am going to the market‚Äù, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don‚Äôt provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‚Äòencoded‚Äô embeddings. In each operation of that sequence, we allow the embeddings of all words to ‚Äòinteract‚Äô and influence each other. The effect is that the new set of ‚Äòencoded embeddings‚Äô encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output. The softmax function normalizes the values in a vector to a probability distribution, bringing higher values closer to 1 and lower values to 0.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt‚Äôs clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs.\n\n\n\n\n\nA machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it‚Äôs better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let‚Äôs say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task.\n\n\n\n\n\n\n\nA lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There‚Äôs been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don‚Äôt clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#introduction",
    "href": "posts/LLM-intro/index.html#introduction",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) are very large deep learning models that aim to predict and generate sensible text in a natural or symbolic human language. LLMs, in other words, are trained to model human language and even symbolic language such as code. We say that LLMs are trained for the task of language modeling.\nWe call models that can generate any form of content as generative models; for examples, models to generate images, videos, or music. Since LLMs generate text, they are also generative models.\nIn this blog post, we look at the difference between LLMs and earlier language models (LMs), we briefly review the architecture and training strategies for LLMs, and explain why they have been astoundingly successful at a wide variety of language tasks. We defer a detailed discussion of more fundamental technical topics such as language modeling, embeddings, and Transformer models to later blog posts."
  },
  {
    "objectID": "posts/LLM-intro/index.html#background",
    "href": "posts/LLM-intro/index.html#background",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "The essence of tasks such as text translation, question-answering, making a chatbot is learning how to model human language. An LM is basically a probabilistic model that assigns a probability \\[ P(w_1, w_2, \\dots, w_n) \\] to every finite sequence of words \\(w_1, \\dots, w_n\\) (grammatical or not). This joint probability can be represented in terms of conditional probabilities as\n\\[ P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\dots \\times P(w_n|w_1, w_2, \\dots, w_{n-1}).\\]\nHence, for the sentence ‚ÄúI am going to the market‚Äù, we have\n\\[P(\\text{I am going to the market}) = P(\\text{I}) \\times P(\\text{am|I}) \\times P(\\text{going|I am}) \\times P(\\text{to|I am going}) \\] \\[\\times P(\\text{the|I am going to}) \\times P(\\text{market|I am going to the}).\\]\nHence, an LM needs to learn these conditional probabilities for many different groups of words and phrases. With a large text dataset at hand, this aim can be formulated as a machine learning task in two ways:\n\nprediction of next word in the text given the previous words/phrases in a sentence; for example,\n\n\n\n\nFig. 1 - Next word prediction\n\n\n\nprediction of masked words or phrases given the rest of the words in the sentence (called masked language modeling); for example,\n\n\n\n\nFig. 2 - Masked word prediction\n\n\nHence, the system creates its own prediction challenges from the text corpus. This learning paradigm where we don‚Äôt provide explicit training labels is called self-supervised learning. Since we do away with the need for expensive labeling, use of large unlabeled text datasets, scraped from the Web, becomes possible. The concept is used not just in the domain of natural language processing (NLP), but for computer vision as well.\nPrediction of masked/next word(s) is powerful because doing it well calls for different kinds of understanding; every form of linguistic and world knowledge from grammar, sentence structure, word meaning, to facts help one to perform this task better. In performing language modeling, a model gathers a wide understanding of language and the world represented in the training corpus.\n\n\n\nTransformers are NLP models that take in text (a sequence of words) and output another text to perform some task such as translation or question-answering.\n\n\nAn embedding represents a word by a position in a real-valued vector space whose dimension can be in hundreds or thousands. The proximity of embeddings of two different words in this space in an indication of their semantic similarity.\n\n\n\nFig. 3 - Schematic representations of embeddings in 2-dimension. Normally embeddings have many more dimensions.\n\n\n\n\nThe Transformer, at a high level, consists of two main components - an encoder and a decoder. The encoder takes in the word embeddings and transforms them by a sequence of operations to produce another set of ‚Äòencoded‚Äô embeddings. In each operation of that sequence, we allow the embeddings of all words to ‚Äòinteract‚Äô and influence each other. The effect is that the new set of ‚Äòencoded embeddings‚Äô encapsulates higher-level context of the sentence.\nThe decoder takes in the set of embedded vectors to produce a sequence of real-valued vectors. Once we pass this sequence through a linear layer and softmax, we obtain the desired output. The softmax function normalizes the values in a vector to a probability distribution, bringing higher values closer to 1 and lower values to 0.\n\n\n\nFig. 4 - Encoder-decoder model.\n\n\n\n\n\nAn key feature of Transformer models is their use of attention during the encoding and decoding phases. Attention layers are special architectural features that are present in encoder and decoder. In encoder, they enable the model to pay specific attention to certain words in the input sentence when trying to form a representation of each word. In decoder, they enable the model to pay attention to already produced output when generating the next output word.\nIt‚Äôs clear that interpretation of words and phrases is context dependent and depends on the remaining words/phrases in the sentences. Crucially, we only need a few context words, and not the whole sentence, to determine the meaning/representation of a particular word. For example, the meaning of the word bat in Cricket is played with a bat can be inferred by looking at Cricket and that in Bats are nocturnal creatures by looking at creatures.\nAttention also lends itself to parallel computation, thereby boosting the speed at which powerful NLP models can be trained on GPUs."
  },
  {
    "objectID": "posts/LLM-intro/index.html#large-language-models",
    "href": "posts/LLM-intro/index.html#large-language-models",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A machine learning model trained on vast quantity of data at scale (generally using self-supervision) can be adapted to a wide range of downstream tasks; such a model is called a foundation model. Large language models are a specific type of foundation model for NLP tasks and they make use of the Transformer architecture we discussed above. Some examples of LLMs are BERT, GPT-3, and T5.\nThough LLMs are based on the already established ideas of deep learning and self-supervised learning, it is the scale of these models and the datasets on which they are trained that make possible the astonishing performance on a wide variety of tasks. This scale is facilitated by improvements in computer hardware (GPU and memory), development of novel Transformer architecture, and the availability of huge datasets. Self-supervised learning is important to the ability to use huge data, since annotation is not required in this case.\nThe significance of foundation models lies in two concepts: emergence and homogenization. Emergence means that the foundation models with their billions of parameters can be adapted to a wide variety of tasks, through mere textual description of the task (prompts); that is, they are able to do in-context learning for many tasks for which they were neither trained nor anticipated to be used for. Homogenization means that there exists a handful of powerful base foundation models, such as BERT or T5, from which almost all state-of-the-art NLP models are derived through fine-tuning.\n\n\nThe original Transformer architecture consists of two parts - encoder and decoder. Depending on the task at hand, researchers use either of the parts or both, giving rise to three types of LLMs:\n\nEncoder-only LLMs (eg. BERT) - This variant uses only the encoder part. It is designed to produce dense embeddings for the input word sequence. While pretraining using masked word prediction, one attaches an un-embedding layer, which produces one-hot encoded words. For downstream tasks, we remove the un-emdedding layer. A small task-specific model is trained on top of the encoder-only model making use of the embeddings. Such models are most suitable for tasks like missing word prediction and document classification.\nDecoder-only LLMs (eg. GPT) - This variant uses only the decoder part of the Transformer. It is mainly used for text generation (output) from a given prompt (input). The input sequence or prompt is first encoded to a single large embedding from which the decoder outputs a sequence of words in an auto-regressive manner. Auto-regression means while generating a word, the model can refer to the previously generated words.\nEncoder-decoder (eg. T5) - This uses both encoder and decoder parts, making such a model quite large. It is used for tasks like language translation.\n\nWhile encoder-decoder models are generalizations of encoder-only and decoder-only, it‚Äôs better to use smaller models with less parameters if the task calls for that. Encoder-only models are good for understanding tasks, decoder-only for generation tasks, and encoder-decoder for tasks where both inputs and outputs can be large sequences.\n\n\n\nLLMs typically follow the paradigm of pretraining and transfer learning.\nPretraining - via self-supervised learning on a large textual corpus such as Wikipedia or GitHub. The resulting model is called pre-trained language model (PLM) and it can be adapted to a wide variety of downstream tasks. This is the part which takes a huge amount of training time and compute resources due to the size of the model and training data.\nTransfer learning - adapting the model to a specific task. Since the PLM has already acquired a lot of language and factual knowledge, this step needs a tiny amount of data and compute. This can be done via:\n\nFine-tuning - The parameters of PLM are adjusted by training with additional data relevant to the application. These can be of 3 types:\n\nUnsupervised: Suppose one is building a programming co-pilot using PLMs. The standard PLMs are usually pre-trained on internet text such as Wikipedia. We can now fine-tune them on code text, again using self-supervised learning.\nSupervised: PLMs are pre-trained for next or masked word prediction. If we want to use them for, let‚Äôs say, document sentiment analysis, we need to replace the output layer with a new one and train it with input-output pairs of texts and the associated sentiments.\nReinforcement Learning from Human Feedback (RLHF): This approach, mainly used by text generation models, consists of repeated execution of the following:\n\nThe model is given a prompt and it generates multiple plausible answers.\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\n\nPrompt engineering - Fine-tuning used to be the only paradigm for transfer learning until recently. Now more powerful PLMs like GPT-3 only require a prompt and no explicit training (zero-shot learning) or a handful of examples (few-shot learning) to adapt to a new task."
  },
  {
    "objectID": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "href": "posts/LLM-intro/index.html#challenges-and-research-directions",
    "title": "An Introduction to Large Language Models",
    "section": "",
    "text": "A lot of research work is being done currently on training foundation models using data from different modalities such as video and audio. By augmenting learning with multiple sensory data and knowledge, we provide stronger learning signals and increase learning speed.\nOne can also situate the foundation model in an environment where it can interact with other agents and objects; such models are called embodied foundation models. This can help the model learn cause and effect like humans by means of physically interacting with surroundings.\n\n\n\nThe impressive performance of LLMs on a wide variety of tasks, even on ones they were not trained for, has given rise to debates about whether or not these models are actually learn language in the way humans do 1 or whether they are just elaborate rewriting systems devoid of meaning.\nThe first position seems partly convincing because state-of-the-art LLMs remain susceptible to unpredictable and unhumanlike intelligence. Sometimes LLMs generate text and responses that seem syntactically correct and natural but in reality they are incorrect factually - this is called hallucination.\nOn the other hand, researchers argue that given the variety and difficulty of tasks multi-modal foundation models like GPT-4 can solve, we can confidently say they exhibit aspects of intelligence2. There‚Äôs been some recent work on understanding in-context learning which posits that perhaps these large foundation models have smaller machine-learning models inside them that the big model can train to perform a new task3. Clearly, some aspects of LLM behavior indeed seem intelligent, but not exactly in human way; this calls for a rethinking and expansion of the meaning of intelligence.\n\n\n\nAlignment refers to the process of ensuring that LLMs behave in harmony with human values and preferences. An aligned LLM is trustworthy. The characteristics needed for an LLM to be used with trust in the real-world are reliability, safety, fairness, resistance to misuse, explainability, and robustness 4. Out of these, here we only consider fairness.\nFairness - Due to the huge size of training data and LLMs, we don‚Äôt clealy understand the biases encapsulated in these models nor have an estimate of safety for use in critical applications. Homogenization is also a liability, since all derived NLP models may inherit the same harmful biases of a few foundation models. This calls for investing significant resources into curating and documenting LLM training data.\nAnother concern is that with more widespread use of LLMs, more content on the web is likely to be LLM-generated. When future models are trained on web data, bias is likely to be propagated and the models can become less capable - a phenomenon known as model collapse 5."
  },
  {
    "objectID": "posts/LLM-intro/index.html#footnotes",
    "href": "posts/LLM-intro/index.html#footnotes",
    "title": "An Introduction to Large Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBender, Emily M., et al.¬†‚ÄúOn the dangers of stochastic parrots: Can language models be too big?ü¶ú.‚Äù Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2021.‚Ü©Ô∏é\nBubeck, S√©bastien, et al.¬†‚ÄúSparks of artificial general intelligence: Early experiments with gpt-4.‚Äù arXiv preprint arXiv:2303.12712 (2023).‚Ü©Ô∏é\nAky√ºrek, Ekin, et al.¬†‚ÄúWhat learning algorithm is in-context learning? investigations with linear models.‚Äù arXiv preprint arXiv:2211.15661 (2022).‚Ü©Ô∏é\nLiu, Yang, et al.¬†‚ÄúTrustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models‚Äô Alignment.‚Äù arXiv preprint arXiv:2308.05374 (2023).‚Ü©Ô∏é\nShumailov, Ilia, et al.¬†‚ÄúThe Curse of Recursion: Training on Generated Data Makes Models Forget‚Äù arXiv preprint 2305.17493‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/Code_insight/index.html",
    "href": "posts/Code_insight/index.html",
    "title": "Automated Qualitative Feedback on Programming Assignments",
    "section": "",
    "text": "I‚Äôve been teaching courses on Computer Systems and Operating Systems at Plaksha University. Programming assignments are an integral part of such systems courses. The challenge comes in evaluating the large number of submissions. One can write scripts to score the correctness of code, perhaps by test cases; even performance measures such as execution time or cache memory hits/misses can be easily captured. The harder part is giving feedback on the quality of the code. Usually, TAs annotate the submission indicating how readable or well-organized the program is and whether it makes use of good language conventions. This calls for good programming expertise from the TAs themselves; a lot of effort is also required to give such feedback for a large class.\nI wrote a tool called CodeInsight to provide such feedback automatically. It makes use of a suite of state-of-the-art LLMs in an agent-based architecture. Given a problem statement and an assignment program, the tool returns an annotated version of the program, inserting comments at appropriate points, either to suggest improvement or to commend good implementation; the original code remains as it is. The parameters are cleanliness, language conventions, organization, data structures, and use of pointers (for C). It goes without saying that program correctness comes first and foremost; this tool does not check for that.\nWhile state-of-the-art models are good at code understanding, they do make mistakes. If let‚Äôs say Claude-3.5-Sonnet is asked to provide feedback in our desired format, three cases can arise:\n\nprovides the correct feedback at the appropriate point\nprovides feedback at some point, but that isn‚Äôt quite correct or relevant. For instance, it may suggest some optimization that may be valid but isn‚Äôt required as part of the assignment\nfails to identify a bad practice where it actually occurs\n\nBesides, different LLMs have different strengths. ChatGPT 4.0 may correctly pick up some points that Claude missed.\nConsidering these, it seemed appropriate to me to use an agent-based approach. Specifically, I used a multi-agent collaboration design pattern. Andrew Ng puts the idea succinctly: ‚ÄúMore than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would‚Äù. The architecture looks like the following:\n\n\n\nFig. 1 - CodeInsight architecture\n\n\nProposers, aggregators, annotators, and comparators are role names given to LLMs hinting the function they carry out. More details can be found in the home page of the GitHub repository. I‚Äôd just like to mention that there are actually 4 proposers (rather than 3 in the figure) and they are distinct models - Claude-3.5-Sonnet, GPT-4o, Mixtral-8x22B, and Llama-3.1-405B. Also in the spirit of breaking down tasks, the proposer-aggregator loop is run once for every evaluation parameter - readability, organization, language convention, etc. Finally, I found that it was critical for the prompts to be detailed and accurate. I made use of this excellent documentation on programming assignment grading from a Stanford course.\nThere are two challenges with such an agent-based approach - overall system latency and API costs. The first is not an issue for us since we use the tool offline on a batch of assignments. The cost can be a concern when the number of assignments is large. I don‚Äôt have cost estimates yet, but I felt it was important to first make the tool output as good as we can. I have yet to check the degradation of performance if we use, let‚Äôs say, 2 proposers instead of 4, or the latest suite of smaller models.\nThe fun part was the system evaluation. I wanted to compare the tool output with sample programs annotated by a good programmer. For the sample programs, I selected 10 C programs written by not-expert programmers (the programs should have scope for us to suggest feedback). I requested two of my good students to annotate those programs with feedback, and also ran the tool on the programs. I let GPT-4o do the comparison and provide a 0 or 1 score based on which of two inputs - tool or human annotation - is better (the comparison prompt was more elaborate). I computed the average win-ratio of the tool for each student. Averaged over both programmers, the win ratio was 5-5. This was encouraging.\nAlthough our tool goes beyond what traditional linters do, it‚Äôll be helpful to incorporate linter output and provide them as part of prompts to proposers. I‚Äôd like to do a more exhaustive human comparison testing on more C programs. Once I use it for some assignments in my course, hopefully students‚Äô feedback will give me pointers to improve the tool."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Throughput vs Latency - GPU vs CPU\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Qualitative Feedback on Programming Assignments\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2025\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nGrounding Language Models in the Physical World\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nA Probabilistic Perspective on Regularization\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in Research versus Production\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Large Language Models\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nPankaj Pansari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Visiting Faculty member in the Computer Science and Artificial Intelligence division of Plaksha University, where I teach courses on Computer Systems and Operating Systems. My current research interests lie in benchmarking reasoning abilities of LLMs.\nI completed my PhD under Dr.¬†Pawan Mudigonda in the OVAL group at University of Oxford, where I worked on optimization problems arising in machine learning. I‚Äôve been a Research Scientist at Naver Labs Europe in Grenoble, France. I also worked as a Data Analyst at General Electric in Bangalore, India.\nI completed my undergraduate studies at the Indian Institute of Technology, Kharagpur.\nCurriculum Vitae: pdf\nEmail: pankaj.pansari1@proton.me"
  },
  {
    "objectID": "posts/Bayesian-inference/index.html",
    "href": "posts/Bayesian-inference/index.html",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "",
    "text": "Regularization is a common technique in machine learning to prevent overfitting. The two most widely used regularizers are the L2 and L1 norms. In this post, we look at how there regularizers can be thought of as being derived from prior distributions on the parameters we‚Äôre estimating."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#background",
    "href": "posts/Bayesian-inference/index.html#background",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "1. Background",
    "text": "1. Background\n\nModel Complexity\nOur intuition is that simpler models are preferable; models that are excessively complicated lead to overfitting. Let \\(L_{\\mathcal D}({\\bf w})\\) measure the misfit of model with parameters \\(\\bf w\\) to a given dataset \\(\\mathcal D\\). We can augment the loss function to penalize more complex models:\n\\[ L({\\bf w}) = L_{\\mathcal D}({\\bf w}) + \\lambda L_{reg}(\\bf w)\\]\nThis is called regularization in machine learning and shrinkage in statistics. \\(\\lambda\\) is called the regularization coefficient and controls the trade-off between fitting the dataset \\(\\mathcal D\\) well and giving simpler, more generalizable model.\nThere are two ways to think of model complexity:\n\nmodel complexity as a function of weights of all the features in a model\n\nA feature weight with a high absolute value is more complex than a feature weight with low absolute value. In this case,\n\\[L_{reg}({\\bf w}) = {\\bf w}^T{\\bf w} = ||{\\bf w}||^2 = w_1^2 + w_2^2 + \\dots + w_n^2\\]\nThis is known as L2 regularization or weight decay. Optimizing loss function with L2 regularization is generally easier and results in small parameter values. The resulting model is known as ridge regression.\n\nmodel complexity as a function of the total number of features with nonzero weights\n\nTo achieve this, we don‚Äôt set our regularization penaly as the number of nonzero parameters - this makes the optimization difficult. Instead, we use L1 regularization:\n\\[L_{reg}({\\bf w}) = ||{\\bf w}||_1^2 = |w_1| + |w_2| + \\dots + |w_n|\\]\nThis has the same effect of setting only some parameter values to be nonzero, and is convex hence easier to optimize. The resulting model is known as Lasso.\n\n\nMaximum Likelihood Estimation\nWe are going to make extensive use of the Bayes‚Äô theorem. Let \\(w\\) be the parameter we want to estimate and \\({\\mathcal D} = (x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\) be the dataset. Then, we have:\n\\[P(w|{\\mathcal D}) = \\frac{P({\\mathcal D}|w) \\cdot P(w)}{P({\\mathcal D})}.\n\\]\nIn the above equation,\n\n\\(P(w)\\) is the prior distribution on the parameters \\(w\\); this encodes our belief about what the parameter values should likely be before we have looked at any data.\n\\(P(\\mathcal{D}|w)\\) is the likelihood of \\(\\mathcal{D}\\) given some assignment of \\(w\\).\n\\(P(w|\\mathcal{D})\\) is the posterior distribution of the parameters \\(w\\) after the dataset \\(\\mathcal D\\) is known.\n\nLet‚Äôs say we want to estimate parameter \\(w\\) from an observed dataset \\(\\mathcal D\\). We assume the relation between \\(x\\) and \\(y\\) as\n\\[y = f(x ; w) + \\epsilon\\]\nwhere \\(\\epsilon\\) is noise drawn from a Gaussian distribution with mean \\(0\\) and variance \\(\\sigma^2\\). This results in a likelihood that is Gaussian distribution:\n\\[P({\\mathcal D}|w) = {\\mathcal N}(y|f(x ; w), \\sigma^2)\\]\nLet us assume that the samples in \\(\\mathcal D\\) are independently and identically distributed (i.i.d). Under this assumption and taking logarithms for ease of computation, the likelihood value now becomes:\n\\[\\log P({\\mathcal D}|w) = \\sum_{i = 1}^{N} \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2).\\]\nWe can estimate parameter \\(w\\) by maximizing the above quantity. This is called maximum likelihood estimation (MLE). Often time, this method of estimating \\(w\\) is called a frequentist approach, in constrast to the Bayesian approach we discuss below."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l2-regularization",
    "href": "posts/Bayesian-inference/index.html#l2-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "2. L2 Regularization",
    "text": "2. L2 Regularization\nWe note that the denominator in the Bayes‚Äô theorem does not depend on the parameters \\(w\\) we want to estimate; hence we can ignore that term. We define the unnormalized posterior distribution as\n\\[P'(w|{\\mathcal D}) = P({\\mathcal D}|w) \\cdot P(w)\\].\nLet us now see what happens when we introduce prior distribution on parameter \\(w\\). In the first case, let us assume that \\(w\\) follows a Gaussian distribution \\({\\mathcal N}(w|0, \\lambda^{-1})\\) where \\(\\lambda\\) is a strictly positive scalar. We then have:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot {\\mathcal N}(w|0, \\lambda^{-1})\\].\nTaking logarithm on both sides, we obtain:\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda w^2 + \\text{const}.\\]\nWe can now see how our selection of prior for \\(w\\) as normal distribution results in L2 regularization. In the above equation, \\(w^2\\) is the squared L2-norm of the vector \\(w\\) with \\(\\lambda\\) controlling the strength of regularization.\nOften, we seek only a point estimate of \\(w\\) instead of the full posterior distribution. One solution is to take the mode of this posterior as the estimate of \\(w\\); this approach is called maximum a posteriori (MAP) estimation. MAP estimation differs from MLE in the fact that we incorporate prior knowledge about \\(w\\)."
  },
  {
    "objectID": "posts/Bayesian-inference/index.html#l1-regularization",
    "href": "posts/Bayesian-inference/index.html#l1-regularization",
    "title": "A Probabilistic Perspective on Regularization",
    "section": "3. L1 Regularization",
    "text": "3. L1 Regularization\nWe‚Äôll first need to look at a distribution called the Laplace distribution. With \\(w\\) as the random variable, it is given by\n\\[ g(w|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|w - \\mu|}{b}\\right)\\]\nand \\(\\mu, b\\) are referred to as location parameter and diversity respectively.\nNow let us assume that the prior distribution on \\(w\\) is a Laplace distribution with location parameter \\(\\mu = 0\\) and \\(b = \\lambda^{-1}\\). The unnormalized posterior distribution in this case becomes:\n\\[P'(w|{\\mathcal D}) = \\prod_{i = 1}^{N} {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) \\cdot \\frac{\\lambda}{2} \\exp\\left(-\\lambda |w|\\right).\\]\nTaking logarithm on both sides,\n\\[log P'(w|{\\mathcal D}) = \\sum_{i = 1}^N \\log {\\mathcal N}(y_i|f(x_i ; w), \\sigma^2) - \\lambda \\cdot |w| + \\text{const}.\\]\nHence with Laplace distribution as prior on \\(w\\), we arrive at L1 regularization. Again, \\(\\lambda\\) controls the strength of regularization."
  },
  {
    "objectID": "posts/Grounding-LLMs/index.html",
    "href": "posts/Grounding-LLMs/index.html",
    "title": "Grounding Language Models in the Physical World",
    "section": "",
    "text": "I recently listened to a podcast episode on The Robot Brains where Jitendra Malik, an eminent computer vision researcher, shared his thoughts and experiences on grounding large language models (LLMs) in the physical world and how to approach this through robotics. I summarize the discussion below:\n\nMoravec‚Äôs Paradox\n\nMoravec wrote in 1988, ‚Äúit is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility‚Äù 1.\nLLMs have shown enormous success recently on a wide variety of cognitive tasks. Among other benchmark tests, these systems have done well on challenging competitive exams. This gives us a feeling that these systems have acquired impressive intelligence. However, this holds true for tasks involving abstract, higher-level cognition. The challenging problems of locomotion and other desirable motor skills in robotics have not become solved as a result of this progress on LLMs. We seem to be still consistent with Moravec‚Äôs Paradox.\n\nEvolutionary Perspective\n\nOn our human evolution journey, brain development followed the development of hands with opposable thumb; hand development in turn followed the development of bipedal walking which left our hands free. We developed sensorimotor skills first, language acquisition is a more recent phenomenon. If we think of human evolution on a 24-hour timeframe, langugage development corresponds only to the final 2-3 minutes. Clearly, all of intelligence cannot be said to reside in those final couple of minutes. Besides, different species of animals have different flavours of intelligence, and these are sophisticated in many cases. These animals do not possess language ability in the conventional sense. Hence, what we can learn from language alone may be inherently limited.\n\nHuman Learning and Development\n\nWe can take inspiration from how babies and children learn. Babies interact with the world around them in a multi-modal way, using different senses. They gradually learn to manipulate objects and perform small experiments of their own. The acquisition of words at this stage is grounded in physical objects and interactions with them. So for instance when a mother says ‚Äòthis is a ball‚Äô, there is a visual input of the ball along with the motor desire to throw or catch a ball.\nWhen children go to school after the age of 5 and acquire knowledge through books, they already have a basic understanding of the world, people, and objects on which to build upon. This points to the need for a multi-modal and staged process of learning (curriculum learning).\n\nEmbodied AI\n\nA new paradigm of intelligent models can be robots equipped with vision, touch, audio sensors with the ability to move, manipulate objects and interact with the real-world. As the robot learns more about the dynamics of the world, we can teach it basics of language. The rest of langugage acquisition will happen in an in-context manner, by combining atomic concepts, much like children do after age 5.\n\nRapid Adaptation of Motor Skills\n\nThere is a compelling argument for this approach of acquiring atomic concepts that are grounded in the world followed by general language acquisition and development of abstract thinking. Any robotic system needs to be adaptable and robust. For instance, a robot must be able to walk on diverse, unseen terrains. The robot must be capable of learning new policies quickly, with the time-frame being task dependent. For example, a walking robot needs to learn to stabilize on a new terrain in about a second else the robot will fall. Just like humans need only a handful of examples to pick up a new concept, we can hope our ML systems modeled on lines of human learning and development, will be able to quickly adapt with a small number of simulations/trial and error."
  },
  {
    "objectID": "posts/Grounding-LLMs/index.html#footnotes",
    "href": "posts/Grounding-LLMs/index.html#footnotes",
    "title": "Grounding Language Models in the Physical World",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia contributors. ‚ÄúMoravec‚Äôs paradox.‚Äù Wikipedia.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html",
    "href": "posts/ML-research-vs-production/index.html",
    "title": "Machine Learning in Research versus Production",
    "section": "",
    "text": "I have been going through the book ‚ÄòDesigning Machine Learning Systems‚Äô by Chip Huyen to better understand how machine learning systems are deployed in production in industry. Coming from a research background, it‚Äôs been a good experience to learn the difference in emphasis and priorities between how machine learning is practised in research versus in production. Here, I summarize some of my learnings on this topic:\n\n\nResearch: The number of stakeholders on a research project is small and they are usually aligned in terms of the project objective. The objective is often to achieve state-of-the-art performace on benchmark datasets. Model complexity is usually not an issue if it helps to eke out a tiny percentage of improvement on peformance metrics.\nProduction: There are often many stakeholders and their requirements can often be different and conflicting. It‚Äôs usually not possible to design an ML system satisfying all requirements, so a compromise must be reached. The value of performance gain is case-dependent. At times, a tiny improvement can save a lot of money for a business. However, often a small improvement does not justify the increase in model complexity.\n\n\n\n\n\n\n\nModel development code is a small part of the full codebase needed for deployment. Adapted from Sculley et al. 1\n\n\nResearch: A research group focuses most of its effort on model development. Due to the need to train many different models, research prioritizes fast training. In other words, high throughput is the desired quality in an ML system where throughput is the number of prediction requests that can be processed by the system in unit time.\nProduction: When deploying a model in production, the surrounding infrastructure of data pipelines, resource management, and servers has to be constructed and this takes a lot of time and effort. Once deployed, the focus is now on fast inference. Hence, we seek low latency in this case where latency is the amount of average waiting time before a prediction request is processed.\n\n\n\nResearch: The benchmark datasets in research are usually clean, static, and well-formatted. The anomalies in the dataset, if present, are usually known and often open-source code to process the datasets is available.\nProduction By contrast, data in real-world systems can be noisy, unstructured, and its distribution can shift with time. There may be bias in the data. Labels may be sparse, imbalanced, or incorrect. The data may not be available as a complete dataset but may arrive over time in form of a stream as it gets generated by the user. Finally, one has to respect user privacy when dealing with their data.\n\n\n\nResearch: Fairness is rarely given consideration in a research setting where the agenda is achieving state-of-the-art accuracy. Even when fairness is considered, it is an afterthought. This also has to do with lack of meaningful fairness metrics.\nProduction: A machine learning system deployed in the real-world can have influence on society members as a result of the decisions it outputs. Giving strong emphasis to fairness can ensure that no section of society gets adversely affected by ML models and that all sources of bias have been dealt with. Unfortunately, much progress remains to be done on this front as a lot of models deployed for loan applications, predictive policing, employee recruitement still discriminate against minority groups.\n\n\n\nInterpretability is the property of an ML system to explain the rationale behind the decision it makes or the output it gives. If a system is interpretable, one can peek inside the model for better understanding; in case there is some mistake/anomaly, one can pinpoint with confidence where the model is going wrong and debug accordingly.\nResearch: With ML research being evaluated on a single objective, that of model performace, there is no incentive to ensure that the model is interpretable.\nProduction: Intrepretability is a key requirement for real-world ML systems. For us to deploy an ML system in society, we need to be able to trust it. This trust is highly dependent on the system being interpretable. In case of misbehavior, we want to be able to diagnose and rectify our model.\n\n\n\nResearch: In research, one often focuses solely on the model development part. The benchmark dataset is static. There is no monitoring to be done once good results on the benchmark dataset have been achieved. Advances in software and hardware, and accumulated common wisdom have made this process relative fast and cheap.\nProduction: The deployment of ML systems is still relatively fast. Once deployed, an ML system needs to be constantly monitored and maintained and this is the hard part. There are some aspects of ML systems that make them much more challenging to monitor and maintain as compared to traditional software systems, particularly the dependence of model performance on external data. At a high level, one needs to constantly ensure that the data pipeline does not get broken, the incoming data is sensible, and the test data distribution does not differ considerably from training distribution. In case of distribution drift, one needs to retrain the model."
  },
  {
    "objectID": "posts/ML-research-vs-production/index.html#footnotes",
    "href": "posts/ML-research-vs-production/index.html#footnotes",
    "title": "Machine Learning in Research versus Production",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSculley, David, et al.¬†‚ÄúHidden technical debt in machine learning systems.‚Äù Advances in neural information processing systems 28 (2015).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/throughtput_latency/index.html",
    "href": "posts/throughtput_latency/index.html",
    "title": "Throughput vs Latency - GPU vs CPU",
    "section": "",
    "text": "CPUs are optimized for latency; GPUs are optimized for throughput. They are independent processors, so can and should work on different things at the same time.\nLatency - Time taken to complete a single operation or instruction; commonly measured in clock cycles. Lower latency is obviously desirable. Note that latency can be talked of for CPU/GPU computation or memory access - they are two different things. The value of latency of an instruction depends on the type of instruction (integer/floating-point/SIMD). Here we are not referring to a particular type of instruction.\nThroughput - Number of operations or instructions completed per unit of time. Again, throughput can refer to computation or memory access. By convention, when we say throughput, we mean computation throughput and that is measured typically in FLOPS (Floating Point Operations Per Second). Higher throughput is obviously better.\nThe analogue of throughput for memory is called bandwidth. It‚Äôs the maximum amount of data transfer that can happen per unit time and these days it‚Äôs typically measured in GB/s. Note that bandwidth encompasses the memory device (DRAM/SRAM etc.) as well as the bus speeds.\nSo here we are - we want higher throughput and lower latency. It may seem like lowering latency would automatically increase throughput and that‚Äôs true in theory; in practice there are constraints and trade-offs. We have to decide whether it‚Äôs more critical for us to have latency-optimized or throughput-optimized system. Optimizing for one practically leads to sub-optimal performance for the other.\n\nAnalogy\nConsider an analogy: we want to get from Secretariat in New Delhi (green) to Dwarka (red). We can take either the metro which takes about 75 minutes or use a car which takes 45 minutes. The metro takes longer for us (higher latency) but it transports a lot more people in let‚Äôs say an hour (higher throughput). So, the metro is optimized for throughput and the car is optimized for latency. Note that a car is more flexible and can take us to places where the metro doesn‚Äôt go.\n\n\n\nMetro vs Car\n\n\nCPUs are the cars - optimized for latency. Context switching between threads is expensive. So the CPU makes an individual thread as fast as possible. By CPU here, we actually mean both the processor and the memory system. Focusing on the processor, there are complex things like branch prediction, out of order execution, prefetching to make the latency low - this makes the cores complex and takes up real-estate on chip. Hence, we only have a few cores on a CPU. The memory hierarchy is elaborate with multiple levels to optimize for latency.\nGPUs, on the other hand, are the metros. The cores are simple and hence there can be a lot of them. The focus is on massive parallelism, so throughput is high. Latency for an individual thread may not be great, but for GPU workloads that‚Äôs not so important. Even the GPU memory has simpler organization and wider buses to optimize for throughput. Such a system, like a metro, works best when it‚Äôs oversubscribed; there is a deep queue of pending threads to be executed.\n\n\nAsynchrony\nCPUs and GPUs are independent processors. They can and should work on different things at the same time. The following is a bad model of how to use a CPU and a GPU together:\n\n\n\nSynchronous\n\n\nNote how GPU does some computation and then waits for something for CPU - this happens repeatedly. It‚Äôs as if the CPU and GPU are operating synchronously. In this case, a much better flow is the following:\n\n\n\nAsynchronous\n\n\nIn the above, the CPU issues computation to GPU and then starts working on its own thing separately. There is good utilization of both processors. We say that they are operating asynchronously - we‚Äôre aiming for this.\n\nThis post was inspired by part of Steve Jones‚Äô talk at GTC 2021. Thanks to Eric for telling me about it."
  }
]